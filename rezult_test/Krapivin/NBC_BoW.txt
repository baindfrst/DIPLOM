--T
The Use of Lemmas in the Model Elimination Procedure.
--A
When the model elimination (ME) procedure was first proposed, the notion
of lemma was put forth as a promising augmentation to the basic complete
proof procedure. Here the lemmas that are used are also discovered by the
procedure in the same proof run. Several implementations of ME now exist, but
only a 1970s implementation explicitly examined this lemma mechanism, with
indifferent results. We report on the successful use of lemmas using the
METEOR implementation of ME. Not only does the lemma device permit METEOR to
obtain proofs not otherwise obtainable by METEOR, or any other ME prover not
using lemmas, but some well-known challenge problems are solved. We discuss
several of these more difficult problems, including two challenge problems
for uniform general-purpose provers, where METEOR was first in obtaining the
proof. The problems are not selected simply to show off the lemma device, but
rather to understand it better. Thus, we choose problems with widely
different characteristics, including one where very few lemmas are created
automatically, the opposite of normal behavior. This selection points out the
potential of, and the problems with, lemma use. The biggest problem normally
is the selection of appropriate lemmas to retain from the large number
generated.
--B
Introduction
The Model Elimination (ME) procedure was defined in the 1960's [17, 18]
nearly coincident with the Resolution procedure [22]. There was little early
This work is supported in part by NSF grants CCR-8900383 and CCR-9116203.
experimentation with ME, so the procedure received relatively little attention.
(However, it is the basis of the SL-Resolution procedure which played an historical
role in the beginning of Logic Programming). A major implementation
of ME was completed in the early 1970s by Fleisig et al. [14] that did provide
a direct comparison with the Unit Preference - Set-of-Support refinement of
Resolution (Wos et al. [29]), as both procedures were implemented with the
same underlying inference tools. Individual wins were found for both methods
but no evidence to prefer ME over the known refinements of Resolution was
forthcoming. The notion of lemma, introduced in the first papers of ME, was
implemented by Fleisig et al. with a resulting indifference to their usefulness.
The paper included this statement:"The use of lemmas usually was detrimental,
although some short refutations were obtained using lemmas. The poor performance
is due to the lack of selection rules for lemmas and is aggravated by the
depth-first nature of search." ([14], p. 135). This paper will strongly refute the
conclusion of that paper regarding the value of lemmas within ME. We report
on results that to date are only obtainable within the ME framework by the use
of lemmas, and the theorems proven include some not yet provable by Resolution
techniques. We now understand that the notion of only direct competition
with Resolution is oversimplistic; this procedure tends to do relatively well on
non-Horn problems where Resolution methods now employed are not focused.
Unlike chess, theorems are a very diverse lot and different proof methods may
excel in different areas.
As is well-known to many in this research area, the idea of ME was kept alive
by the work of Mark Stickel. Most notably, Stickel exploited the fact that ME
is an extension of SLD-Resolution (Prolog) to develop the Prolog Technology
Theorem Prover (PTTP) [24, 25]. In the late 1980's almost simultaneously three
groups took this one step further, building on the architecture (the Warren Abstract
Machine (WAM)) developed for Prolog that the logic programming community
had extended to parallel machines. These projects were PARTHENON
(CMU) [9], PARTHEO (Munich) [23], and METEOR (Duke) [4, 3]. (The Munich
[16] and Duke efforts included sequential provers also.) The work reported
here has been implemented on one of the METEOR family of ME provers.
What has changed in 20 years that makes the lessons of the Fleisig et al.
paper invalid in part? A fair number of things: maybe most important is
the WAM architecture ideas, but also important are use of iterative deepening
(introduced to ME by Stickel), a sophisticated implementation exploiting the
WAM ideas, vastly more powerful yet cheaper computers, and careful use of
techniques to restrict lemma creation and use.
We look in depth at three examples, theorems proven with use of lemmas that
would otherwise not be proved using ME. Two theorems are from the theory of
continuous functions and one is a collection of three challenge problem from the
1960's not all solved automatically until now. (Two of the three latter challenge
problems were solved in the weak fully automated mode; that is, some initial
restructuring of the problem was done to force generation of lemmas. This
was done by splitting the problems into two and six cases respectively, without
knowledge of the problem solution. The details are presented later. We mention
here that the problem has subsequently been solved in (strong) fully automated
mode by the Semantic Hyper-Linking prover of [11]. To our knowledge no
other prover has succeeded on this problem in non-interactive mode.) Because
the purpose of this paper is to look in-depth at the nature of lemma use, and
because other papers [5, 3], contain results of METEOR on standard problems
of the Automated Theorem proving (ATP) community, we omit such listings
here. For example, the Astrachan and Stickel paper [5] discussing caching in
includes two tables of results and a brief discussion on lemma use. (Caching
is not applicable for the examples we treat here.)
The Model Elimination procedure is a linear input procedure; that is, one
parent clause is the preceding clause and the other clause is an input clause.
This is the key property of SLD-resolution that allows compilation of Prolog
programs, elegantly implemented in the WAM architecture. ME can use the
architecture structure and also enjoy a high inference rate. However, like
Prolog, ME does suffer high search redundancy due to the depth-first search
mode of the WAM architecture. (ME does use iterative deepening rather than
pure depth-first search, however.) Lemmas shorten proofs, thus reducing this
redundancy. We show by example that the compression of proof achieved with
lemma use can be striking, and the gain occurs on "real" (vs. toy) problems.
We note that there is a version (restriction) of resolution that is in 1-1 correspondence
with ME. The Resolution version is a linear input procedure except
for a restricted "ancestor resolution" operation where resolution between two
deduced clauses is necessary. (See the TOSS procedure in [19] 1 .) Thus, the
procedure can be regarded as an encoding of the TOSS Resolution restriction
using 2-sorted logic (framed and unframed literals), but it should be noted
that ME is not, strictly speaking, a Resolution procedure. This is highlighted
by the fact that Linear Input Resolution is complete only for a small extension
of Horn clause logic whereas ME is complete for all of first-order logic.
This paper is organized as follows: In Sections 2 and 3 we briefly describe
the ME proof procedure and the realization of this procedure in METEOR. In
Section 4, Section 5, and Section 6 we describe the successful application of
lemmas to three theorems none of which can be proved without lemmas using
We conclude with a short summary in Section 7.
2 The Model Elimination Procedure
Like Resolution, Model Elimination is a refutation procedure, with the formula
to be refuted presented in conjunctive normal form. Skolem functions are
1 The TOSS procedure corresponds to a slightly different ME procedure.
2 None of these problems yields a proof in less than 24 CPU hours using "vanilla" ME
without lemmas.
used to eliminate existential quantifiers. (See [10] or [19] for details on formula
preparation; [19] contains a full description of the ME procedure.) We view the
clauses as sets of literals and the formula as a set of clauses. The corresponding
entity to a derived clause in Resolution is the (derived) chain, an ordered list
of literals where each literal is of one of two classes, an A-literal or B-literal.
ME, as currently defined, has two basic operations 3 , extension and reduction.
The basic operation of extension is like the Resolution operation with retention
of one of the literals resolved upon. The retained literal is promoted to a A-
literal. (Intuitively, the A-literal is a type of ancestor literal.) We give a succinct
presentation of the ME operations and lemma mechanism here, followed by an
example. The appendix gives an expanded presentation of the procedure.
The first chain in a deduction is an ordered input clause with all literals
classified as B-literals.
Below and in general we oversimplify the reference to occurrences of literals.
In particular, we refer to a literal in successive chains as the same literal when
in fact the later literal may be an instantiation of its "parent" literal. This
simplification should cause no confusion.
The extension operation glues a (shortened) input clause, ordered by user
choice, to the left of the current chain if the leftmost B-literal of the chain unifies
with the complement of some literal of the input clause. The new chain is the
instantiation of the current chain by the unifier with the unifying literal of the
input clause dropped, and the other unifying literal (the leftmost literal of the
current chain) promoted to A-literal. Newly added literals are B-literals and
other literals retain their classification from the current chain 4 . All leftmost
A-literals, if any, are removed back to the leftmost B-literal.
The reduction operation removes the leftmost B-literal of the current chain
if it can be unified with the complement of an A-literal of the chain. The new
chain is the instantiation of the current chain by the unifier with the leftmost
B-literal missing. Again, all leftmost A-literals, if any, are removed back to the
leftmost B-literal.
The creation of lemmas occurs when the leftmost A-literals are removed,
at the end of the extension and reduction operations. Here we use only unit
lemmas, although a more general notion of lemma, allowing multiliteral lemmas,
is defined in [19]. The lemma chains are created as the complements of the
removed A-literals, but only some A-literals can produce a lemma. To create
only unit lemmas the eligibility mechanism is simple. In a reduction operation,
the A-literal that complements the leftmost B-literal is the reduction A-literal.
During a reduction step, all A-literals strictly to the left of the reduction A-
literal are marked. Every A-literal in a newly created clause inherits the mark,
if any, of its parent A-literal. Any A-literal unmarked at the time of removal
3 Early papers [17, 18] had three operations.
Traditionally, ME chains grew "to the right" but Prolog conventions of replacing the
leftmost literal have influenced recent implementations. We choose to follow the convention
used in the METEOR implementation.
creates a lemma. Not all generated lemmas are retained; retention depends
on a number of criteria discussed later, some under control of the user. If the
lemma is retained it acts exactly as if it were a unit input clause regarding
extension. (Lemmas are subject to restrictions and modifications not shared by
input clauses, however.)
The ME refutation in Figure 1, adapted from [19, 2], illustrates the ME
mechanisms.
1. p(X) -q(Y) input clause
2. -p(X) r(Y) input clause
3. -p(a) -r(Y) input clause
4. q(X) p(a) input clause (and goal chain)
begin proof
5. p(Y) [q(X)] p(a) extension with 1
variable renaming, clause 1
6. r(Z) [p(Y)] [q(X)] p(a) extension with 2
variable renaming, clause 2
7. -p(a) [r(Z)] [p(Y)] [q(X)] p(a) extension with 3
8. [r(Z)] [p(a)] [q(X)] p(a) prior to removal of A-literals
p(a) reduction
unit lemmas formed
-p(a)
9. 2 extension with lemma -p(a)
Proof completion if lemma mechanism is not used:
9. -r(Y) [p(a)] extension with 3
10. -p(X) [-r(Y)] [p(a)] extension with 2
reduction

Figure

1: ME refutation
In

Figure

1 the unit lemmas are created during the final stage of the reduction
that yields chain p(a) at step 8. The reduction creates A-literal p(a) from parent
p(Y) and the subsequent removal of A-literals p(a) and q(X) create the lemmas.
Removal of r(Z) cannot create a unit lemma.
In

Figure

2 we present proof trees associated with the example, one proof
tree for the proof without a lemma use and one that incorporates the lemma
use. In a proof tree, each clause used in an extension is represented, here by its
clause number. The goal clause is listed as root of the proof tree. The literals
of a clause are represented by the child nodes of the clause, where the clause
(actually, the appropriate chain of the clause) that is used in the extension
on that literal is named. Reduction is shown by a backwards arrow from a
node labeled Rn, for the nth reduction. The arc to an ancestor node of the
reduction is labeled by the same Rn to indicate which is the A-literal of the
reduction. The proof tree gives sufficient information to reconstruct the proof
without backtracking; the instantiations of the variables are determined during
the tracing of the proof using the proof tree.
The proof tree on the right side of Figure 2 is a method of displaying lemma
use and also representing a proof tree without lemma use. The leftmost box
encloses the subdeduction that defines the lemma and the other occurrences
of that box in the deduction indicate positions where the lemma is used. As
such, the box would be replaced by a single node. The subdeduction in the box
provides a subtree that could occur if the lemma device is not used. (Note that
the tree to the left shows that a slightly different deduction actually occurred
when the lemma device was not used.) This reporting device is used later in the
paper for deductions of considerably larger size, and does give a graphic feeling
of the saving in proof size realized by lemma use.
The reduction arrow in the boxes is somewhat confusing in this instance
because the reduction is to the A-literal that creates or calls the lemma. For
the right side box the reduction arrow refers back to the literal that calls the
lemma, for that is the valid A-literal of reduction if the lemma is replaced by
its deduction.3
without lemma with lemma

Figure

2: proof trees
3 The METEOR Architecture
We provide here only a brief description of the METEOR architecture. A full
description can be found in [2]. METEOR is written in C and runs on workstations
and in parallel and distributed computing environments. The same search
engine is used in both the sequential and parallel setting. A discussion of the
architecture of METEOR with emphasis on the parallel and distributed modes,
appears in [3]. Iterative deepening is used to ensure completeness of the search
strategy in all cases.
METEOR is designed to conform to the principles of the Warren Abstract
Machine (WAM) [1], the de facto standard for Prolog implementations. How-
ever, clauses are not compiled into WAM code, but into a data structure that
is then interpreted at runtime by either a sequential or parallel search engine.
Several different depth measures can be used in bounding a potential proof
during an iterative deepening search. We report on four measures here, these
are described in Table 1.
measure description
D inf inference depth - bound total number of inferences in proof tree
D Alit A-literal depth - bound depth of proof tree
D roll rollback-reduction - combination of D inf and D Alit
(preference given to reductions)
Dweight weight depth - weight clauses (per branch of proof tree)

Table

1: depth measures employed in METEOR
The measures D inf and D Alit are commonly used in ME provers and are the
default measures in PTTP and SETHEO respectively. As a bound on the entire
proof tree, the use of D inf ensures that a minimal-length proof (in terms of
extensions and reductions) is found. D Alit was also used in the first implementation
of ME [14]. At any point in a proof search one can tell the current depth
with respect to the D Alit measure by counting the number of A-literals in the
current chain, whereas for the D inf measure it is the total number of extensions
and reductions needed to derive the current chain. (Of course, one does not
include subsearches removed by backtracking.) Note that the depth of a proof
tree corresponds to the number of A-literals in the longest chain of the proof;
this depth is the minimal D Alit depth for which this proof tree can be found.
For the example deduction at the end of the last section, when no lemma is
used the D inf is 7 and D Alit is 3. By this we mean that the proof can be found
in the depth stated, and that a depth bound less than that stated would not
permit the proof shown to be found. This can be seen from the given derivation
or by consulting the appropriate proof tree. Note that for the proof tree the
D inf measure is determined by counting the nodes in the proof tree (excluding
the root), and the D Alit is determined by counting the depth of the proof tree
with the root having depth 0. For the deduction using the lemma the D inf is
5 and the D Alit is 3. For the latter depth we assume that the left branch is
searched first; if the right branch were searched first then the D Alit is 2.
When D roll is employed, depth is calculated as for D inf regarding extension,
but a successful reduction operation results in "rolling back" the depth to that
depth in force when the A-literal of the reduction was introduced to the chain
as a B-literal. The measure was discovered by accident, and has no clear intuition
for its definition; we simply noted it worked well in many cases. One
observation regarding its success is that it charges for extension by unit clauses,
e.g. lemmas, (whereas D Alit has no charge) yet gives added resources to explore
the consequences of a reduction as does the D Alit measure, indeed often more
resources than D Alit gives. For the example deductions of the last section the
D roll is 4 for both lemma use and no lemma use. None of these measures is uniformly
superior to the others nor does it appear that simple syntactic criteria
can be used to determine which measure is more appropriate (see [2]).
Dweight is used in conjunction with any of these depth measures rather than
as a replacement for them. Input clauses are annotated with an integral weight
that contributes to the weight of a chain when the clause is used. The weight
of a chain is the sum of the weight of all clauses whose literals "appear" in the
chain. When Dweight is used it is incremented in an iterative manner just as
other depth bounds are incremented in an iterative deepening search.
Minimum Value Theorem
We first consider a problem where the lemmas are very effective. The theorem
we consider is the Minimum Value Theorem (MinVT). This is also known in
the ATP literature as AM8 [27].
Theorem: (MinVT) A continuous function in a real closed interval [a; b]
attains its minimum in the interval.
In

Table

2 we give a readable rendition of the axioms for MinVT. The numbers
on the left refer to the clause numbers for these axioms as they are given
in [27] and in the order submitted to METEOR.
In

Figure

3 we give the axioms as submitted to METEOR. The clause numbering
relates the input clause to its counterpart in Table 2. The "nocontra"
suppresses the contrapositive of the dichotomy axiom (axiom 1) from being generated
as it is clearly not needed and is a very costly axiom to include, as is the
dichotomy axiom itself. Line 16 is for naming Skolem functions, for which we
do not permit self nesting. The semantics of the problem made it likely that no
function needed to be so nested.
In

Figure

4 and Figure 5 we present in proof tree format the proof found by
METEOR. The proof tree is given in a coded form with the lemmas labeled but
unwound so that the full structure of the tree is apparent.

Figure

4 (on the left) shows the proof actually discovered by METEOR. It
is a proof involving seven input clauses and four lemmas. Using depth measure
D inf , this would be a proof of depth 10. Actually, it took D inf depth 12 (it
can be done in D inf depth 11) to discover the proof because some lemmas used
in the proof required that depth to be discovered. Figures 4 (on the right)
and 5 present the full proof, with each inference labeled by a lemma label or
by a clause number. The dotted arrows at the bottom of the proof tree to
the right in Figure 4 link to the appropriate subtrees in Figure 5. The node
labels beginning with "L" refer to lemmas, with the numbering determined by
the proof recovery procedure (not the order of lemma generation); other node
labels refer to the input clauses by number.
the axioms for total order
1. x - y - y - x
2. x - y - y - z
Roughly, because of the symmetry of the antecedent
3. x - y - y
l is a minimum point in [a,l]
4. a - l
5. l - b
Any point greater than l has a smaller point with smaller f-value.
7-9. a - x
For all x, h(x) is the smallest point in [a,b] s.t.
10-13. a - b - a - y
For all x, there exists a point k(x) in [a,b] such that
This is the negation of the theorem conclusion
14,15,17. a - x

Table

2: Axioms for Minimum Value Theorem (MinVT)
We made the point previously that lemmas are often discovered in earlier,
"failed" parts of the search tree. That is very evident here from the proof trees.
For example, some top lemmas are used only once in the proof and so, to be
invoked as lemmas, they must have been established before the proof was found.
(In particular, note lemmas L2, L3, and L4, labeled on the trees in Figure 4 with
derivations continuing in Figure 5.) Lemmas used in the proof of these lemmas
were clearly discovered by the time the more complex lemmas were obtained.
Note that this does not mean that they were discovered earlier in the same
proof tree iteration because lemmas are retained through successive iterations.
Thus information from "later" in the tree can be used on early branches of the
succeeding iterate. In general, iterative deepening gives a breadth-first form of
search within which the search is depth-first.
The effectiveness of lemmas is dramatically made by noting that the depth of
the proof given here, when D inf (the depth measure used here) is employed with
no use of lemmas, is over 600! Such search depth is well beyond the capability
of any implementation of ME, or any other prover we know.
It should be pointed out that resolution does exactly this kind of compres-
sion, and that resolvents are lemmas also, in the sense of retaining intermediate
results. Indeed, we noted that unit lemmas are resolvents from tree portions
where reduction is not used. The difference, as noted before, is that our lemmas
nocontra
1. p(X,Y) - p(Y,X)
2. -p(X,Y)-p(Y,Z) - p(X,Z)
3. -p(X,Y) -p(Y,X) - p(f(X),f(Y))
4. p(a,l)
5. p(l,b)
7. -p(a,X) -p(X,b) - p(a,q(X)) - p(X,l)
8. -p(a,X) -p(X,b) -p(f(X),f(q(X))) - p(X,l)
9. -p(a,X) -p(X,b) - p(q(X),X) - p(X,l)
11. -p(a,X) -p(X,b) - p(h(X),b)
12. -p(a,X) -p(X,b) - p(f(h(X)),f(X))
13. -p(a,X) -p(X,b) -p(a,Y) -p(Y,b) -p(f(Y),f(X)) - p(h(X),Y)
14. -p(a,X) -p(X,b) - p(a,k(X))
15. -p(a,X) -p(X,b) - p(k(X),b)
17. -p(f(X),f(k(X)))-p(a,X)-p(X,b)

Figure

3: Input clauses for Minimum Value Theorem (MinVT)
are optional regarding completeness. That does not mean they are optional
regarding realizing an actual proof, as this case demonstrates. However, not
being a priori concerned about having to retain almost every nonredundant
intermediate result we do achieve much more trimming. In the first proof we
obtained of MinVT, taking 171 secs., we created and retained just 593 lemmas
while making about 393,000 successful inferences. The only constraint we had
(besides the unit lemma constraint, which is universal for us but still a con-
straint) was a term-length stipulation of 10 on the lemmas and a disallowance
of iterative nesting of any function. For speed considerations the term-length
check is done only on the term replacing a variable at a substitution occurrence;
thus the literal may contain many more constant and function letters than the
bound indicates. This check can be done on literals in new chains as well as
on lemmas, but here was applied only to lemmas. Note, by the way, that the
nesting restriction applied here is almost totally safe for a first run as all functions
but f are Skolem functions, for which one should never gain by nesting,
and nesting f makes no heuristic sense given the problem nature. The sensitivity
to term length is quite modest; if term-length is not limited at all, the
same number of lemmas are retained. For this problem, the limit on nesting
of the same function symbol is necessary in order to constrain the number of
lemmas generated. (That is not always the case, however.) Thus we illustrate
the point that we can trade off more search with input clauses for less retention
goal
goal
Figure

4: top-level proof of AM8 and partial unrolling of proof
of intermediate results than many strategies of resolution employ.
5 Wang Challenge Problem
It is the usual case that we generate too many lemmas and seek ways of trimming
lemmas without removing those helpful to the proof. Therefore, it is an interesting
situation to encounter a problem where too few lemmas are produced.
This situation arose when we pursued a challenge problem set that had not
been fully proven by an automated theorem prover, to our knowledge. In 1965,
Hao Wang put forth several challenge problems for automated theorem provers,
problems beyond the capability of theorem provers at that time [26]. Moreover,
they proved difficult for automated provers for some time. The problem set we
consider were labeled ExQ1, ExQ2, and ExQ3 by Wang, and were treated in
the section "Examples from Quantification Theory". Although Wang suggests
that for a more intuitive understanding of these examples, we should think of
F as the membership relation ffl, this suggestion did not achieve its purpose for
us.
We state the three problems, which are formulas to be refuted in Table 3,
followed, in Figure 6, by one set of axioms used by METEOR. In Figure 6,
replaces constant n of the axioms (the latter for
purely historical reasons).
Problem ExQ1 has been solved by a number of systems, we suspect, because
it is also known as "Wos 31", in [28]. As such it has been solved by OTTER [20],
but because this is one of numerous problems that circulate under labels as
meaningful as "Wos 31", it is not easily determined what provers have solved

Figure

5: proof of AM8, lemmas unrolled
ExQ1: The conjunction of the following formulas is unsatisfiable.
1. m 6= n
2. n 6= k
3.
4.
5.
ExQ2: Replace (2) and (3) above by
1.
2.
ExQ3: Remove (1) from ExQ2.

Table

3: Axioms for ExQ problems
this first problem. OTTER has not solved ExQ2 or ExQ3 although ITP, in some
sense an ancestor of OTTER, did solve ExQ2.
We have solved it with some preconditioning on the manner of presentation
of the problem; we broke it into cases so as to force generation of lemmas.
As previously mentioned, the semantic hyper-linking theorem prover of Chu
and Plaisted subsequently has provided a fully automated proof, the first to
our knowledge [11]. (This problem is particularly well suited for their prover
because it is highly non-Horn, like a logic puzzle. That is why it turns out to be
poorly structured for us; the high number of reductions blocks the production
of unit lemmas.)
A proof of Wos31 (ExQ1) was obtained using D Alit without breaking the
theorem into cases; using Dweight reduces the time to one-third of that when no
weighting is employed. Note that our ExQ3 solution contains the ExQ1 case,
in terms of the inequalities for the three basic constants. As for ExQ1 we again
use D Alit for case 6 of ExQ3; for all other cases we use D roll . It is interesting
8 -p(Y,V),-p(V,Y),r(Y,m),p(Y,m),r(V,m),r(V,Y) 21 -r(X,Y),r(f(X),f(Y))
9 -r(g(Y),b),r(Y,b),p(Y,b) 22 -r(X,Y),r(g(X),g(Y))

Figure

Input clauses for ExQ2, case 1
that all other cases are solved using D roll and not easily solved (in some cases)
using D Alit .
When we first ran ExQ2, very few lemmas were generated and no proof was
obtained. Indeed, an overnight run (14 hours) yielded only 6 lemmas. In con-
trast, for ExQ1 87 lemmas were created in one successful run. Our experience
with the success of lemmas made us consider immediately how more lemmas
could be generated. Because unit lemma generation is inhibited by reductions
we decided to introduce unit clauses to allow more frequent proof branch termination
by unit extension rather than reduction. Although we did not understand
the proof of the theorem (and still don't) it did seem apparent that ExQ1 and
ExQ2 were in effect dealing with cases of equality and inequality of three con-
stants; k,m,n. We broke up the problem into the cases listed in Table 4 and
were able to prove each theorem. Note that the case definition for ExQ2 follows
immediately from the axioms added to define ExQ2. Statistics for these cases
are provided in Table 4.
ExQ2 cases
case
case
ExQ3 cases
case
case
case
case
case
case
problem depth time # inferences # lemmas
measure (secs)
(case 1) D roll Dweight 4,562 14,462,880 47
ExQ2 (case 2) D roll 4.6 12,683 26
(case 1) D roll 108 170,185 67
ExQ3 (case 2) D roll 14 19,916 104
(case
(case
ExQ3 (case 5) D roll 9 12,741 26
(case

Table

4: cases and statistics for ExQ1, ExQ2, and ExQ3
As reported in Table 4 for ExQ2(case 1), we used Dweight , although the same
result was obtained by D roll alone in twice the time (9784 sec.) and double the
inference total (29,886,276 inferences). The weights are the entries seen at the
end of each input clause. It is interesting to view a case of use of weights; these
weights were entered once prior to any weighted run, using an "algorithm" that
accounts for clause length primarily, but with high penalty for known problem
axioms such as the equality axioms. There is a small adjustment in general for
the number of free variables in a clause. The major gain in using the weights
here is the control of the equality axioms.
We comment on other aspects of the METEOR listing of the axiom set
in

Figure

6. The lex predicate is used to order constant symbols for use in
demodulation, and the skolem predicate lists the Skolem functions as mentioned
in the previous section. Neither restriction was used in the runs reported in
this section; demodulation was not used at all in these reported runs. (We
obtained so few lemmas that use of demodulation was not a useful option, but
the problem was initially specified so that demodulation could be used.) The
input axioms are seen to differ in small ways from the natural translation of the
Wang problem specification. Besides the replacement of b for n, there is the
appearance everywhere of example. We had done
our experiments with the axiom set obtained from [27] before tracing the origin
of the problem in preparation for this paper. We have chosen to accept the
variance from the original problem statement since [27] has become a standard
source for problems, including this problem set.
We now feel that we have discovered a new technique for ME, that of breaking
problems into cases to generate useful lemmas. It may even pay to do this
when many lemmas are generated, by severely limiting the natural lemmas being
produced if they do not yield a proof, and trying to generate new lemmas by
the case method. It is clear that this is not (yet) an automatable technique, as
other tries at this have not been useful. That is, we tried to see if the discovery
of a proof for ExQ1 could be speeded up by introduction of cases. However,
all runs of ExQ1 using added cases resulted in increased time to proof. But we
have seen that the technique of forcing lemma creation is fruitful. How widely
applicable the idea is awaits further experience.
6 Bledsoe Challenge Problem
The third problem we consider is actually a collection of variants on a problem
investigated by Bledsoe starting in the early 1970's. Using a natural deduction
style theorem prover with some special heuristics for limit theorems, Bledsoe
and Boyer [7] proved that the sum of two continuous functions at a limit point
is the limit of the sums at that point. They assigned the label LIM+ to this
problem. The theorem has since been proven on the STR -
+VE prover of Hines
and Bledsoe [8, 15], a general theorem prover with some special devices for
handling inequalities on the real line. Bledsoe recognized the difficulty of the
theorem for the uniform provers and issued a challenge set of problems based
on the LIM+ problem [6]. There are five different first-order axiom sets designated
as the challenge set; two make explicit use of the equality predicate with
one formulation requiring use of paramodulation or another rule that builds-in
equality.
Using METEOR we have been able to prove the first three formulations of
the challenge problem. The lack of a full built-in equality mechanism (no mechanism
such as paramodulation) precludes an attempt on the fifth formulation of
the problem. The fourth formulation, using equality without any strong equality
restriction or simplification mechanism stronger than the unadorned demodulation
mechanism for lemmas that we do have, is too much for METEOR at
this point. The first three formulations use the single predicate - (also used in
the MinVT theorem discussed in Section 4). Several uniform provers have now
obtained the third formulation of the challenge problem but METEOR was the
first to achieve success at this level to our knowledge. (See Digricoli [13, 12] regarding
proofs of the fourth and fifth formulations done in a partially interactive
mode.)
Our purpose here is to report on the experience of METEOR with lemma
use on this problem set. The third formulation is the hardest problem where
METEOR has succeeded, where by "hardest" we mean that the strongest set of
restrictions we have ever used were needed to obtain a proof. One proof requires
over 20 hours on a SUN SparcStation 2; shorter proofs required some insight as
to the proof so do not count as legitimate datapoints regarding the capability
of METEOR as a fully automated theorem prover. Proof attempts using proof
knowledge are used to test various constraints in shorter time periods. We will
make clear which experiments use proof insight when such results are presented.
In

Figure

7 we give the clauses used in the three formulations, in roughly
the order that METEOR took them as input (read from left-to-right and top-to-
bottom). The clause numbers are those of the original presentation by Bledsoe
in [6]. METEOR normally re-orders clauses by number of literals (with preference
for unit clauses), with attention to degree of generality also considered.
The item "n at the end of each clause assigns a clause weight n, as discussed
later.
We do not give the original non-clausal formulation of the clause set because
the meaning of most clauses is self-evident, once the constants are interpreted.
The predicate lt denotes -, the function ab denotes absolute value, d1 and and
d2 are Skolem functions for regions (see below), ha is one-half, pl is +, ng
is negative, xs is a Skolem function for exception points in the delta regions,
and min is, of course, minimum. Clauses 3 and 4 give the continuity condition
for function f and g respectively. For example, clause 3 is a direct translation
of Equation 1
%clause 5 % clauses 1,2
clause 12 % clause 8
clause 8.1
clauses 3,4
clause 6
clause 9.1 % clause 10.1
clause 9.2 % clause 10.2
clause 9.11 % clause 10.11
clause 10.3 % clause 11.3
lt(X,0),lt(Y,0),-lt(min(X,Y),0)"6 -lt(X,ha(Z)), -lt(Y,ha(Z)), lt(pl(X,Y),Z)"6
clause 14 % clause 15
clause 7.1
clause 7.2

Figure

7: clauses for Bledsoe challenge problems
The goal clauses, 7.1 and 7.2, give the negation of the theorem we assert (in
conjunction with clause 6). For example, clause 7.1 states
Clause 7.1 is a more difficult goal to achieve because of the need to use the
triangle inequality. However, that change and the related need for clause 8.1,
are not significant changes. Formulation 2 is not much harder than formulation
1. The introduction of the dichotomy axiom (axiom 14) and especially the transitivity
axiom (axiom 15) along with axiom changes for min makes formulation
3 a considerably harder problem.
We list the three formulations by clause set, but will here only study the third
formulation. The first two problem variations METEOR could handle relatively
straightforwardly. The clauses for all three formulations are:
formulation 1 clauses 1, 2, 3, 4, 5, 6, 7.2, 9.11, 10.11, 10.3, 11.3, 12. The goal
is clause 7.2.
formulation 2 clauses 1, 2, 3, 4, 5, 6, 7.1, 8.1, 9.11, 10.11, 10.3, 11.3, 12. The
goal is clause 7.1.
formulation 3 clauses 1, 2, 3, 4, 5, 6, 7.1, 8, 9.1, 9.2, 10.1, 10.2, 11.3, 12, 14,
15. The goal is clause 7.1.
We include some data on runs for the first two formulations with little dis-
cussion; see Table 5. In the lemmaizing run, only ground lemmas are stored and
only ground subgoals are extended by lemmas. For reasons not important here
these combined restrictions are labeled "ground generalizations only".
thm search parameters
time (secs) and number of inferences
D Alit Dweight y lemmaizing z
one 1222.7 2,854,399 3.94 14,682 26.37 36,232
two 3859.8 12,294,133 9.76 43,980 10242.9 11,682,410
yusing D Alit and Dweight
zground generalizations only

Table

5: time and inferences for formulations one and two
We also give the proof tree for the second and third formulations in Figure
8 with the proof of the second formulation on the left. The dotted boxes
surrounding subtrees denote lemma occurrences as discussed below.
6.1 The Third Formulation
We now list the proof devices and restrictions used to tackle the third formula-
tion. Many have been used here only, so our experience with them is limited.
Others are more common, and we can comment on our more general experience
with them.
1. We use a combination of depth measures, D Alit and Dweight . This has been
tried often, usually obtaining considerably improved performance. This is
one of the few times that the proof has not been obtained without this
device. ( This is an oversimplification in that a proof has been obtained
using a combination of D roll and Dweight but only under optimal limits
on other parameters. It was considerably more time-consuming than with
D Alit and Dweight , so more general attempts were not tried.)
2. Only ground lemmas are stored, and these are only used when no instantiation
of the calling goal occurs. This trims the branching factor to one
at this node.
6 10.1

Figure

8: proof trees for second and third formulations
3. Lemma literals are limited in number of symbols allowed.
4. The number of occurrences of any specific input clause allowed in a proof
is controlled. METEOR is very sensitive to this parameter, so we could
not set it by uninformed guess and realize a proof. For our no-proof-
knowledge proofs METEOR increments the upper bound on the clause
occurrence count as a function of current depth according to a ratio set
by the user. The ratio needed is moderately insensitive to user setting in
terms of obtaining a proof at all, but the time needed to realize the proof
is strongly sensitive to that ratio.
5. Literals containing terms of certain forms are not retained. The exclusions
range from obvious redundancies to possibly strong exclusions. We have
results for several combinations of exclusions.
6. Reduction is restricted to B-literals whose source is the transitivity axiom.
This restriction is more natural than it sounds, for reasons stated below.
7. The goal clause is reordered from the usual rule of ordering. There are
only two literals so there is a binary choice, but normally we would expand
the more constrained literal first. This is a significant deviation, because
we rarely had to have to rotate through the literals of the goal clause, yet
in this case no proof has been obtained using the usual literal as the literal
first expanded.
We now expand on some of the above.
The depth measure Dweight utilizes the clause weight that appears appended
at the end of each input clause. The depth measure functions as does D Alit
except the clause weight is used to increment the count instead of the unit
increase used by D Alit . This depth measure may be used in conjunction with
each of the other measures; the proof development terminates appropriately
when any depth count is exceeded, so one gets the conjunction of the depth
criteria.
The weights used in the clauses were guessed once and not changed, following
a prescribed pattern. Some adjustment from a strict formula occurred. For
example, the min axioms constitute one-third of the axioms, but are not likely
to constitute one-third of the clauses used in a proof, so are weighted higher.
The initial "guess" did not use proof knowledge, so is considered a non-sensitive
constraint.
Controlling the number of occurrences of each input clause was necessary for
the any proof to succeed. In actuality it is control of dichotomy and transitivity
that is crucial; experiments show that we can make quite safe guesses about
upper bounds for all other axioms if the transitivity and dichotomy axioms
are limited to the optimal count. We have solved the problem with iterative
incrementing on all clauses, but this is effectively equivalent to binding only
dichotomy and transitivity (clauses 14 and 15) as shown in Line 6 of Table 7
which indicates limits on clause use of optimal plus twenty except for dichotomy
and transitivity. When iterative deepening on clause limits is employed the user
still must pick a ratio of clause-limit increment to depth bound increment, but
this is less sensitive than guessing the clause count. Data on Line 7 of Table 7
increases the count by one for every two levels of increase in the D Alit count.
Literals can be excluded from retention when certain terms are created upon
unification. This is an action at the chain level, not just at the lemma level.
Thus this can effect completeness "in the small". This avenue is taken because
of the large number of proof branches explored that seem to expand from along
very questionable routes, dealing with terms that are either unlikely to occur in
the proof or are redundant with other terms. The terms excluded range from
clearly safe for exclusion to perhaps quite risky. We stress that we do obtain
proofs of the theorem excluding only the very safe exclusions, but performance is
definitely enhanced with more risky exclusions. In Table 6 we list the exclusions
used.
1. d1(d2(X)) 2. d2(d1(X)) 3. d1(d1(X)) 4. d2(d2(X))
5. d1(pl(X,Y)) 6. d2(pl(X,Y)) 7. min(min(X,Y),Z) 8. min(X,min(Y,Z))
9.
13. min(ab(X),Y) 14. min(pl(X,Y),Z) 15. min(X,pl(Y,Z)) 16. min(X,X)
17.

Table

terms excluded in Bledsoe challenge problems
We used four classes of risk in the exclusion heuristic: very safe, safe, some
risk, more risk. In the very safe category are the nested Skolem functions, terms
such as d1(d2(X)). Also included is the term min(X; X). Still considered
safe is the term ha(ab(X)), because ab(ha(X)) is not excluded. Likewise for
included. That is slightly riskier
perhaps because the symbol count has increased and there is a limit on the
total number of symbols allowed per literal. (The limit used was either 11 or
15, about equally used, but 20 symbols still allowed a proof.) Others in Table 6
are riskier; we discuss this further in the next paragraph. Line 3 of Table 7 gives
data for a run using very safe exclusions only. (The phrase "ex. 1-4, 16" means
that terms 1-4 and 16 are excluded terms for that run.) Lines 4 and 5 provide
data using other subsets of the terms in Table 6. The column labeled att.infs
gives attempted inferences, which are potential inferences identified prior to the
unification attempt.
run statistics
description time infs. att.infs. # of lem.infs.
of run (sec)
1. optimal (termsize=15) 27858 23,146 66,485 115 590
2. optimal (termsize=20) 23095 23,052 66,346 153 601
3.y very safe (ex. 1-4,16) 66609 65,871 196,968 186 1,136
4.y ex. 1-4,7-10,12,14,16 49206 47,861 143,409 133 1,037
5.y ex. 1-4,9,10,16 64368 61,499 186,207 180 1,124
limits
7. iter. deep.z 152077 129,419 355,250 181 2,443
ytermsize limit of 15 symbols
zSparc ELC, increase all limits by 1 every other stage

Table

7: altering limits on extensions for third Bledsoe problem
We now briefly consider some of the riskier exclusions we considered. Excluding
similar terms may seem very risky, but we noticed
a proliferation of such terms and it led to asking if they were needed.
Note that terms of this type could still arise through instantiation of variables.
Here exclusion means that unification cannot instantiate a variable with an excluded
term. To illustrate this, the goal \Gammalt(min(e0; e0); min(X; e0)) can be
refuted with the exclusion of all terms in Table 6. In obtaining the proof the
term min(min(e0; e0); e0) is generated when X in the goal is instantiated to
e0). Thus, an excluded term's appearance is not forbidden, just the
use of a term that embeds the excluded term as a substitution instance. The
reader can see this by first extending with clause 9.2 followed by extension with
clause 9.1. Even though completeness is endangered this is a reasonable action
to take and this type of step may be needed to get very difficult theorems. We
are adverse to this not because it endangers completeness if the exclusions are
risky, but because we have been developing the thesis that basic ME should be
hidden and the user should play with the lemmas. We give in Table 8 data on
runs with exclusions, and note again that the exclusions are not needed for the
proof.
run statistics
description time infs. att.infs. # of lem.infs. % succ.
of run (sec)
1. all limits 23324 23,144 66,994
2. D roll , all limits 38663 37,928 116,069 71 1,081 33
3. ex. 1-17 38144 38,450 116,421 71 987 20
4. ex. 1-10,12-16,18 23533 23,709 68,196
5. ex. 1-8,18 43228 38,601 110,147 75 664 22
6. ex. 1-4,7,8,16-18 42101 37,633 107,665 75 659 22
7. ex. min terms 26529 31,290 89,898
8.y ex. 1-4,7-10,12,14,16 49206 47,861 143,409 133 1,037 21
9.y ex. 1-4,9,10,16 64368 61,499 186,207 180 1,124 12
10.y ex. 1-4,16 66609 65,871 196,968 186 1,136 12
14.z trm.size=15 27858 23,146 66,485 115 590 48
yterm-size
zotherwise as in line 1

Table

8: performance characteristics for the third formulation
That reductions are restricted to B-literals from the transitivity axiom actually
came from noting that the simpler proofs for formulations 1 and 2 did not
use reductions. That puts this in the category of "cheating" by our standards.
Even though it was not based on knowing the proof of formulation three, it is
not likely in the real world that simpler versions of the problem are around to
break in our heuristics. Even though that is how we arrived at this guess, there
is reason to guess that way again in similar situations. That is, it is a guess
we would make in other similar situations. The reason is that we have seen
many proofs having some non-Horn clauses where nevertheless reduction is not
needed. So, after our initial tries that tells us the problem is hard, we would
first try without reduction. Then we would try as here, allowing it on transi-
tivity, because they often do use it. This is the first time that a problem has
strongly benefited from this restriction; we have simply made the observation
about when reduction occurs in other problems. More experience is needed to
tell whether the pattern we just have suggested really holds.
The interchange of literal order within the goal is the hardest to justify on
uniform grounds. It works this way and not the other way. Usually, starting
with the more instantiated literal first is the winning strategy. The normal
ordering works in the first two formulations; the proof can be obtained without
lemma use starting with the more instantiated literal. The proof is impossible
to find without lemmas when the literal lt(D; 0) is expanded first. The situation
reverses even for these easier formulations when lemmas are tried. The reason
is clear from the proof tree. In Figure 8 we see that if one follows the second
literal (the right branch) then the key lemma, in the dotted line box, is reached
in D Alit depth 8 while pursuit down the other branch requires depth 13. (Recall
that D Alit is proof tree depth, so the reader can check this directly by counting
branch lengths in the proof trees of Figure 8).
We note that the variable D in the goal is instantiated by the key lemma
as follows: \Gammalt(min(d1(ha(e0)); d2(ha(E0))); 0). This instantiation gives the
found in the typical proof found in analysis texts: for a given epsilon (ffl 0 )
the corresponding delta is the minimum of the deltas (ffi 1 and corresponding
to half of the given epsilon. The min axioms are necessary for discovering
this instantiation. However (and somewhat surprisingly), if the min axioms
are removed from the input clause set a proof is obtained in less than 600
seconds with no limits on clause use. However, in this case "lt(d1(ha(e0)),0) or
lt(d2(ha(e0)),0)" is deduced, giving less information than is provided when the
min axioms are included, but in significantly less time.

Summary

Plaisted [21] has shown that ME with unit lemmas and caching is one of the few
goal-sensitive procedures that has a polynomial search time within the propositional
Horn clause setting in a suitable sense. The result assumes that ME is
used with iterative deepening search, the normal mode for METEOR. Elsewhere
(see Astrachan and Stickel [5]) data is given on use of unit lemmas and caching
in the Horn clause domain. Here we have presented examples in the non-Horn
setting (where caching is not implemented nor easily designed) but where we
have demonstrated that unit lemmas are still very effective. We have considered
examples illustrating very different situations: the nearly ideal, the insufficient
lemma supply situation (where case analysis provided the needed lemmas), and
a barely adequate lemma supply (where only one deep lemma is used and not
even created until the correct proof is under development). This wide scope
of situations we believe demonstrates the usefulness of unit lemma addition to
the Model Elimination procedure. We hope to achieve enough knowledge about
lemma use that henceforth we will view the basic ME procedure as a black box
and the collection of lemmas as the data the user views and manipulates.

Appendix


We give a more formal definition of the Model Elimination(ME) procedure as
used by METEOR. A fuller treatment of the ME procedure(s) is given in [19].
A chain is a sequence of literals, each literal an A-literal or a B-literal. (That
is, a chain is an ordered clause with two types of literals.)
An input chain is a chain of B-literals, obtained by placing an ordering
on a clause from a conjunctive normal form (CNF) formula to be tested for
unsatisfiability. Clauses from the CNF formula are called input clauses. A
input clause of n literals from the CNF formula must be represented by at least
chains, where each literal of the input clause is leftmost in at least one input
chain.
The empty chain is denoted by 2.
If l is the leftmost literal in chain C we write (C \Gamma flg) to represent the
chain C with the leftmost occurrence of l removed. There will be no occasion to
remove a literal other than the leftmost literal, or a sequence of literals leftmost.
In the latter case we regard the literals as being removed one at a time as the
literal becomes leftmost in an intermediate chain.
A chain is preadmissible iff
1. complementary literals are separated by an A-literal;
2. no B-literal is to the left of an identical A-literal;
3. no A-literal is identical or complementary to another A-literal.
A chain is admissible iff it is preadmissible and the leftmost literal is a B-
literal. In practice we will sometimes not enforce condition (2); that can be a
user option.
Let C 1 be an admissible chain and let C 2 be a variable-disjoint input chain.
If there exists a most-general-unifier(mgu) oe of the leftmost literal l 1 of C 1 and
the complement of the leftmost literal l 2 of C 2 , then the extension operation
extends C 1 by C 2 to form chain C 3 by promoting the leftmost literal of C 1 oe
to an A-literal and placing (C g)oe to the left, the literals of (C
arranged in any order the user chooses. All literals in C 3 oe inherit the type
of their parent literal in C 1 and C 2 with the noted exception of the promoted
A-literal. If the chain that results is not preadmissible then the extension is
not defined. If the chain that results is a preadmissible chain with an A-literal
leftmost (i.e., nonadmissible) then the leftmost A-literals are removed back to
the leftmost B-literal, which yields an admissible chain.
Let C be an admissible chain. If there exists a mgu oe of the leftmost literal l
of C and an A-literal of C, then the reduction operation yields chain (C \Gamma flg)oe.
If the chain that results is not preadmissible then the reduction operation is not
defined. If the chain that results is a nonadmissible preadmissible chain then
the A-literals to the left of the leftmost B-literal are removed.
An ME derivation is a sequence of chains composed of an input chain as
first (goal) chain, followed by derived chains, each either an extension of the
preceding chain by some input chain or a reduction of the preceding chain.
An ME refutation is an ME deduction of the empty chain.
Lemmas, an optional property of ME, are created when A-literals are removed
in the transformation from preadmissible to admissible chain. A lemma
is an informal label for either a lemma clause that results from the process of
lemma creation, or the chains that are created from the clause. Here we consider
only unit lemmas so the distinction is pedantic. From the lemma clause only
one chain is created, consisting of the single B-literal and regarded as an ordered
clause. A lemma chain is treated as an input chain after generation (if it passes
filters that may eliminate it; these are generally user-controlled options).
The lemma clause is the complement of the A-literal removed when the A-
literal is eligible to create a lemma. The eligibility criterion is exactly as stated
in the body of the paper but repeated here for completeness. This definition
is only correct for unit lemmas. In a reduction operation, the A-literal that
complements the leftmost B-literal is the reduction A-literal. During a reduction
step, all A-literals strictly to the left of the reduction A-literal are marked.
Every A-literal in a newly created clause inherits the mark, if any, of its parent
A-literal. Any A-literal unmarked at the time of removal creates a lemma clause.



--R


Investigations in Model Elimination Based Theorem Prov- ing
METEOR: Exploring model elimination theorem proving.
METEORs: High performance theorem provers using model elimination.
Caching and lemmaizing in model elimination theorem provers.
Challenge problems in elementary calculus.
Computer proofs of limit theorems.
Variable elimination and chaining in a resolution-based prover for inequalities
A parallel theorem prover for non-Horn clauses
Symbolic Logic and Mechanical Theorem Proving.
Semantically guided first-order theorem proving using Hyper-linking
The Rue theorem-proving system: the complete set of LIM+challenge problems

An implementation of the model elimination proof procedure.
The central variable strategy of str - +ve

Mechanical theorem proving by model elimination.
A simplified format for the model elimination procedure.
Automated Theorem Proving: A Logical Basis.
Otter 2.0.
The search efficiency of theorem proving strategies.
A machine-oriented logic based on the resolution principle
PARTHEO: A high performance parallel theorem prover.
A Prolog technology theorem prover: Implementation by an extended Prolog compiler.
A Prolog technology theorem prover: Implementation by an extended Prolog compiler.

Hierarchical deduction.

Efficiency and completeness of the set of support strategy in theorem proving.
--TR

--CTR
Allen Van Gelder , Fumiaki Okushi, Lemma and cut strategies for propositional model elimination, Annals of Mathematics and Artificial Intelligence, v.26 n.1-4, p.113-132, 1999
Fumiaki Okushi , Allen Van Gelder, Persistent and Quasi-Persistent Lemmas in Propositional Model Elimination, Annals of Mathematics and Artificial Intelligence, v.40 n.3-4, p.373-401, March 2004
Allen Van Gelder, Autarky Pruning in Propositional Model Elimination Reduces Failure Redundancy, Journal of Automated Reasoning, v.23 n.2, p.137-193, August 1999
Allen Van Gelder , Fumiaki Okushi, A propositional theorem prover to solve planning and other  problems, Annals of Mathematics and Artificial Intelligence, v.26 n.1-4, p.87-112, 1999

extracted:['feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-containment'
 'finite automata' 'zero storage biometric authentication' 'semantics']
marked:['METEOR', 'lemma', 'model elimination']
--T
Testing Positiveness of Polynomials.
--A
Many problems in mathematics, logic, computer science, and engineering
can be reduced to the problem of testing positiveness of polynomials (over
real numbers). Although the problem is decidable (shown by Tarski in 1930),
the general decision methods are not always practically applicable because
of their high computational time requirements. Thus several partial methods
were proposed in the field of term rewriting systems. In this paper, we
exactly determine how partial these methods are, and we propose simpler
and/or more efficient methods with the same power.
--B
Introduction
Many problems in mathematics, logic, computer science, and
engineering can be reduced to the problem of testing positiveness
of polynomials. In 1930, Tarski [33, 34] showed that the
problem is decidable. In fact, he gave a decision method for
a more general problem than just testing positiveness. Since
many improvements and new methods were proposed
19, 18, 25, 36, 8, 21, 22, 5, 12, 23].
However, these methods are computationally expensive due
to their generality. Naturally one is interested in devising
more efficient methods for the sub-problem: testing posi-
tiveness. But then, this sub-problem turns out to be still
difficult. Thus, several authors (mainly from the field of
rewriting systems, for proving termination) proposed
some partial methods, partial in the sense that these method
might not be able to decide the positiveness of some polynomials
[24, 4, 10, 10, 31, 32, 11].
In this paper, we determine how much partial these methods
are. In other words, we find proper subsets of the set
of all positive polynomials that are decided by those meth-
ods. Equivalently, we determine restrictions on the positiveness
that are decided by them. Further, we propose simpler
and/or more efficient methods for the same sets of polynomials

The structure of the paper is as follows. In Section 2, we
list three different restrictions on the property of positiveness
(eventual, absolute, eventual absolute). In the subsequent
sections, we show that each of the existing methods is a decision
procedure for one of the above restricted positiveness
properties. We also give several new methods.
Several Types of Positiveness
let P be a non-zero polynomial in
R[x], and let - 2 R.
Definition 2.1 (Types of Positiveness) We say that P is
ffl positive from - iff 8x
eventually positive 1 iff there exists a - such that P is
positive from -.
absolutely positive from - iff P is positive from - and
every partial derivative P   is non-negative from -, that
eventually absolutely positive iff exists a - such that P
is absolutely positive from -. 2
Example 2.1 Let polynomial. The polynomial
P is positive from 0. In fact one sees easily that it
is positive from all - ? \Gamma1. Therefore it is also eventually
positive. However, it is absolutely positive only from - 0
because its second derivative (6x) is non-negative only for
non-negative values of x. As it is absolutely positive from
some - it is also eventually absolutely positive. 2
Example 2.2 Let 1. It is easy to see that
the polynomial P is positive from every - and therefore it
is eventually positive. On the contrary, it is not absolutely
positive from any -, because the derivative
always negative. Consequently it is not eventually absolutely
positive. 2
1 The notion of eventual positiveness was first introduced by Lankford
[24].
Naturally, we are interested in the relations among the
properties. Let P - , EP, AP - , EAP respectively stand for
positiveness from -, eventual positiveness, absolute positiveness
from - and eventual absolute positiveness. The following
proposition shows the relations among them.
Proposition 2.1 The following diagram describes the implication
relation among the different kinds of positiveness prop-
erty. An arrow goes from A to B if A implies B.
@
@
@
@R
\Gamma\Psi
@
@
@
@R
\Gamma\Psi
Proof.
The proofs of all indicated implications follow easily from the
definition. It is equally easy to see that the indicated implications
are the only implications valid for the given properties.
We can use polynomials from Examples 2.1 and 2.2 as counter
examples to other implications. 2
3 Methods for Absolute Positive-
ness
We start with presenting a necessary and sufficient condition
for absolute positiveness of a polynomial. The theorem will
be used in several following subsections and will form a base
for the new method we present later.
From now on, let oe - stand for "shifting" by -. Precisely,
oe - (P ) is the polynomial obtained from P by replacing each x i
with polynomial P can be rewritten
as
e c e x e where the exponent vectors (e) are distinct and
(distributed representation). We will call those c e 's
coefficients of P .
Theorem 3.1 Let P be a non-zero polynomial. Then P is
absolutely positive from - iff every coefficient of the polynomial
oe positive and the constant term is non-zero. 2
Proof.
First note that since P is a non-zero polynomial, oe - (P ) is
also a non-zero polynomial.
Assume that every coefficient of oe - (P ) is positive and the
constant term is non-zero. Then obviously oe - (P ) is positive
from 0. Note that the graph of P is obtained by shifting that
of oe - (P ) by - to the positive direction. Thus the polynomial
P is positive from -.
Let @ be an arbitrary but fixed partial differentiation operator
as @ e 1 +\Delta\Delta\Delta+e n
. Since the polynomial oe - (P ) has all co-efficients
positive, the polynomial @(oe - (P )) also has all coefficients
positive. Therefore @(oe - (P )) is non-negative from 0.
Observing that differentiation and shifting commutes, that is,
we see that oe - (@(P )) is non-negative
from 0. Note again that the graph of @(P ) is obtained by
shifting that of oe - (@(P )) by - to the positive direction. Thus
@(P ) is non-negative from -. Therefore P is absolutely positive
from -.
We can use the same arguments as in the first part of the
proof. The assumption of the polynomial P being absolutely
positive from - is then equivalent to the fact of polynomial
oe being absolutely positive from 0. Therefore we want to
prove that if the polynomial oe - (P ) is absolutely positive from
then it has all coefficients positive and non-zero constant
term. The proof will be done by contradiction.
Assume the constant term of oe - (P ) is zero. Then the value
of oe - (P ) at 0 is 0, contradicting the fact that oe - (P ) is absolutely
positive from 0. Assume there exists a term with
negative coefficient in oe - (P ). Then there exists a derivative 2
of oe - (P ) with a negative constant term (it is the derivative
eliminating all variables of the term with the negative coef-
ficient). The derivative is negative at 0, contradicting the
absolute positiveness of oe - (P ). 2
3.1 Partial Derivative Method
Giesl [11] gave a partial method for testing the positiveness,
by extending the idea of Lankford [24]. The method is based
on the following implications (called differentiation rules):
It might be the 0-th order derivative, that is, the polynomial oe - (P )
itself.
The method, starting from the sentence
repeatedly applies the implications (backward) until it results
in a conjunction of inequalities among numbers. If all those
numeric inequalities are satisfied, then the method outputs
'yes' (P is positive from -). If not, it outputs `don't know'.
Since it is a partial method, a question naturally arises:
How much partial is the method? In other words, we want
to know the proper subset of the set of all positive polynomials
that are decided by the method. As the reader might
have guessed (from the fact we put the method into the section
about absolute positiveness), the method is a decision
procedure for absolute positiveness (Theorem 3.2).
The rest of the subsection is devoted to the proving the
theorem. For this we start with a lemma which will simplify
the proof of the theorem.
Lemma 3.1 The partial derivative method outputs 'yes' on
an input P and - iff it outputs 'yes' on the input oe - (P ) and 0.Proof.
Suppose that by applying Rule-1 on an inequality Q
we generate two new inequalities Q
is easy to see, from the commutativity of shifting and differ-
entiation/evaluation, that by applying the same rule on the
inequality oe - (Q) ? 0, we will generate two new inequalities
The same holds for applying
Rule-2 on Q - 0.
The computation of the partial derivative method on input
ends in a set A of numerical inequalities. Following
the observation above, we easily see that applying the
same computation path on oe - (P ) and 0 results in the set A 0
of numerical inequalities, which are obtained by shifting the
inequalities in A. But then shifting a numerical inequality results
in the same inequality. Thus we conclude that
and so their truths are the same. 2
Now we can prove the exact power of the partial derivative
method.
Theorem 3.2 (Power of Partial Derivative Method)
The partial derivative method outputs 'yes' for an input P
and - iff the polynomial P is absolutely positive from -.
Proof.
Assume the partial derivative method outputs 'yes' on P
and -, it means P is positive from -. From Lemma 3.1 we
know it also outputs 'yes' on oe - (P ) and 0, and oe - (P ) is positive
from 0.
Now we will show, by contradiction, that oe - (P ) has all co-efficients
positive and non-zero constant term. Assume that
the polynomial oe - (P ) has either a negative monomial or constant
term equal to zero.
We start with the case when the constant term is zero.
Then we know oe - (P ) is not positive at 0 and it contradicts
the positiveness of oe - (P ) from 0.
Next we assume that a monomial in oe - (P ) has a negative
coefficient. We would like to show that the partial derivative
method does not prove the positiveness of oe - (P ) from 0. For
this, let us first observe the following. Let Q be a polynomial
in an inequality and let M be a monomial in Q with negative
coefficient. We claim that after applying a rule (Rule-1
or Rule-2) one of the resulting polynomials still contains a
monomial with negative coefficient. To see this, let x i be the
variable on which the rule is applied. Let Q 1 be the polynomial
resulting from substituting 0 into x i in Q and let Q 2
be the polynomial resulting from differentiating Q in x i . Assume
that x i occurs in M . Then the monomial @M
occurs in
since it cannot be canceled by other monomials. Further,
the monomial obviously has a negative coefficient. Assume
that x i does not occur in M . Then M occurs in Q 1 because
there cannot be any cancelation among different monomials
after substituting 0 into x i .
Repeated applications of the rules results in a set of numerical
inequalities. From the observation above we know that
at least one of the inequalities must be of the form a - 0 or
a ? 0 where a ! 0. Thus the partial derivation method does
not output 'yes' for oe - (P ) and 0. Contradiction.
Thus, we showed that oe - (P ) has all coefficients positive
and the constant term is non-zero. From Theorem 3.1 we
conclude that P is absolutely positive from -.
Assume P is absolutely positive from -. From Theorem
3.1 we know that oe - (P ) has all coefficients positive and non-zero
constant term. We will show that the method proves the
positiveness of oe - (P ) from 0.
In the final set of numerical inequalities there is only one
strict inequality (resulting from the application of the first
part of Rule-1). Note that the left hand side of the strict
numerical inequality is the constant term of oe - (P ). Because
the constant term is positive, the strict inequality is true.
All other numerical inequalities are non-strict. Note that the
applications of the rules cannot create a negative term. Thus
all the numbers in the non-strict numerical inequalities are
non-negative. Hence, all non-strict inequalities are true,
We showed that the initial inequality is transformed into a
valid set of numerical inequalities. Thus the partial derivative
method outputs 'yes' for the input oe - (P ) and 0. From Lemma
3.1 we conclude that the method outputs 'yes' also on the
input P and -. 2
3.2 Merging Method
The method was first described by Ben Cherifa and Lescanne
[4]. Steinbach [31, 32] increased the power of the method and
removed some restriction imposed by the original version.
The main idea of this method is deciding the positiveness of
polynomial P by finding a sequence of polynomials
such that
The positiveness of polynomial Pn is checked with the "all
coefficients are positive" rule. Of course this rule decides the
positiveness of a polynomial only from positive -.
The transformation of P i into P i+1 is done in two main
steps. Firstly, a pair of monomials, one having negative
and one positive coefficient are chosen. Secondly, the two
monomials are merged into a new monomial. The procedures
CHOOSE and CHANGE are the essential part of the method
and for a detailed discussion on them we refer to Section 5.3
of [32].
We are again interested in the relation between the method
and the positiveness properties we have defined. Giesl in [11]
proved the following facts regarding the comparison of his
method with the merging methods of Steinbach [32] and Ben
Cherifa and Lescanne [4].
Theorem 3.3 (Power of Merging Method (Giesl 95))
ffl If the merging method can prove a polynomial P to be
positive from -, then the partial derivative method can
do so as well.
ffl If partial derivative method can prove P to be positive
from -, then there exists a - 0 - such that the merging
method can prove P to be positive from - 0 . 2
Giesl also gives an example for the second observation,
where the merging method cannot prove positiveness of P
from - though the partial derivative method does. Therefore
the partial derivative method is stronger than the merging
method.
We see that the merging method besides being a partial
method for deciding positiveness of polynomial also only partially
decides absolute positiveness of polynomial. Naturally
one can be again interested in an exact characterization of
the power of the method. We do not know a simple characterization
for the polynomials positiveness of which is decided
by the merging method and leave it as an open problem.
3.3 Shifting Method (New)
We present a new and simple method for checking absolute
positiveness of polynomial. The method is based on Theorem
3.1 which gives the necessary and sufficient condition
for absolute positiveness. From the theorem the method is
obvious.
1. Shift P by -, that is, compute the polynomial P 0 such
that
2. If all the coefficients of P 0 are positive and the constant
term is non-zero then output 'yes', else output `no'.
Example 3.1 Check if the following polynomial 3 P is absolutely
positive from 2:
We apply the shifting method. For this, we first shift P
by 2,
Next, we find that all the coefficients are positive and the
constant term is non-zero (8), thus we conclude that P is
absolutely positive from 2. 2.
3.4 Comparing the Complexity of the Method

Since all the above methods are of equal testing power, we are
interested in complexity differences. In the following theorem,
we recall the analysis of the merging method by Steinbach [31]
and the analysis of the partial derivate method by Giesl [11].
Then we give the analysis of the shifting method. Below we
assume that every arithmetic operation takes a constant time.
Theorem 3.4 (Computing Times) The computing times
for worst cases are bounded from above as follows:
ffl Partial Derivative Method O(d n ')
ffl Merging Method O(' ' )
Shifting Method O(d n ')
where
n the number of variables,
' the number of monomials,
d the maximum of the degrees in each variable. 2
Proof.
Partial Derivative Method: In [11], Giesl gave the following
derivation: Let T (n; d; ') be the computing time for
an input characterized by n, d and '. First we compute
store the results for the later use. This
3 This polynomial arises from a certain polynomial interpretation of
the term rewrite system for associativity and endomorphism [11].
takes O(d). Let S(n; d; ') be the computing time for the re-
maing part (successive elimination of variables). The elimination
of one variable involves the following: d partial differen-
tiations, where each differentiation takes O('), d evaluations,
where each evaluation takes O('), using the stored values of
the powers of -. Further this process produces (d
polynomials in variables to work on. Thus, we get the
following recurrence formula:
By solving the recurrence formula, we see that S(n; d;
O(d n '): Thus, T (n; d;
Merging Method: Steinbach [31] proved the following bound
where s is the number of the positive monomials and t is
the number of the negative monomials. He then, using the
formula of Stirling(s! aproximates to ( s
2-s), simplified it
to
t, it can be rewritten in terms of ' as
Shifting Method: First, we need to shift each monomial. 4 For
this, we need to shift the power of each variable, that takes
O(d 2 ) for each variable. Then we need to multiply out the
shifted powers, that takes O(d n ). Thus, shifting each monomial
takes O(d shifting all the
monomials takes O(d n '). Next, we need to sum all the shifted
monomials, which takes O(d n '). Thus, all together, the shifting
method takes O(d
These are asymptotic results (with the big O's), and thus
one should be careful not to draw any too definitive conclusion
about their performances on small/moderate size inputs,
which often arise in application domains.
4 Methods for Eventual Absolute
Positiveness
Now we shift our focus to the (known and new) methods for
testing eventual absolute positiveness.
4.1 Partial Derivative Method (eventual
positiveness version)
We have already mentioned the Lankford's method [24] which
was suggested as a partial decision method for eventual pos-
itiveness. Let P be a non-constant polynomial. Then the
4 Actually, there are practically better ways, such as Honer evaluation
or synthetic division, etc. But they do not seem to give a better bound,
thus we will stick to the brute-force way since it easier to analyze.
method is based on a simple fact that if every non-vanishing
first order partial derivative of P is eventually positive then
P is eventually positive. We can apply the same reasoning
on the partial derivatives repeatedly until all polynomials are
numbers. If all of them are positive then the input polynomial
is eventually positive. Otherwise, we don't know.
Since it is a partial method, a question again naturally
arises: How much partial is the method? We will answer this
question by following the similar approach that we already
used in studying the power of the partial methods for testing
positiveness (Section 3).
Lemma 4.1 The partial derivative method (Lankford) outputs
'yes' on an input P iff it outputs `yes' on the input
Proof.
Suppose that by applying differentiation on an inequality
another inequality Q   ? 0 . It is easy to
see, from the commutativity of shifting and differentiation,
that by differentiation in the same variable on the inequality
oe 0, we will generate a new inequality oe - (Q
The computation of the partial derivative method (Lank-
ford) on input P ends in a set A of numerical inequalities.
Following the observation above, we easily see that applying
the same computation path on oe - (P ) results in the set A 0
of numerical inequalities, which are obtained by shifting the
inequalities in A. But then shifting a numerical inequality results
in the same inequality. Thus we conclude that
and so their truths are the same. 2
Now we can prove the exact power of the partial derivative
method (Lankford).
Theorem 4.1 (Power of Partial Derivative Method)
The partial derivative method (Lankford) outputs 'yes' on input
P iff the polynomial P is eventually absolutely positive.
Proof.
Assume that the partial derivative method outputs 'yes' on
an input P. We need to show that P is eventually absolutely
positive. From the assumption, we know that P is eventually
positive. Therefore there exists a - 0 such that P is positive
from - 0 . Further from the details of the execution of
the partial derivative method, we observe that every partial
derivative of P is either a zero polynomial or eventually posi-
tive. Thus for every non-zero partial derivative P   of P , there
exists a -   such that P   is positive (thus non-negative) from
-   . Now let - be the maximum of - 0 and all -   's. Then, P is
absolutely positive from -. Thus, P is eventually absolutely
positive.
Assume that the polynomialP is eventually absolutely pos-
itive. We need to show that the partial derivative method
outputs 'yes' on this input. Since P is eventually absolutely
positive, there exists - such that P is absolutely positive
from -. Then, oe - (P ) is absolutely positive from 0: By Theorem
3.1, we see that all coefficients of oe - (P ) are positive and
the constant term is non-zero. Note that the partial derivative
method does not introduce negative terms during the
execution on the input oe - (P ). Thus, the method will output
'yes' on the input oe - (P ). By Lemma 4.1, we conclude that
the method will also output 'yes' on the input P . 2
4.2 Dominating Monomials Method (New)
Now we propose a new and more efficient method for testing
eventual absolute positiveness of polynomial. First note that
the eventual (absolute) positiveness of a univariate polynomial
can be trivially decided by checking whether its leading
coefficient is positive. We generalize the observation to multivariate
case. For this, the notion of leading monomial must
be generalized. The new method is essentially based on one
such generalization (which we call dominating monomial).
Definition 4.1 (Dominating monomial) We say that a
monomial s dominates a monomial m. iff m divides s and m
is different from s. We say that s is a dominating monomial
of P iff there is no monomial in P dominating s. 2
Example 4.1 Let us consider polynomials
There are three dominating
monomials in P there are two in Q
univariate polynomials there is only one dominating
monomial - the leading monomial. 2
Now we are ready to describe the new method:
1. Collect all the dominating monomials of P .
2. If their coefficients are all positive, then output 'yes', else
output 'no'.
The proof of the correctness of the method will be given later
(Theorem 4.2). Let us first get familiar with the method by
working on several small examples.
Example 4.2 Check if the following polynomial is eventually
absolutely positive:
One sees immediately that the only dominating monomial is
xyz. Since its coefficient is positive, the polynomial is eventually
absolutely positive.
Let us try another:
One sees that the dominating monomials are x 4 , 2xy 3 and
y. Since their coefficients are positive, the polynomial is
eventually absolutely positive.
Let us try still another (obtained by changing signs in the
previous example),
Again, the dominating monomials are x 4 , \Gamma2xy 3 and 3x 2 y.
Since one coefficient is negative, the polynomial is not eventually
absolutely positive. 2
Remark: Note that one can use the dominating monomial
method as a quick pre-pruning method while testing absolute
positiveness: Suppose that we would like to check whether a
polynomialP is absolutely positive from -. We first apply the
(very cheap) dominating monomial method. If it turns out
that P is not eventually absolutely positive, then we do not
need to carry out the more expensive absolute positiveness
check, since we already know that the answer is 'no'. 2
The remaining of this section is devoted to proving the correctness
of the dominating monomial method (Theorem 4.2).
We begin with a lemma which "justifies" the notion of dominating
monomial.
Lemma 4.2 The set of dominating monomials of P and that
of oe - (P ) are the same. 2
Proof.
Let m be a dominating monomial of P . We need to show
that m is a dominating monomial of oe - (P ). We prove it by
contradiction. Assume that m is not a dominating monomial
of oe - (P ). There are two cases:
ffl m is not a monomial of oe - (P )
ffl m is a monomial of oe - (P ), but not dominating.
Case 1: m is not a monomial of oe - (P ).
Note that m is a monomial of oe - (m). Since m is not a monomial
in the oe - (P ), there must be a monomial q (different
from m) in P such that there is a monomial q 0 of oe - (q), that
has the same degree vector as m. Every monomial of oe - (q)
divides q, in particular, q 0 divides q. Since m and q 0 have the
same degree vector, m divides q. Recall that m is different
from q, thus q dominates m. Therefore m is not a dominating
monomial of P . Contradiction.
Case 2: m is a monomial of oe - (P ), but not dominating.
Thus there is a monomial q 0 in oe - (P ) that dominates m.
Hence there is a monomial q of P such that there is a monomial
q   of oe - (q), that has the same degree vector as q 0 . Every
monomial of oe - (q) divides q, in particular, q   divides q. Since
q 0 and q   have the same degree vector, q 0 divides q. Recall
that q 0 dominates m. Thus m divides q 0 and they are differ-
ent. Since q 0 divides q, we see that m divides q and they are
different. Thus q dominates m, and m is not a dominating
monomial of P . Contradiction.
Thus m must be a dominating monomial of oe - (P ).
Let m be a dominating monomial of oe - (P ). We need to show
that m is a dominating monomial of P . This can be done in
the exactly same way as before, since
The following theorem relates the notions of eventual absolute
positiveness and dominating monomials, proving the
correctness of the dominating monomial method.
Theorem 4.2 A polynomial P is eventually absolutely positive
iff the coefficients of all dominating monomials of P are
positive. 2
Proof.
Assume that P is eventually absolutely positive. We show
that the coefficients of all dominating monomials are positive.
From eventual absolute positiveness of P we know that
there exists a - such that P is absolutely positive from -.
From Theorem 3.1 we know that oe - (P ) must have all coefficients
positive and non-zero constant term. Therefore also all
coefficients of the dominating monomials of oe - (P ) are posi-
tive. By Lemma 4.2, the set of all dominating monomials of P
is the same as that of oe - (P ). Thus all dominating monomials
of P have positive coefficients.
Assume that the coefficients of all dominating monomials
of P are positive. We need to prove that P is eventually
absolutely positive.
We begin by taking care of a trivial case where all other
(non-dominating) monomials of the polynomial P also have
positive coefficients. It is easily seen, that such a polynomial
is absolutely positive from any - ? 0 and therefore eventually
absolutely positive.
Therefore assume that there exists at least one monomial
with negative coefficient (negative monomial) in P . We partition
the polynomial P into a sum of several sub-polynomials
such that one of the sub-polynomials consists of all positive
non-dominating monomials of P and that each of the remaining
sub-polynomials consists of exactly one dominatingmono-
mial of P (which we will call the leading monomial) and some
(possible none) monomials dominated by the leading monomial
(thus, these monomials are all negative.)
Obviously, if each sub-polynomial is eventually absolutely
positive, so is P . Thus, from now on, we will show that each
sub-polynomial is eventually absolutely positive. Let Q be
one of sub-polynomials.
If Q consists of only positive monomials, it is absolutely
positive from any - ? 0 and therefore eventually absolutely
positive. Thus, from now on assume that Q has at least one
negative monomial.
First we will prove that Q is eventually positive. Let
We claim that Q is positive
from - 5 . To see this, first recall that x m dominates x e i for
every i. Hence, we see immediately that for every i and for
every x -,
a
ab
a
Thus, we see that for every x -, Q
Q is eventually positive.
It remains to show that for every partial derivative Q   of
there exists a -   such that Q   is non-negative from -   .
It is obvious that every vanishing derivative is non-negative
from any -. Thus from now on let Q   be a non-vanishing
partial derivative of the polynomial Q. Note that Q   again
consists of a positive leading monomial (a derivative of the
leading monomial of Q) and some (possibly none) negative
monomials dominated by the leading monomial. Thus, by
applying the idea used for showing the eventual positiveness
of Q, we see that there exists a -   such that Q   is positive
from -   and therefore it is also non-negative from -   . As
there are only finitely many derivatives, we can take -m the
maximum of the bound for positiveness of the polynomial Q
and the bounds for non-negativeness of its derivatives. Then
the polynomial Q is absolutely positive from -m . Thus Q
is eventually absolutely positive. We showed that every sub-
polynomial of P is eventually absolutely positive. Therefore
P is also eventually absolutely positive. 2
4.3 Comparing the Complexity of the Method

We know that the partial derivative method (Lankford) and
the dominating monomial method have the same power.
The following theorem compares their asymptotic complexity
bounds.
Theorem 4.3 (Computing Times) The computing times
for worst cases are bounded from above as follows:
ffl Partial Derivative Method (Lankford) O(d n ')
Dominating Monomial Method O(n' log ')
where
n the number of variables,
' the number of monomials,
d the maximum of the degrees in each variable. 2
Proof.
5 Actually, one can find a much smaller value for - than the one
given here. But here we are only interested in proving the existence of
a bound, and thus any bound suffices.
The bound for the partial derivative method is immediate
from the fact that the method performs up to d n differentiations
of polynomials, where each differentiation takes O(').
The bound for the dominating monomial method is immediate
from the number of comparisons needed for sorting the
monomials according to the partial order defined by the domination
property, where each comparison takes O(n). 2
Conclusions
Several results were presented in the paper. We determined
how much partial the several known methods are for testing
positiveness of polynomials. After that we proposed simpler
and/or more efficient methods with equal power.
There are several directions for the future work. Regarding
the shifting method, one can use more sophisticated methods
for the shifting. For example, the number of monomials
necessary to be shifted can be significantly reduced by exploiting
the structure of the polynomial (not all monomials
have to be shifted, the shifting can be stopped after no negative
monomials can be generated, summing new monomials
need not be done for only positive monomials etc). Also, it
will be interesting/important to find out experimentally how
the new methods (shifting, dominating monomial) behave on
small/moderate size inputs.



--R

Algorithms for the Geometry of Semi-Algebraic Sets
Cylindrical algebraic decomposition II: An adjacency algorithm for the plane.
An adjacency algorithm for cylindrical algebraic decompositions of three-dimensional space
Termination of rewriting systems by polynomial interpretations and its implementation.
Simplification of Truth Invariant CAD's and Solution Formula Construction.
Improved algorithms for sign and existential quantifier elimination.
Quantifier elimination for the elementary theory of real closed fields by cylindrical algebraic decomposition.
Quantifier elimination by cylindrical algebraic decomposition - 20 years of progress
Partial cylindrical algebraic decomposition for quantifier elimination.
Termination of rewriting.
Generating polynomial orderings for termination proofs.
A combinatorial algorithm solving some quantiifier elimination problems.
The complexity of deciding Tarski algebra.
On the complexity of semialgebraic sets.
An improvement of the projection operator in cylindrical algebraic decomposition.
Improvements in CAD-based Quantifier Elim- ination
Simple solution formula construction in cylindrical algebraic decomposition based quantifier elimina- tion
Heuristic search and pruning in polynomial constraint satisfaction.
Parallelization of quantifier elimination on a workstation network.
Quantifier elimination for formulas constrained by quadratic equations via slope resultants.
Approximate quantifier elimination.
Generic quantifier elimination.
Approximate Quantifier Elimination.
A finite termination algorithm.
Applying linear quantifier elimination.
An Improved Projection Operator for Cylindrical Algebraic Decomposition.
An improved projection operator for cylindrical algebraic decomposition.
Solving polynomial strict inequalities using cylindrical algebraic decomposition.
On the computational complexity and geometry of the first-order theory of the reals
A parallel implementation of the cylindrical algebraic decomposition algorithm.
Proving Polynomials Positive.
Termination of rewriting.
The completness of elementary algebra and geometry.
A Decision Method for Elementary Algebra and Geometry.
The complexity of linear problems in fields.
Quantifier elimination for real algebra - the cubic case
--TR

--CTR
Salvador Lucas, Practical use of polynomials over the reals in proofs of termination, Proceedings of the 9th ACM SIGPLAN international symposium on Principles and practice of declarative programming, July 14-16, 2007, Wroclaw, Poland
Jrgen Giesl , Aart Middeldorp, Transformation techniques for context-sensitive rewrite systems, Journal of Functional Programming, v.14 n.4, p.379-427, July 2004
Jrgen Giesl , Ren Thiemann , Peter Schneider-Kamp , Stephan Falke, Mechanizing and Improving Dependency Pairs, Journal of Automated Reasoning, v.37 n.3, p.155-203, October   2006
Nao Hirokawa , Aart Middeldorp, Tyrolean termination tool: Techniques and features, Information and Computation, v.205 n.4, p.474-511, April, 2007

extracted:['file assignment' 'feedback controls' 'feature weights'
 'fault-tolerant software systems' 'fault-tolerant routing algorithm'
 'fault-tolerant routing' 'fault-tolerant algorithms' 'flow analysis'
 'zero storage biometric authentication' 'computational complexity']
marked:['positiveness of polynomials', 'term rewrite systems', 'termination proofs']
--T
The Design of the CADE-13 ATP System Competition.
--A
Running a competition for automated theorem proving (ATP) systems is a difficult and arguable venture. However, the potential benefits of such an event by far outweigh the controversial aspects. The motivations for running the CADE-13 ATP System Competition were to contribute to the evaluation of ATP systems, to stimulate ATP research and system development, and to expose ATP systems to researchers both within and outside the ATP community. This article identifies and discusses the issues that determine the nature of such a competition. Choices and motivated decisions for the CADE-13 competition, with respect to the issues, are given.
--B
Introduction
Running a competition for Automated Theorem Proving (ATP) systems 1 is a difficult and
arguable venture. The reasons for this are that existing ATP systems are based on different
logics, are designed for different types of reasoning, require different amounts of user interaction,
have different input formats, and may run on specialized hardware. Moreover, there is no clear
work profile with respect to which ATP systems should be evaluated. However, a competition
will make a significant contribution to the following important needs:
ffl to evaluate the relative capabilities of ATP systems,
ffl to stimulate ATP research in general,
ffl to stimulate ATP research towards autonomous systems,
ffl to provide motivation for implementing and fixing systems,
ffl to provide an inspiring environment for personal interaction between ATP researchers,
ffl to expose ATP systems to researchers both within and outside the ATP community.
In other disciplines competitions are regularly used to stimulate research and development.
The annual ACM computer chess championship [New94] is very well known and attracts much
interest. Participants in the chess championship are clearly motivated to improve their pro-
grams. Programming competitions are also popular [CRRP90, AKK93, Dem96], and encourage
programmers to improve their skills. Competitions in machine learning [MMPS94], algorithm
implementation [CIJ + 94], and other disciplines have all stimulated effort in their respective
fields. In ATP there have already been competitions at the propositional [BKB92] and 1st
order [Ove93] levels, which have encouraged researchers to improve their ATP systems. As an
example, Otter's autonomous mode [McC94] resulted in part from the competition described
in [Ove93].
As well as being of interest in its own right, an ATP system competition, or at least an
examination of the issues involved in a competition, will provide insight into the more general
notion of ATP system evaluation. For a long time the testing and evaluation of ATP systems has
been very ad hoc. Results being published seldom provide an accurate reflection of the intrinsic
power of the ATP system being considered. Inadequate evaluation has the potential to damage
a field of research; bad ideas may appear to be good and are thus adopted and perpetuated 2 ,
while good ideas may appear bad and are hence discarded. A first step towards improving ATP
system evaluation has been the release of the TPTP problem library [SSY94, SS96]. A common
library of problems is a necessary, but not sufficient, tool for meaningful system evaluation.
There are also other issues that need careful consideration. This examination of the issues
surrounding an ATP system competition goes a fair way towards capturing the requirements
for meaningful ATP system evaluation. It should be noted that using benchmarks and test
beds (such as the TPTP problem library) in system evaluation requires special care (see also
[HPC93]).
In order to benefit fully from a competition, a thoroughly organized event, with unambiguous
and motivated rules, is necessary. One approach to ATP system competitions, as was taken in
We use the term "ATP system" to refer to the functional unity represented by the implementation of a
reasoning calculus and its control, i.e., the implementation of a proof procedure.
Geoff thinks that paramodulation is an example of this in ATP.
the competition at CADE-11 [Ove93], is to run very specialized competitions with specifically
selected problems. While this allows a detailed analysis and comparison of the proofs obtained
by ATP systems, it does not provide a realistic comparison of the ATP systems in terms of
their general usefulness. In [Ove93] Overbeek notes that the ATP community had "never [been]
able to formulate an acceptable mechanism for comparing different systems". In order for a
comparison of different ATP systems to make sense, it is necessary that all the systems should
be attempting to capture a common notion of "truth", as is described in the Realist viewpoint
in [Pel91]. Given this commonality across all systems, we believe that it is possible to set
out rules for a competition that can determine a winner, relative to some clearly specified
constraints. For some issues relevant to an ATP competition, inevitable constraints emerge.
For other issues there are several choices, and a decision has to be made for the competition.
The arising issues, choices, and decisions are described in this paper.
Although this paper focuses on the issues relevant to experimental comparison of ATP
systems, it is important to acknowledge the contribution of analytic approaches, e.g., [Let93,
Dun94, Pla94]. These approaches investigate metrics such as search space duplication, search
tree size, Herbrand multiplicity, number of variables to bind, and connectivity. At the propositional
level analysis can be reasonably accurate, and is thus important for that limited situation.
At the 1st order level the analysis is significantly more difficult, and results obtained so far are
insufficient. Complete analysis of search guidance at the 1st order level is of course impossible
(for otherwise 1st order logic would be decidable!). It is for these reasons that we have focused
on experimental comparison of ATP systems; in this way meaningful judgements can be made.
Competition Issues
There are four types of issues (and resultant choices and decisions) relevant to the design of an
ATP system competition:
1. Issues based on the fundamental nature of ATP.
2. Issues associated with common practice in ATP research.
3. Issues arising from the application of ATP systems.
4. Issues directly related to the competition.
An overview of the issues that are discussed in the following sections is given in Table 1.
The table also shows the two types a particular issue most belongs to, according to the four
types presented above.
Type Issue
ATP System Properties
2,3 Degree of Automatization : How much user interaction is allowed?
2,3 Soundness and Completeness: How do soundness and completeness affect
evaluation?
2,3 Proofs and Satisfiability Checking: Should the systems be expected to find
proofs or models?
2,3 Proof Objects : Are explicit proof objects required?
1,2 Monolithic and Compositional Systems: Does the system structure matter?
Problem Presentation
1,2 Scope of Logic : Which logics (e.g., Classical, Modal) should be allowed?
1,2 Built-in Theories : Are built-in theories (e.g., equality) allowed?
2,3 Input Language : What input language (e.g., CNF, FOF) should be used?
3 Clause Type Information : Is usage of such information acceptible?
Clause and Literal Orderings : How can bias be avoided?
Problem Selection
2,4 Problem Source : Where should the problems come from?
4 Problem Difficulty : How difficult should the problems be?
2,4 Problem Axiomatization : What about biases in problem axiomatizations?
4 Number of Problems : How many problems should be used?
Resource Limits
- Should a runtime limit apply per problem or for all problems altogether?
What runtime limit should be imposed?
4 Hardware and Software resources : What hardware and software should be
available for the competition?
System Evaluation
Should a distinguished "winner" be determined?
2,3 Performance Metrics : What metrics should be used for evaluation?
3,4 Ranking Scheme : How should performance metrics be combined for ranking?

Table

1: Relevant issues for an ATP system competition design.
3 ATP System Properties
3.1 Degree of Automation
From a user's viewpoint, ATP systems are of two types : fully automatic systems 3 and interactive
systems. Currently, and presumably in the future, the most important application of
fully automatic ATP systems is as embedded systems in more complex reasoning environments,
where they serve as core inference engines. This setting is shown in Figure 1. The environment
may call on the fully automatic ATP system to solve subtasks within the operation of the
overall system. The subtasks are thus generated by the environment. Examples of fully au-
3 Astrachan and Loveland [AL94] define three types of fully automated deduction (F.A.D.) systems . "pure
F.A.D. systems, where only the problem can be entered ., the strong F.A.D. systems, where parameters can
be set only once, and the weak F.A.D. system, where many experiments with parameters are permitted, but
each adjustment requires a restart from the beginning of the problem. For the purposes of this discussion,
automatic ATP systems are the pure and the strong F.A.D. systems; weak F.A.D. systems are considered to be
interactive.
tomatic systems are Otter [McC94], SETHEO [LSBB92], SATCHMO [SGar], the CLIN series
[LP92, CP94, Ale95], and SPASS [WGRar].
Problem Solution
Language
Transformers Interface
User
Management
Data
System
Control
(fully automatic)
ATP System
atomic
proof
requests
proofs/
models

Figure

1: The embedding of an ATP system in some reasoning environment.
Interactive systems find application in hardware and software verification research [BM90,
HRS90, ORS92, WG92], the formalization of informal mathematical proofs [FGT90, Pau90],
the teaching of logic [Por94], and as tools of mathematical and meta-mathematical research
86]. Examples of interactive systems
NQTHM [BM90], and NUPRL [CAB systems typically embed some
form of a fully automatic ATP system as a subsystem 4 , and are an example of the reasoning
environments mentioned above. At any point the user of an interactive system may ask that
some (sub)problem be submitted to the fully automatic component.
Both fully automatic and interactive theorem proving systems are important. It is clear,
however, that the two types of system are very different. In terms of a competition, it would
not make sense to compare fully automatic systems with interactive systems; the qualities that
are required of the two types are largely distinct. In particular, an interactive system must
be judged largely on its ability to assist the user in the organization and tracking of complex
proof objects [PS94]. While for fully automatic systems the system runtime is the dominating
issue. It may be the case that there are ATP system evaluation criteria that are independent
of whether the systems are automatic or interactive. However, at this stage no such criteria are
clear. Thus the ATP competition can compare either fully automatic systems or interactive
systems, but the two types cannot be compared with each other.
Many of the assessment criteria for a competition between interactive theorem proving
systems would be very subjective, as individual user abilities and tastes would necessarily factor
into the comparison. There are some criteria, e.g., "user-friendliness", that do not appear to
have any objective measure at all. In contrast, the criteria for comparing fully automatic
ATP systems are mostly objective, as described in this paper. Also, recalling that interactive
systems typically embed a fully automatic ATP system as a subsystem, a comparison of the
fully automatic subsystems of interactive systems should be part of a comparison of interactive
systems 5 . Decision : The ATP competition will be between fully automatic ATP systems. No
4 At the current stage, most interactive systems contain a quite rudimentary and simple theorem proving part,
while (justifiably so) most effort is spent on the development of adequate logics, proof tactics, and interfaces.
5 Today, a direct comparison of the fully automatic subsystems would be difficult, because due to their
human interaction will be allowed.
In the rest of this paper attention is limited to fully automatic ATP systems, and they
will be referred to simply as ATP systems. Note that the decision to focus on fully automatic
systems does not exclude the possibility of a competition between semi-automated or interactive
systems. Indeed, such a competition would be of interest, if the difficulties involved can be
adequately resolved.
A consequence of the limitation to fully automatic systems is that the competition will
not appreciate system properties which relate to human computer interaction, and therefore
will not necessarily determine the best "human assistant". Rather, the competition will assess
the pure reasoning capabilities, and nothing else 6 . The intention is to foster automation. In
particular, the competition aims to identify powerful general purpose deduction procedures,
and to encourage automatic problem analysis and system configuration. The development of
deduction procedures has been a focus of ATP research since its inception, and is certainly a
mature enough area to evaluate. On the other hand, automatic problem analysis and system
configuration are underdeveloped. In various research groups there is much knowledge about
how to analyze problems, and how to configure that group's ATP system. This knowledge is
exploited when the ATP system is evaluated, by having the knowledgeable person manually
configure the system for each problem. Due to this common practice of manual configuration,
the effort to capture and implement problem analysis and configuration techniques is rarely
taken. Implemented systems often feature strong reasoning components but have very poor
automatic control. This makes it difficult for other users of such systems to get the best
possible performance from the systems, and makes the systems unsuitable for use as embedded
systems. Hopefully this ATP competition will encourage researchers to formalize and capture
their analysis and configuration skills in their ATP systems.
3.2 System Soundness and Completeness
Traditionally, ATP systems are designed to be deductively sound and complete: every answer
returned is deductively correct and if a solution exists it is eventually deduced. However, ATP
systems are usually too complex to formally verify either property. Thus systems are tested
experimentally to detect violations of these properties. While soundness can be assumed for
extensively tested systems (at least if a large number of the produced solutions have been
verified), completeness is more difficult to assess. The reason for this is that the inability to
find a solution is not necessarily a sign of incompleteness - the problem could simply be too hard
for the system. In fact, due to the finite amount of resources allocated to any particular ATP
system run, every search for a solution has limited completeness, independent of the system's
theoretical completeness. A special case of incompleteness is a bugged ATP system, which
crashes on some problems. The question then arises, how should unsound and/or incomplete
and/or bugged systems be treated in the ATP competition?
From a users point of view, there is little difference between not finding a solution due to
system incompleteness, not finding a solution due to a resource limit being reached, and not
finding a solution due to a system crash. In all cases the user learns nothing about the solution
to the problem. There are also positive aspects to each form of failure. If a resource limit is
reached then the user can optimistically increase the resources available (although this is futile
simplicity no modular system design is used and an isolated test is not easily possible.
6 But note: the success of ATP systems as automated reasoning assistants will depend on their usability by
non-experts. For high usability the dependence on user interaction, in terms of low level guidance, needs to be
minimized. This argues that even in interactive systems less (low level) interaction is better.
if the resource limit was reached due to incompleteness, and further, the supply of resources
typically cannot deal with the exponential growth in demand for them). If a system terminates
quickly, due to incompleteness or a bug, then time is saved and there is hope of having the
system repaired. In practice a bugged or incomplete system may solve more problems within
some time limit than a bug free complete system, and therefore may be more useful. Decision:
In the ATP competition, systems must be sound but may be incomplete and may be bugged.
The soundness of competing systems will be assessed before the competition with specific test
problems. Systems found to be unsound will be excluded from the competition.
A property associated with soundness and completeness is that of stability. If an ATP
system finds a solution to a problem, then the system must be able to find that solution again
if required. ATP systems that simply 'get lucky' are of little use. Repeatability of testing is also
a standard requirement of the scientific method. Decision: For every solution found in the ATP
competition, the solution process must be reproducible by running the system again. If randomness
plays a role, reproducibility must still be ensured, e.g., by using pseudo-random numbers and
recording the seeds.
3.3 Proofs and Satisfiability Checking
There are two distinct classes of problems that may be presented to ATP systems. Firstly
there are problems that require the proof of a theorem, and secondly there are problems that
require satisfiability to be established (via the generation of a model). Both types of problem
are of interest. However, mixing proof performance and satisfiability assessment would blur the
interpretation of competition results. One reason for this blurring is that most ATP systems are
designed for one or the other purpose, but not both (exceptions are, for example, the RAMCEC
algorithm [BCP94] and hyper-linking [LP92]). As a result, a comparison of specialized systems
of the different types is arguable. In the future more dual purpose systems may be developed,
in which case it may not be necessary to divide the work profile for a competition, as is now
the case. Thus the work profile for a competition should not require both theorem proving and
model generation.
Historically, more emphasis has been placed on the ability to find proofs, and therefore this
is considered to be the more important issue (albeit this is not the case in some applications).
: The ATP competition will focus on theorem proving performance. In order to implement
this decision, only theorems will be used (see Section 5.1).
3.4 Proof Objects
Depending on its generality and purpose, there are various responses that an ATP system may
make when given a problem. A proof or a model may be returned, or only an assurance that
a proof or model exists may be given.
There are some ATP systems, e.g., completion based systems, that conserve resources by
not building a proof object during their search. Such systems merely retain enough information
to build the proof object later, if required. Other ATP systems, e.g. model generation systems,
are not able to build a proof object, and can only give an assurance that a proof exists.
There is no evidence that usage of an ATP system as an embedded system typically either
requires or does not require delivery of proof objects or models, and it is desirable to make
the ATP competition accessible to as many ATP systems as possible. Decision : The ATP
competition will not require ATP systems to return proof objects. The soundness testing (see
Section 3.2) will ensure that competing systems do not make invalid claims to have found a
proof. This decision evidently gives some bias towards those systems that do not build a proof
object. This bias is considered to be slight, and justified in terms of extending the scope of the
competition. However, the added functionality of ATP systems that do produce proof objects
should be acknowledged. Decision : In the presentation of the ATP competition results, it will be
noted which of the systems do produce complete proof objects.
3.5 Monolithic and Compositional Systems
Today, a large number of different calculi, inference rules, and proof procedures exist. Ideally,
an ATP system would have all known techniques available, and would, during a particular
solution search, switch back and forth between the most suitable techniques. First steps in this
direction are ATP systems which are formed from a collection of different proof procedures, and
which analyze the given problem in order to decide which procedure or inference rules to use.
A well known example of this type of system is Otter in its autonomous mode [McC94], which
enables or disables inference rules according to a check whether the problem is propositional,
whether it is Horn, and whether it contains equality and/or equality axioms.
An ATP system in which no components are chosen as alternatives to others, based on the
given problem's characteristics, is called a monolithic ATP system. An ATP system that runs
as one of several possible distinct monolithic systems, as determined by the given problem's
characteristics, is called a compositional system. Combining several monolithic systems in a
parallel manner also results in a compositional system. The definition does not limit monolithic
systems to a single calculus or rule of inference, but requires that any switching back and forth
between different calculi and/or rules is done in an integrated way, and occurs continuously
during the proof process 7 .
Compositional systems provide a valuable approach to building more versatile systems, and
compositional systems can be expected to outperform monolithic systems. Conversely, monolithic
systems can be improved by integrating more specialized components. Altogether, both
the construction of compositional systems and of monolithic systems are valuable. Research
into compositional systems will improve knowledge of which approach is best for what problems,
while research into monolithic systems will push the performance of uniform approaches (which
form the core of compositional systems) to the limit 8 . The results may one day lead to a type
of monolithic system which integrates all available techniques. Altogether, the competition
should not put either of these approaches at a disadvantage. Decision : In the ATP competition
there will be two categories: Open, which includes both types of systems, and Monolithic, which
includes only monolithic systems. Any controversy about which category a system belongs to
will be resolved by the competition panel (see Section 7.1).
4 Problem Presentation
4.1 Scope of Logic
Many "hot logics" in ATP research, such as higher order logics, modal logics, and temporal
logics, are very interesting. Sufficient research has been done to support a comparison of ATP
7 Rigid definitions of monolithic and compositional systems seem hardly possible. For any formal definition
made it seems that an ATP system that violates the intuitive notion can be contrived.
8 This issue can also be viewed from the evaluation viewpoint: the goal of evaluating a system is not simply
to see how well it performs, but also to learn why it achieves the observed performance. This is difficult for
compositional systems, and therefore it is desirable to obtain evaluations of their individual components as well.
\Phi \Phi \Phi \Phi \Phi \Phi
1st order
\Phi \Phi \Phi \Phi \Phi \Phi
propositional
non-propositional
no-equality
pure-equality
mixed
unit
non-unit

Figure

2: Classification of 1st order logic problems.
systems within many of these logics, as well as common classical logics. However, a comparison
of ATP systems across logics would be unlikely to make sense, for several reasons: the types of
output and expected performance vary from logic to logic, different logics may have different
semantics, and specific logics may be particularly designed for specific applications (making a
fair problem selection difficult). Thus the problems for the competition should include problems
from any one (but only one) logic.
The wide-spread use of 1st order logic in ATP suggests it as a starting point; specialized
competitions for other logics can still occur in the future. Furthermore, transformation techniques
(e.g., [Ker91, Ohl91]) allow the use of 1st order logic ATP systems for solving problems
in some other logics. Decision : The ATP competition will be restricted to theorems expressed in
classical 1st order logic.
Classical 1st order logic can be subdivided as shown in Figure 2.
The leaves in Figure 2 correspond to syntactically identifiable problem classes, and it is
trivial to determine which class a given problem belongs to. It would be possible to use
problems from all classes together, or to have different competition categories for the classes.
For some of the problem classes specialist ATP systems, that employ the appropriate tech-
niques, exist. Specialist systems can be expected to perform better on problems in their class
than systems not specialized for that class. A best overall ATP system can be built easily
by choosing the best system for each problem class, and using a trivial switch to invoke the
appropriate system. Trivial switch systems are of high interest to users, because they provide
better overall performance than any of their components. For the competition, however, there
is little interest in evaluating the actual switches within such systems, due to their simplicity.
Thus an overall performance evaluation of such systems is not desirable. Instead it is better
to evaluate the component systems separately, and from that conclude the overall performance
achievable by their combination. Decision : Specific problem classes of 1st order logic (according
to the classification above), for which specialist ATP systems exist, should be treated in separate
categories in the ATP competition.
Ideally the ATP competition would have separate categories for all of the problem classes.
However, merging or omitting some classes may still be appropriate.
The development of specialized ATP systems for propositional logic is largely separated from
ATP for non-propositional logic, and there are separate specialized competitions for evaluating
propositional systems 9 . There is no need to duplicate those competitions. Decision : There will
not be a category for propositional problems in the ATP competition.
In the original competition announcement the no-equality and mixed problem classes were
merged, for the following reasons. Firstly, systems that include specialized equality treatment
9 The next such event will be at the "International Competition and Symposium on Satisfiability Testing",
to be held in Beijing, China.
are typically competitive on no-equality problems, and therefore can be evaluated on such
problems. Secondly, a natural formulation of many interesting problems involves equality, and
therefore most state-of-the-art ATP systems include specialized equality treatment. These
motivations indicate that it is acceptable to keep these two classes merged, and it is desirable
for the competition to maintain its announced format. Decision : The no-equality and mixed
problem categories will be combined in the ATP competition.
Most pure equality problems known to the ATP community are written in Clause Normal
Form (see Section 4.3) as unit equality problems. There are specialized unit equality ATP
systems which cannot handle any other format of problem, so it is necessary to have a separate
category for these systems. Decision : There will be a separate category for unit equality problems
in the ATP competition. Non-unit pure equality systems are able to handle no-equality and
mixed problems by translating such problems to pure equality, and are able to compete with
no-equality and mixed systems (even if at a disadvantage, the extent of which is currently
unknown). The small number of commonly known non-unit pure equality problems seems
insufficient for a separate category. Decision : The no-equality, mixed, and non-unit pure equality
problem categories will be combined in the ATP competition 10 .
4.2 Built-in Theories
A theory can be built into an ATP system, rather than expressing it by a set of axioms (e.g.,
equality theory). This approach is chosen in order to provide specialized, and hopefully more
efficient, treatment of that theory. Systems capable of such specialized action should not be put
to a disadvantage by including axioms unnecessary for them. Decision : In the ATP competition,
theory axioms that have been built into an ATP system can be removed from the input to that ATP
system, using an automatic tool.
4.3 Input Language
The problems submitted to an ATP system may be expressed in full First Order Form
or possibly in some constrained subset of 1st order logic, such as clause normal form (CNF).
Therefore the ATP system must, in general, be able to deal with FOF. An examination of
current ATP systems reveals three possible approaches (see Figure 3):
ffl A non-specialized clausifier is used to convert the submitted problems to CNF, and the
ATP system uses the CNF versions of the problems (CNF systems).
ffl A specialized clausifier is prepended to a CNF ATP system, and this combined system
deals with the submitted problems directly (Clausifying systems).
ffl The ATP system does it's processing in FOF, and thus deals with the submitted problems
directly systems).
A comparison of CNF systems and FOF systems is not sensible, as CNF systems are disadvantaged
if FOF problems are used, and FOF systems are disadvantaged if CNF problems are
used. A comparison of FOF systems and clausifying systems is possible using FOF problems,
and a comparison of CNF systems and clausifying systems is possible using CNF problems.
A comparison of CNF systems and clausifying systems would constitute a comparison of
the clausifiers as well as a comparison of the ATP systems' basic theorem proving capabilities.
In future ATP competitions these categories should be separate.
Non-specialized
Clausifier
atomic proof
requests
in FOF
System
CNF
proofs/
models
Clauses
atomic proof
requests
in FOF
System
CNF
Specialized
Clausifier
proofs/
models
proofs/
models
atomic proof
requests
in FOF
FOF
System

Figure

3: Alternative system designs regarding the input language.
This would blur the interpretation of the results. Also, current ATP research is dominated by
CNF systems. There are applications that generate CNF problems directly (e.g., test pattern
generation for electronic circuits). These factors indicate that a CNF competition is particularly
viable. Decision : The ATP competition will use theorems expressed in Clause Normal Form. This
constraint does not exclude the other two types of ATP systems from the competition. However,
such systems will not be able to take advantage of their abilities to deal with FOF problems 11 .
CNF problems can be either Horn or non-Horn, and there are particular techniques designed
for solving each type. In the same way that the different classes of problems shown in Figure 2
are treated separately, ideally Horn and non-Horn problems should be treated separately.
It is not clear what techniques are best for which type of problem. ATP systems for non-Horn
problems can perform well on Horn problems, and some non-Horn problems are handled
well by systems designed for Horn problems. There is no corpus of ATP systems that are
specialized for either type of problem. For switch systems, classifying problems as being Horn
or non-Horn is easy, but selecting superior specialists for either type is not. Decision : Horn and
non-Horn problems will be used together in the ATP competition.
4.4 Clause Type Information.
A clause of a CNF problem may be classified, depending on whether it is part of the conjecture,
part of assumptions made, or part of the underlying theory. This clause type information may
be useful to an ATP system.
In the application of ATP systems such information will typically be available, and therefore
should be used. Decision : Systems may use clause type information in the ATP competition.
11 A competition using FOF problems is planned for the future. Having established the quality of their CNF
ATP systems in this competition, developers will be able to spend time building specialized clausifying programs
ready for a subsequent FOF competition.
4.5 Clause and Literal Orderings.
A possible source of bias in the competition is the particular ordering of the clauses within each
problem, and the literals within clauses.
For system evaluation it is necessary to ensure that pre-tuning (possibly accidental) to a
particular clause or literal order is of no benefit. Decision : In the ATP competition, random
clause and literal orderings will be used. The same clause and literal ordering will be used for all
systems.
5 Problem Selection
5.1 Problem Source
There are several possible sources for the unsatisfiable sets of clauses to be used in the ATP
competition. The competition could use problems supplied by the entrants, could use problems
selected from those commonly used by the ATP community, or could use new problems designed
especially for the competition (there are other options too).
If entrants were to supply the problems, they would almost inevitably supply (pathological)
cases for which their ATP system is particularly effective and that are difficult for other com-
petitors. The problems would not reflect any real work profile of ATP systems, and the ability
of the entrant to make such selections would influence the results. Designing new problems for
the competition would be acceptable, but requires additional effort and could still (unintention-
ally) be biased towards a particular system. Using problems from the ATP community will to
some extent reflect common usage of ATP systems. The TPTP Problem Library [SSY94, SS96]
contains a broad selection of such CNF theorems, as required. Decision : The unsatisfiable sets
of clauses to be used in the ATP competition will be selected from the TPTP problem library 12 .
The tptp2X utility, distributed with the TPTP problem library, will be used to remove
unnecessary equality axioms (see Section 4.2) and to reorder the problem clauses and literals
(see Section 4.5).
The tptp2X utility can be used to produce a pure equality representation of problems for
non-unit pure equality systems (see Section 4.1).
TPTP problems provide clause type information (see Section 4.4). Use of the clause type
information is possible either directly, by reading the TPTP problem, or indirectly, by using
a tptp2X formatting option that takes clause type information into account (see [SS96] for
details).
In principle, knowing the problem source allows problem-specific information to be precomputed
and accessed during the competition (in the extreme, proof look-up). Such activity is
contrary to the purpose of the competition, and is disallowed. Global optimization of an ATP
system for the TPTP is acceptable, because such an optimization seems likely to work well as
a general technique, due to the size and scope of the TPTP.
5.2 Problem Difficulty
The TPTP problem library contains a broad selection of problems in CNF, most of which are
unsatisfiable sets of clauses, as required. Suitably difficult problems must be selected for the
12 In a sense, entrants influence the problem selection by their submission of problems to the TPTP. However,
due to the selection process this constitutes a very weak influence.
competition. The TPTP rating system categorizes problems as follows:
solvable by all state-of-the-art ATP systems
difficult solvable by some state-of-the-art ATP systems
unsolved solvable by no state-of-the-art ATP systems
4 open theorem-hood unknown
Here "state-of-the-art ATP systems" means specialist ATP systems for the class of the
problem, according to an extended version of the classification given in Section 4.1.
Using easy problems would not differentiate sufficiently between systems performances. On
the other hand, each theorem must be provable by at least some of the systems. Decision : The
ATP competition will use TPTP problems with difficulty rating 2.
5.3 Problem Axiomatization
Many of the problems in the TPTP problem library have been taken from publications and
existing problem libraries. Some of the theorems contain only those axioms necessary for a
particular proof, or contain lemmas that assist a proof. Also, the axioms are often selected to
suit a particular ATP system, and are therefore biased towards that system. For each problem
using such a non-standard axiomatization, the TPTP provides a version of the problem using
a standard axiomatization containing all the axioms and no lemmas.
The use of a standard axiomatization is desirable because it reflects the typical initial
situation when formulating a new problem, and also avoids any bias towards a particular
system. Decision : Only standard versions of TPTP problems will be used in the ATP competition.
5.4 Number of Problems
Ideally, each ATP system would be evaluated using all eligible TPTP problems. However, the
limited time and hardware resources available determine a maximal number of problems 13 . At
the same time, a minimal number of problems is necessary for meaningful system evaluation.
The number of problems to be used needs to be determined.
The minimal number of problems will be determined to ensure sufficient confidence (say
85%) that the competition results are the same as would be obtained using all eligible problems.
For an evaluation based on the number of problems solved, as is essentially the case for the
ranking schemes to be used in the ATP competition (see Section 7.3), and assuming a worst
case proportion of problems solved (50%), this is computed by first computing n 0 , the minimal
number assuming an infinite number of eligible problems:
and then adjusting for the actual number of eligible problems as follows [Wad90]. Decision :
The minimal number of problems to be used in the ATP competition will be:
13 A complete evaluation pursued in a larger time frame is an interesting project by itself. Available TPTP
result data for various systems have been collected in [SS95]. These results, however, are difficult to compare,
because they have been obtained under differing constraints and on differing hardware platforms.
min number of problems =
23:04 \Theta number of eligible problems
number of eligible problems
The rating of TPTP problems is currently being completed. From that the number of
eligible problems, and hence the minimal number of problems, will be determined.
The maximal number of problems is determined by the resources available for the competition
(the number of workstations and the total time), the number of ATP systems competing,
and the minimal time limit that can be imposed on each proof attempt (see Section 6.1).
The maximal number of problems to be used in the ATP competition is:
number of problems = number of workstations \Theta time for competition
number of ATP systems \Theta minimal time limit
6 Resource Limits
6.1 Time Resource
Problems may be presented to an ATP system one-at-a-time or in batches. In both cases the
ATP system will have to deliver results within some time limit. Also, the time available for the
competition is limited. It is thus necessary to impose a time limit in the competition, either on
individual proof attempts or on the total time for all proof attempts.
If the problems are presented as a batch and an overall time limit is used, that will allow
the ATP systems to examine all the problems and perform once-only tuning before starting the
proof attempts. In terms of the competition, an overall time limit would simplify the winner
assessment; the system that proves the most theorems within the time limit is the winner, and
if more than one system proves the same number of theorems then the total time taken to prove
those theorems is compared (this ranking scheme has been adapted in Ranking Scheme A in
Section 7.3). In applications of ATP systems, problems are likely to be presented one-at-a-time,
as and when the problems are generated. Even if an ATP system is presented with a batch
of problems, they are likely to be treated individually, again requiring individual time limits
to decide when an unsuccessful attempt to find a solution should be abandoned 14 . Also, an
evaluation based on individual time limits still provides relevant performance information for
the batch case, but not vice versa. Decision : In the ATP competition, a time limit will be imposed
on individual proof attempts.
A minimal time limit is necessary so that the ATP systems spend most (say 95%) of their
time on proof search. This ensures that their deductive capabilities are compared, rather
than their initialization and termination efficiencies. The non-proof time of each ATP system
will be estimated by measuring the time that it takes to prove a trivial theorem, e.g.,
:p. The geometric average of the system's non-proof times will be taken to determine
the non proof time. Then, to ensure that the systems spend 95% of their time searching for a
proof, the minimal time limit = non proof time \Theta 20.
The time limit to be imposed on each proof attempt depends on the resources available for
the competition, the number of ATP systems, and the number of problems (which in turn is
determined by the minimal time limit; see Section 5.4). Decision : The time limit to be imposed
on each proof attempt in the ATP competition will be:
14 Processing all problems on a single processor via multiprogramming would not require imdividual time
limits, but realistically allows only as many proving processes as can fit in memory.
number of workstations \Theta time for competition
number of ATP systems \Theta number of problems
The way that the number of problems is determined in Section 5.4 ensures that
time limit - minimal time limit. In Section 7.2 it is explained that the granularity of timing
will be one second. Thus the minimal non proof time is one second, so time limit -
seconds.
It is important to note that the imposition of a time limit means that the competition
results will have to be viewed as modulo that time limit. A different time limit may produce a
different ranking of the systems. Results could be computed for a range of time limits, up to
the time limit used, and this may give further insight into the relative abilities of the systems.
If the ranking remains stable for a reasonable range of time limits, this provides empirical
evidence that the ranking reflects a time limit independent ranking of the ATP systems. Decision
: The ATP competition results will be plotted as a function of the time limit.
6.2 Hardware and Software Resources
ATP systems have been developed using a wide range of hardware and software resources. The
most commonly used computers in ATP research are UNIX based workstations. Many current
ATP systems do, or can be made to, run on such workstations. Other hardware used includes
PCs, LISP machines, parallel computers, and distributed networks of computers. The ATP
competition could limit itself to the common workstation environment, or could allow the use
of other hardware.
Running all ATP systems on the same hardware allows a direct comparison of the systems,
and is thus the preferred environment for the ATP competition. The host institution of the
competition is able to provide a homogeneous set of UNIX workstations, thus making this
option available. However, some ATP systems have been specially designed for a particular
computer, and cannot easily be ported. The host institution will be unable to supply the range
of specialized hardware that the ATP systems could use. Although a direct comparison of ATP
systems running on widely disparate hardware is not meaningful, performance data from such
systems does allow a comparison of the overall performance from a user's perspective. Since it
would be of interest to the ATP community to see the performance of all systems, ATP systems
that run on specialized hardware need to be catered for in the ATP competition. Decision :
Each of the ATP competition categories will be divided into two subcategories : General Hardware
and Special Hardware. ATP systems in the General Hardware subcategories will all be executed on
the same type of UNIX workstation, as supplied by the host institution. ATP systems in the Special
Hardware subcategories will be allowed to use any hardware brought to the competition or accessible
via the InterNet.
Common languages used to implement ATP systems are C, Prolog, and LISP. Other languages
have also been used. For the ATP competition, the major difference between implementation
languages is whether the code is interpreted, as is often the case for Prolog and LISP,
or compiled to machine code, as for C. ATP systems that run in an interpreted environment
are disadvantaged by the time taken to start up the interpreter, and by the relative slowness
of interpreters compared to compiled code. It would be possible to provide separate categories
for the two types of implementation.
There are some implementation environments that do not fall clearly into one or other of
the two types. Many vendors of Prolog and LISP interpreters also provide compilers, thus
alleviating the problems to at least some extent. Further, it is also important to acknowledge
the effort taken to code an ATP system in a language such as C, and the benefit derived
is competition will not be concerned with the languages used to
implement the ATP systems.
7 System Evaluation
7.1 Winner Assessment
It is arguable if an overall "winner" should be assessed. The reason for this is that potential
entrants may be frightened off by the fact that some other system may be believed to perform
better than theirs. The ATP competition could avoid this problem by simply reporting the
performance data of the ATP systems and allowing observers to draw their own conclusions.
Not determining a winner would leave out much of the spice of the competition, and would
remove much of the motivation for improving the capabilities of current systems 15 . It must be
remembered that the assessment done in the competition will be with respect to the decisions
made in this paper, i.e., the winner may not be the best ATP system in a general sense. Rather,
the ranking will simply provide feedback to potential users about which system is currently the
most adequate for their particular use. An useful byproduct of the competition will be the
charting of the performance data; that may reveal interesting properties of systems that do not
win. Decision : The ATP competition will both determine a specific ranking of the ATP systems
and present a listing of the performance data.
Two possible ways of determining a ranking are to have a quantitative ranking scheme or
to have a judging panel.
A quantitative ranking scheme can be implemented mechanically (as a computer program,
such as the RoboJudge described in [AKK93]) and checked for bias, but has little flexibility.
A judging panel is more flexible, can take into account more aspects, and can impose intuitive
judgment on the results. The benefits of both approaches are desirable. Decision : The ranking
of ATP systems in the ATP competition will be done by quantitatively evaluating system performance
and having that evaluation vetted by a panel of ATP researchers.
7.2 Performance Metrics
There are many criteria by which ATP systems can be evaluated. Issues such as completeness,
soundness, correctness, and proof performance are all of direct interest to ATP researchers.
Soundness (correctness) should be required in-so-far as falsely stating theoremhood of a non-
theorem renders the system untrustworthy, and therefore useless. Completeness plays a role
in-so-far as systems capable of proving more theorems should be ranked higher than systems
proving less. Incomplete systems, systems that fail to find a proof due to resource constraints,
and those that crash because they are bugged must be compared (see Section 3.2). Proof
performance can be measured in many ways, including runtime, number of inference steps
(proof size), and proof tree depth (if proofs are represented as trees). In the broader context of
computing science, issues such as software quality (portability, modularity), user friendliness,
execution environment, etc., may also be of interest. It is necessary to decide what criteria
should be used to evaluate the ATP systems in the ATP competition.
Would you train hard for the Olympics if you could not stand on a pedestal when you won?
The panel will consist of Peter Andrews, Alan Bundy, and Jeff Pelletier.
The broader issues of computer science are of lesser interest to the ATP community, and
factoring them into the evaluation would blur the focus on ATP. Also, no generally accepted
metrics exist for the broad issues, and evaluation would become a matter of taste. Decision :
The ATP competition will evaluate the ATP systems in terms of issues directly relevant to the ATP
community.
The quantitative ranking scheme needs to observe performance metrics of the ATP systems,
and combine those values to produce a ranking of the systems. Quantitative performance
metrics available for evaluation are:
ffl Calculus-relative measures
number of inferences performed (ideally split according to the inference rules used)
number of unifications performed (ideally split into failed and succeeded unifications)
proof size (e.g., length, depth)
ffl Absolute measures
number of problems solved
runtime (possibly split into startup time and search time)
memory usage
For the purposes of the ATP competition the measures used must be independent of the
ATP systems, so that values can be meaningfully compared. Measures such as the number
of inference steps and proof length are not suitable because the units of measure can vary
from calculus to calculus, and from implementation to implementation. System independent
measures that are readily obtained are the number of problems solved, runtime, and memory
usage.
The number of problems solved and the runtime are direct indicators of the quality of an
ATP system. Memory usage is important in so far as it can affect runtime. If an ATP system
requires less than the available memory, then the effect on runtime is negligible. If an ATP
system requires more than the available memory, then either the system cannot handle the
problem, or swapping increases the wall-clock time of the computation. Therefore the effect of
memory usage can be subsumed in a proper definition of runtime. Decision : The number of
problems solved and the runtime will be used for winner assessment in the ATP competition. The
memory usage will also be recorded and presented.
The are two reasonable ways of measuring runtime: CPU time taken, and wall-clock time
taken. The advantage of CPU time is that it is easy to measure and it is independent of
system influences such as external load, daemon processes, memory size, disc performance.
However, CPU time seems inappropriate if swapping occurs because it does not reflect the
user's perception of the runtime. Wall-clock time takes swapping into account, but is dependent
on system influences, and therefore can be difficult to measure in a reproducible manner. From
the developer's viewpoint, CPU time is more interesting. From the user's viewpoint, wall-clock
time is more relevant. Decision : For runtime, both CPU time and wall-clock time will be measured
and reported in the ATP competition.
In the General Hardware category, the choice of which time measurement is used for winner
assessment will depend on the computing environment. If no swapping occurs then: Decision :
In the General Hardware category CPU time will be used for winner assessment. If swapping does
occur, and the wall-clock time measurements are stable and representative of the time required
for the computation (essentially CPU time plus time required for swapping), then: Decision :
time will be used for winner assessment. Otherwise: Decision : CPU time plus an
estimate of the swapping time will be used for winner assessment.
In the Special Hardware category CPU timings are typically incomparable. In contrast,
wall-clock times can be compared, in the context of the hardware used. Decision : In the Special
Hardware category wall-clock time will be used for winner assessment.
The precision of time measurement on a computer system is limited. In order to reduce the
effect of this, and to emphasize significant rather than marginal differences, the competition
timing will be discrete. Decision : In the ATP competition timing will be done in units of one
second. In particular, the minimal time a system can take to find a proof is one second.
7.3 Ranking Schemes
The ATP competition must have a ranking scheme that combines the performance metrics to
produce a ranking of the ATP systems. The ranking scheme must have certain properties:
Monotonicity requirements:
ffl The ranking improves as the number of problems solved increases.
ffl The ranking improves as the time taken to solve the problems decreases.
Required invariants:
ffl For systems that solve the same number of problems, the one that takes the least time
to solve them obtains a better ranking.
ffl For systems that take the same amount of time, the one that solves the most problems
obtains a better ranking.
ffl Systems which solve the same number of problems in the same total amount of time
obtain the same ranking.
The issue of how to combine the performance metrics to obtain a ranking scheme is a
contentious one. Two different quantitative schemes for determining a system ranking, representing
different emphases between number of problems solved and time taken, have been
developed. For both schemes higher scores are better. Both schemes use the total time takenas
a parameter. This value is the sum of the time taken over all problems, including those for
which no proof is found, in which case the time limit is used.
Ranking Scheme A. This ranking scheme focusses on the ability to find as many solutions
as possible. The idea is to rank the systems according to the number of problems solved, and
to further differentiate by considering the number of erroneous claims that no proof exists, the
number of system errors, and the runtime. Since for erroneous claims and system errors the full
time limit is used as system runtime, considering only the runtime accounts for all the latter
issues. Thus, each system is given a score:
number of problems solved \Gamma
total time taken \Gamma best total time taken
where best total time taken = least total time taken by a system that solves the same number of
problems, and number of problems \Theta time limit. Achievable scores range
from 0 to the number of theorems.
Ranking Scheme B. This scheme measures solutions per unit time. Each system is given a
score:
number of problems solved
total time taken
Achievable scores range from 0 to 1.
The schemes place different emphasis on the two performance metrics: scheme A puts
emphasis on the number of problems solved while scheme B balances the emphasis. This
difference can lead to different rankings of ATP systems. Under scheme A a system which
solves the most problems will win, while under scheme B this is not necessarily so. Each
ranking scheme suits specific user requirements, and thus neither can be ignored. Decision : In
the ATP competition, in each competition category a winner according to Scheme A and a winner
according to Scheme B will be determined. As well as the scheme specific rankings, an overall
assessment is desirable. However, any attempt to combine the scores from Schemes A and
requires a trade-off between the two metrics: number of problems solved and time taken.
Any particular trade-off is unlikely to be acceptable to everyone. However, if a particular ATP
system is the winner in both schemes, then it is superior in terms of both performance metrics.
In the ATP competition, iff a single system is the winner of a category according to both
Schemes A and B, it will be the overall winner of the category.
Note: If only one ATP system is registered for a particular category, no winner will be
announced for that category, but the results for that system will still be presented.
Example. The example in Table 2 gives some system runtime values ("-" denotes an unsolved
problem). The example is designed to illustrate the differences between the schemes, to the
extent that each scheme produces a different ranking of the systems. It also shows the scores
and rankings that are produced by the different schemes, assuming that time limit = 20.
System Runtime for problem Scores by scheme Ranking by scheme
2. 1.
2.

Table

2: Example showing the runtimes of three different systems on five different problems
and the unscaled scores and rankings produced by the ranking schemes.
8 Conclusion
From the numerous issues that need to be resolved for organizing a useful competition, and
the impossibility of making an indisputable decision in several cases, it becomes clear that
alternative competition designs are possible. However, we believe that this rationally planned
competition will provide most of the benefits hoped for in Section 1.
We see a clear potential for improved future competitions, by extending their scope; additional
coverage of FOF systems (instead of just CNF systems) and model generation (instead
of just theorem proving) are the most important issues. However, it seems preferable to start
with a core design for a competition, and to add such extensions when more experience has
been gained.
After more than 30 years of research, automated theorem proving abounds with techniques
developed and ideas proposed. The future requires that these techniques and ideas be evaluated
to determine which are viable, and to integrate them into systems which are far more flexible
than the simple monolithic and compositional systems available today. The most important
challenge ahead in theorem proving research will be to provide adequate control, a subject still
in its infancy, since it is difficult to approach theoretically.
For all these goals, system evaluation is a crucial research tool, and competitions such as
this one will provide stimulus and insight that can lay the basis for the development of future
ATP systems.

Acknowledgments

. We are indebted to many researchers who discussed and commented
on our ideas; especially Alan Bundy, Bertram Fronh-ofer, Reinhold Letz, Bill McCune, David
Plaisted, and Christoph Weidenbach.



--R

The Internet Programming Contest: A Report and Philosophy.
Measuring the Performance of Automated Theorem Provers.
Proving First-Order Equality Theorems with Hyper-Linking
A Method for Building Models Automati- cally

A Theorem Prover for a Computational Logic.
Implementing Mathematics with the Nuprl Proof Development System.
The 4th DIMACS Interantional Algorithm Implementation Challenge.
Semantically Guided First-order Theorem Proving using Hyper-linking
The ACM Scholastic Programming Contest - <Year>1977</Year> to <Year>1990</Year>
The 2nd Annual Prolog Programming Contest.
Search Space and Proof Complexity of Theorem Proving Strategies
IMPS: An Interactive Mathematical Proof System.
Introduction to HOL

Controlled Experimentation and the Design of Agent Architectures.
Tactical Theorem Proving in Program Verification.
How to Prove Higher Order Theorems in First Order Logic.
On the Polynomial Transparency of Resolution.
Eliminating Duplication with the Hyper-Linking Strat- egy

OTTER 3.0 Reference Manual and Guide.
3 Inductive Learning Com- petitions
The 24th ACM International Computer Chess Championship.
Based Translation Methods for Modal Logics.
PVS: A Prototype Verification System.
The CADE-11 Competitions: A Personal View
The Next 700 Theorem Provers.
The Philosophy of Automated Theorem Proving.
The Search Efficiency of Theorem Proving Strategies.
Automated Advice in Fitch-style Proof Construction
Theorem Proving in Interactive Verification Systems
Efficient Model Generation through Compilation.
ATP System Results for the TPTP Problem Library (upto TPTP v1.
The TPTP Problem Library (TPTP v1.
The TPTP Problem Library.
Handbook of Statistical Methods for Engineers and Scientists.
RVF: An Automated Formal Verification System.

Automated Reasoning Contributes to Mathematics and Logic.
--TR

--CTR
Geoff Sutcliffe , Christian Suttner, The Procedures of the CADE-13 ATP System Competition, Journal of Automated Reasoning, v.18 n.2, p.163-169, April 1997
Geoff Sutcliffe , Christian Suttner, The Results - of the CADE-13 ATP System Competition, Journal of Automated Reasoning, v.18 n.2, p.271-286, April 1997
Francis Jeffry Pelletier , Geoff Sutcliffe , Christian Suttner, Conclusions about the CADE-13 ATP System Competition, Journal of Automated Reasoning, v.18 n.2, p.287-296, April 1997
G. Sutcliffe , C. B. Suttner, The CADE-15 ATP System Competition, Journal of Automated Reasoning, v.23 n.1, p.1-23, July 1999
Francis Jeffry Pelletier , Geoff Sutcliffe , Christian Suttner, The development of CASC, AI Communications, v.15 n.2, p.79-90, September 2002
Francis Jeffry Pelletier , Geoff Sutcliffe , Christian Suttner, The development of CASC, AI Communications, v.15 n.2,3, p.79-90, August 2002
G. Sutcliffe, The CADE-17 ATP System Competition, Journal of Automated Reasoning, v.27 n.3, p.227-250, October 2001
Geoff Sutcliffe, The CADE-16 ATP System Competition, Journal of Automated Reasoning, v.24 n.3, p.371-396, April 2000
Christian Suttner , Geoff Sutcliffe, The CADE-14 ATP System Competition, Journal of Automated Reasoning, v.21 n.1, p.99-134, August 1998

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'finite domain'
 'zero storage biometric authentication' 'graph coloring']
marked:['design', 'competition', 'automated theorem proving']
--T
The TPTP Problem Library.
--A
This paper provides a detailed description of the CNF part of the TPTP
Problem Library for automated theorem-proving systems. The library is
available via the Internet and forms a common basis for development and
experimentation with automated theorem provers. This paper explains the
motivations and reasoning behind the development of the TPTP (thus
implicitly explaining the design decisions made) and describes the TPTP
contents and organization. It also provides guidelines for obtaining and
using the library, summary statistics about release v1.2.1, and an overview
of the tptp2X utility program. References for all the sources of TPTP
problems are provided.
--B
Introduction
This paper describes the TPTP (Thousands of Problems for Theorem Provers)
Problem Library. The TPTP is a library of problems for automated theorem
proving (ATP) systems, for 1st order logic. The TPTP is comprehensive, and
thus provides an overview, and a simple, unambiguous reference mechanism for
problems. All problems in the TPTP are presented in an unambiguous
format, and automatic conversion to other known ATP formats is provided.
The principal motivation for this project is to move the testing and evaluation
of ATP systems from the present ad hoc situation onto a firm footing. This
is necessary, as results currently being published seldom provide an accurate
reflection of the intrinsic power of the ATP system being considered. A common
library of problems is necessary for meaningful system evaluations, meaningful
system comparisons, repeatability of testing, and the production of statistically
significant results. Guidelines for using TPTP problems and presenting results
are given in Section 4.2.
1.1 State of the Art
A large number of interesting problems have accumulated over the years in the
ATP community. Besides publishing particularly interesting individual problems,
from early on researchers have collected problems in order to obtain a basis
for experimentation. The first major publication 3 in this regard was [MOW76],
3 The first circulation of problems for testing theorem provers to our knowledge is due
to L. Wos in the late sixties.
which provides an explicit listing of clauses for 63 problems, many of which are
still relevant today. In the same year Wilson and Minker [WM76] tested six
resolution strategies on a collection of 86 named problems. The problem clauses
are not supplied in [WM76], however. A second major thrust was provided by
[Pel86], which lists 75 problems. Other more recent collections are [BLM
[Qua92a], [MW92], and [McC93], to name a few. Also, the Journal of Automated
Reasoning's Problem Corner regularly provides interesting challenge problems.
However, problems published in hardcopy form are often not suitable for testing
ATP systems, because they have to be transcribed to electronic form. This is
a cumbersome, error-prone process, and is feasible for only very small numbers
of problems. A problem library in electronic form was made publicly available
by Argonne National Laboratories (Otter format, [McC90]) in 1988 [ANL]. This
library has been a major source of problems for ATP researchers. Other electronic
collections of problems are available, but have not been announced officially (e.g.,
that distributed with the SPRFN ATP system [SPR]). Although some of these
collections provide significant support to researchers, and formed the early core
of the TPTP library, none (with the possible exception of the ANL library) was
specifically designed to serve as a common basis for ATP research. Rather, these
collections typically were built in the course of research into a particular ATP
system. As a result, there are several factors that limit their usefulness as a
common basis for research. In particular, existing problem collections are often:
- hard to discover and obtain,
- limited in scope and size,
outdated,
- formatted and tuned for a particular ATP system,
- inconsistent in their presentation of equally named problems,
- provided without a regular update service for new problems,
- provided without a reliable error correction service.
As a consequence, several problems arise. Firstly, system development and
system evaluations typically rely on a limited range of problems, depending on
the collections of problems available to the researcher. Secondly, the presentation
format used in a collection may not be appropriate for the desired purpose,
and a comparatively large effort is required just to make the problems locally
usable (which in practice often means that such a collection of problems is simply
ignored). Thirdly, using a particular collection may lead to biases in results,
because the problems have been designed and tuned specifically for a particular
ATP system. Fourthly, the significance and difficulty of a problem, with respect
to the current state-of-the-art in ATP systems, is hard to assess by newcomers
to the field. Existing test problems are often not adequate anymore (e.g.,
Schubert's Steamroller [Sti86]), while others may be solvable only with specialized
techniques (e.g., LIM+ [Ble90]) and therefore are much too hard to start
with. Finally, many copies and variants of the same "original" problem may exist
in different libraries. Coupled with a lack of documentation, this means that
unambiguous identification of problems, and therefore a clear interpretation of
performance figures for given problems, has become difficult.
The problem of meaningfully interpreting results is even worse than indi-
cated. Commonly a few problems are selected and hand-tuned (clauses and literals
are arranged in a special order, irrelevant clauses are omitted, lemmas are
added in, etc) specifically for the ATP system being tested. However, the presentation
of a problem can significantly affect the nature of the problem, and
changing the clauses clearly makes a different problem altogether. Nevertheless
the problem may be referenced under the same name as it was presented else-
where. As a consequence the experimental results reveal little. Some researchers
avoid this ambiguity by listing the clause sets explicitly. But obviously this usually
cannot be done for a large number of problems or for large individual prob-
lems. The only satisfactory solution to these issues is a common and stable
library of problems, which the TPTP provides.
1.2 What is Required?
The goal for building the TPTP has been to overcome the current drawbacks,
and to centralize the burden of problem collection and maintenance to one place.
The TPTP tries to address all relevant issues. In particular, the TPTP
- is available by anonymous ftp.
The TPTP is thus easily available to the research community. Awareness of
the TPTP is assured by extensive formal and informal announcements.
- spans a diversity of subject matters.
This reduces biases in the development and testing of theorem provers, which
arise from the use of a limited range of problems. It also provides an overview
of the domains that ATP is used in. The significance of each domain may be
measured by the number of problems available.
- is large enough for statistically significant testing.
In contrast to common practise, an ATP system should be evaluated over
a large number of problems, rather than a small set of judiciously selected
examples. The large size of the TPTP makes this possible.
- is comprehensive and up-to-date.
The TPTP contains all problems known to the community.There is no longer
a need to look elsewhere.
- is presented in a well structured and documented format.
All problems in the TPTP are presented in an unambiguous format. Automatic
conversion to other known formats is provided, thus eliminating the
necessity for transcription. The design and arrangement of the problems is
independent of any particular ATP system.
- provides a common, independent, source for problems.
This provides unambiguous problem reference, and makes the comparison of
results meaningful.
contains problems varying in difficulty, from very simple problems through
to open problems.
This allows all interested researchers, from newcomers to experts, to rely on
the same problem library.
provides statistics for each problem and for the library as a whole.
This gives information about the syntactic nature of the problems.
- will provide a rating for the difficulty of each problem.
This is important for several reasons: (1) It simplifies problem selection according
to the user's intention. (2) It allows the quality of an ATP system to
be judged. (3) Over the years, changes in the problem ratings will provide
an indicator of the advancement in ATP. The problem ratings are currently
being worked on, and will be part of a future TPTP release.
documents each problem.
This contributes to the unambiguous identification of each problem.
- provides standard axiomatizations that can be used in new problems.
This simplifies the construction of new problems.
- is a channel for making new problems available to the community, in a simple
and effective way.
- removes the necessity for the duplicated effort of maintaining many libraries.
specifies guidelines for its use for evaluating ATP systems.
As the TPTP is a standard library of problems, it is possible to set ATP
evaluation guidelines which make reported results meaningful. This will in
turn simplify and improve system comparisons, and allow ATP researchers
to accurately gauge their progress.
The TPTP problem library is an ongoing project, with the aim to provide
all of the desired properties.
Current Limitations of the TPTP. The current version of the TPTP library
is limited to problems expressed in 1st order logic, presented in clause normal
form. In particular, there are no problems for induction and nonclassical theorem
proving. However, see Section 5 for upcoming and planned extensions.
2 Inside the TPTP
Scope. Release v1.0.0 of the TPTP contains 1577 abstract problems, which
result in 2295 ATP problems, due to alternative presentations (see Section 2.2).

Tables

1, 2, and 3 provide some statistics about release v1.0.0 of the TPTP.
The problems have been collected from various sources. The two principal
sources have been existing problem collections and the ATP literature. Other
sources include logic programming, mathematics, puzzles, and correspondence
with ATP researchers. Many people and organizations have contributed towards
the TPTP. In particular, the foundations of the TPTP were laid with David
Plaisted's SPRFN collection [SPR]; many problems have been taken from Argonne
National Laboratory's ATP problem library [ANL] (special thanks to Bill
McCune here); Art Quaife has provided several hundred problems in set theory
and algebra [Qua92b]; the Journal of Automated Reasoning, CADE Proceedings,

Table

1. Statistics on the TPTP.
Number of problem domains 23
Number of abstract problems 1577
Number of problems 2295
Number of non-Horn problems 1449 (63%)
Number of problems with equality 1805 (79%)
Number of propositional problems 111 (5%)
being non-Horn 56 (50%)
Total number of clauses 322156
Total number of literals 970456

Table

2. Statistics for non-propositional TPTP problems.
Measure Minimum Maximum Average Median
Number of clauses 2 6404 124 68
Percentage of non-Horn clauses 0% 99% 6% 4%
problems 0% 99% 9% 4%
Percentage of unit clauses 0% 100% 32% 22%
Number of literals 2 18966 276 162
Percentage of equality literals 0% 100% 47% 47%
problems 4% 100% 57% 47%
Number of predicate symbols 1 4042 12 4
Number of function symbols 0 94
Percentage of constants 0% 100% 46% 33%
Number of variables 0 32000 311 271
Percentage of singletons 0% 100% 6% 7%
Maximal clause size 1 25 4 5
Maximal term depth

Table

3. Statistics for propositional TPTP problems.
Measure Minimum Maximum Average Median
Number of clauses 2 8192 444 36
Percentage of non-Horn clauses 0% 99% 20% 1%
problems 1% 99% 40% 45%
Percentage of unit clauses 0% 100% 16% 1%
Number of literals 2 106496 3303 132
Number of predicate symbols 1 840 69 13
Maximal clause size 1 21 5 3
and Association for Automated Reasoning Newsletters have provided a wealth
of material; smaller numbers of problems have been provided by a number of
further contributors (see also the Acknowledgements at the end of Section 5).
The problems in the TPTP are syntactically diverse, as is indicated by the
ranges of the values in Tables 2 and 3. The problems in the TPTP are also semantically
diverse, as is indicated by the range of domains that are covered.
The problems are grouped into 23 domains, covering topics in the fields of logic,
mathematics, computer science, and more. The domains are presented and discussed
in Section 2.1.
Releases. The TPTP is managed in the manner of a software product, in the
sense that fixed releases are made. Each release of the TPTP is identified by a
release number, in the form v!Version?.!Edition?.!Patchlevel?. The Version
number enumerates major new releases of the TPTP, in which important new
features have been added. The Edition number is incremented each time new
problems are added to the current version. The Patch level is incremented each
time errors, found in the current edition, are corrected. All changes are recorded
in a history file, as well as in the file for an affected problem.
2.1 The TPTP Domain Structure
An attempt has been made to classify the totality of the TPTP problems in
a systematic and natural way. The resulting domain scheme reflects the natural
hierarchy of scientific domains, as presented in standard subject classification
literature. The current classification is based mainly on the Dewey Decimal
Classification [Dew89] and the Mathematics Subject Classification used for the
Mathematical Reviews by the American Mathematical Society [MSC92]. We define
four main fields: logic, mathematics, computer science, and other. Each field
contains further subdivisions, called domains. These TPTP domains constitute
the basic units of our classification. The full classification scheme is shown in

Figure

1.
2.2 Problem Versions and Standard Axiomatizations.
There are often many ways to formulate a problem for presentation to an ATP
system. Thus, in the TPTP, there are often alternative presentations of a prob-
lem. The alternative presentations are called versions of the underlying abstract
problem. As the problem versions are the objects that ATP systems must deal
with, they are referred to simply as problems, and the abstract problems are
referred to explicitly. Each problem is stored in a separate physical file. The primary
reason for different versions of an abstract problem is the use of different
axiomatizations. This issue is discussed below. A secondary reason is the use of
different formulations of the theorem to be proven.
TPTP Logic
Mathematics
Computer science
Other
Combinatory logic
Logic calculi
Henkin models
Theory
Graph Theory
Algebra Boolean Algebra
Robbins Algebra
distributive
Lattices
Groups
Rings
General Algebra
Number Theory
Topology
Analysis
Geometry
Category theory
Computing Theory
Planning
Program Verification
Syntactic
Puzzles
Miscellaneous
COL
HEN
GRA
BOO
ROB
LDA
GRP
ALG
NUM
ANA
GEO
COM
PLA
PUZ
MSC
# of Problems
abs. / vers.
284 / 321
Fig. 1. The domain structure of the TPTP.
Different axiomatizations. Commonly, different axiomatizations of a theory
exist, and, in general, most are equally acceptable. In the TPTP an axiomatization
is acceptable if it is complete (in the sense that it captures some closed
theory) and it has not had any lemmas added. Such axiomatizations are called
standard axiomatizations. Within the ATP community some problems have been
created with non-standard axiomatizations. An axiomatization is non-standard
if it has been reduced (i.e., axioms have been removed) and the result is an incomplete
axiomatization, or if it has been augmented (i.e., lemmas have been added)
and the result is a redundant axiomatization. Non-standard axiomatizations are
typically used to find a proof of a theorem (based on the axiomatization) using a
particular ATP system. In any 'real' application of an ATP system, a standard
axiomatization of the application domain would typically have to be used, at
least initially. Thus the use of standard axiomatizations is desirable, because it
reflects such 'real' usage. In the TPTP, for each collected problem that uses a
non-standard axiomatization, a new version of the problem is created with a
standard axiomatization. The standard axiomatization is created by reducing or
augmenting the non-standard axiomatization appropriately.
The standard axiomatizations used in the TPTP are kept in separate axiom
files, and are included in problems as appropriate. If an axiomatization uses
equality, the required axioms of substitution are kept separate from the theory
specific axioms. The equality axioms of reflexivity, symmetry, and transitivity,
which are also required when equality is present, are also kept separately, as they
are often used independently of the substitution axioms.
2.3 Problem and Axiomatization Naming
Providing unambiguous names for all problems is necessary in a problem library.
A naming scheme has been developed for the TPTP, to provide unique, stable
names for abstract problems, problem versions, and axiomatizations. File names
are in the form DDDNNN-V[MMM].p for problem files, and DDDNNN-E.TT
for axiom files. DDD is the domain name abbreviation, NNN is the index within
the domain, V is the version number, MMM is an optional generation parameter,
E is the axiomatization extension number, .p indicates a problem file, and TT
is either .ax for theory specific axioms or .eq for equality substitution axioms.
The complete file names are unique within the TPTP.
Abstract problems and axiomatizations have also been allocated semantic
names. The semantic names can be used to augment file names, so as to give an
indication of the contents. While these names are provided for users who like to
work with mnemonic names, only the standard syntactic names are guaranteed
to provide unambiguous reference. The semantic names are formed from a set of
specified abbreviations. The semantic names can be added to the syntactic file
names using a script that is provided.
2.4 Problem Presentation
The physical presentation of the TPTP problem library is such that ATP researchers
can easily use the problems. The TPTP file format, for both problem
files and axiom files, has three main sections. The first section is a header section
that contains information for the user. The second section contains include instructions
for axiom files. The last section contains the clauses that are specific
to the problem.
The syntax of the problem and axiom files is that of Prolog. This conformance
makes it trivial to manipulate the files using Prolog. All information in the files
that is not for use by ATP systems is formatted as Prolog comments, with a
leading %. All the information for ATP systems is formatted as Prolog facts. A
utility is provided for converting TPTP files to other known ATP system formats
(see Section 3). A description of the information contained in TPTP files is given
below. Full details are given in [SSY93].
The Header Section
The header contains information about the problem, for the user. It is divided
into four parts. The first part identifies and describes the problem. The second
part provides information about occurrences of the problem. The third part
gives the problem's ATP status and a table of syntactic measurements made
on the problem. The last part contains general information about the problem.
An example of a TPTP header, extracted from the problem file GRP039-7.p, is
shown in Figure 2.
Released v0.0.0, Updated v0.11.5.
Theory (Subgroups)
Subgroups of index 2 are normal
O is a subgroup of G and there are exactly 2 cosets in
% G/O, then O is normal [that is, for all x in G and y in O,
% x*y*inverse(x) is back in O].
Problems and
Experiments for and with Automated Theorem Proving Programs
IEEE Transactions on Computers C-25(8), 773-782.
% Number of literals :
% Number of predicate
% Number of function
% Number of variables : 42 ( 0 singletons)
Maximal clause size : 4
Maximal
Comments : element-in-O2(A,B) is A-1.B. The axioms with
force index 2.
fixed.
Fig. 2. Example of a problem file header (GRP039-7.p).
The header fields contain the following information:
- The % File field contains the problem's syntactic and semantic names, the
TPTP release in which the problem first appeared, and the TPTP release
in which the problem was last updated.
- The % Domain field identifies the domain from which the problem is drawn.
- The % Problem field provides a one line, high-level description of the abstract
problem.
- The % Version field gives information that differentiates this version of the
problem from other versions of the problem.
- The % English field provides a full description of the problem if the one line
description is too terse.
- The % Refs field provides a list of references to items in which the problem
has been presented.
- The % Source field acknowledges, with a citation, where the problem was
(physically) obtained from.
- The % Names field lists existing known names for the problem (as it has been
named in other problem collections or the literature).
- The % Status field gives the problem's ATP status 4 , one of satisfiable,
unsatisfiable, open, or broken.
- The % Syntax field lists various syntactic measures of the problem's clauses.
- The % Comments field contains free format comments about the problem.
- The % Bugfixes field documents any changes which have been made to the
clauses of the problem.
The Include Section
The include section contains include instructions for TPTP axiom files. An
example of an include section, extracted from the problem file GRP039-7.p, is
shown in Figure 3.
%-Include the axioms of equality
include('Axioms/EQU001-0.ax').
%-Include the equality axioms for group theory in equality form
include('Axioms/GRP004-0.ax').
%-Include the subgroup axioms in equality formulation
include('Axioms/GRP004-1.ax').
Fig. 3. Example of a problem file include section (GRP039-7.p).
Each of the include instructions indicates that the clauses in the named
axiom file should be included at that point. Axiom files are presented in the
same format as problem files, and include instructions may also appear in
axiom files. Full versions of TPTP problems (without include instructions) can
be created by using the tptp2X utility (see Section 3).
4 This field has not yet been filled in all problems of the TPTP.
The Clauses Section
TPTP problems are presented in clausal normal form. The literals that make
up a clause are presented as a Prolog list of terms. Each literal is a term whose
functor is either ++ or -, indicating a positive or negative literal respectively.
The single argument of the sign, i.e., the atom of the literal, is a Prolog term.
The signs ++ and - are assumed to be defined as prefix operators in Prolog.
Each clause has a name, in the form of a Prolog atom. Each clause also has
a status, one of axiom, hypothesis, or theorem. The hypothesis and theorem
clauses are those that are derived from the negation of the theorem to be proved.
The status hypothesis is used only if the clauses can clearly be determined as
status is theorem. The name, status, and literal list of each
clause are bundled as the three arguments of a Prolog fact, whose predicate
symbol is input clause. These facts are in the clauses section of the problem
file.
Two examples of clauses, extracted from the problem file GRP039-1.p, are
shown in Figure 4.
%-Definition of subgroup of index 2
input-clause(an-element-in-O2,axiom,
++subgroup-member(A)]).
input-clause(prove-d-is-in-subgroup,theorem,
[-subgroup-member(d)]).
Fig. 4. Examples of problem file clauses (GRP039-1.p).
2.5 Physical Organization
The TPTP is physically organized into five subdirectories:
- The Problems directory contains a subdirectory for each domain, as shown
in

Figure

1. Each subdirectory contains the problem files for that domain.
- The Axioms directory contains the axiom files.
- The TPTP2X directory contains the tptp2X utility, described in Section 3.
- The Scripts directory contains some useful C shell scripts.
- The Documents directory contains comprehensive online information about
the TPTP, in specific files. This provides quick access to relevant overview
data. In particular, it provides a simple means for selecting problems with
specific properties, by using standard system tools (e.g., grep, awk).
3 The tptp2X Utility
The tptp2X utility converts TPTP problems from the TPTP format to formats
used by existing ATP systems. Currently, the tptp2X utility supports the following
output formats:
- the MGTP format [FHKF92];
- the Otter .in format [McC90] (the set of support and inference rules to be
used are also specified, and are included in the output file);
- the PTTP format [Sti84];
- the SETHEO .lop format [STvdK90];
- the SPRFN format [Pla88];
- the TPTP format, substituting include instructions with the actual clauses.
It is simple to add new formatting capabilities to the tptp2X utility. The tptp2X
utility can also be used to rearrange the clauses and literals in a problem. This
facilitates testing the sensitivity of an ATP system to the order in which the
clauses and literals are presented. Full details of the tptp2X utiltity are given in
[SSY93].
4 You and the TPTP
4.1 How to FTP the TPTP
The TPTP can be obtained by anonymous ftp from the Department of Computer
Science, James Cook University, or the Institut f?r Informatik, Technische Uni-
versit-at M-unchen. The ftp hosts are coral.cs.jcu.edu.au (137.219.17.4) and
flop.informatik.tu-muenchen.de (131.159.8.35), respectively. At both sites
the TPTP is in the directory pub/tptp-library. There are three files. Information
about the TPTP is in the ReadMe file (9.3 KByte), a technical report
[SSY93] is in TR-v1.0.0.ps.Z (357 KByte), and the library itself is packaged
in TPTP-v1.0.0.tar.Z (2.4 MByte). Please read the ReadMe file, as it contains
up-to-date information about the TPTP.
4.2 Important: Using the TPTP
By providing this library of ATP problems, and a specification of how these
problems should be presented to ATP systems, it is our intention to place the
testing, evaluation, and comparison of ATP systems on a firm footing. For this
reason, you should abide by the following conditions when using TPTP problems
and presenting your results:
- The specific version, edition, and patch level of the TPTP, used as the problem
source, must be stated (see the introduction of Section 2).
Each problem must be referenced by its unambiguous syntactic name.
may be added/removed without explicit notice. (This
holds also for removing equality axioms when built-in equality is provided
by the prover.)
- The clauses/literals may not be rearranged without explicit notice. If clause
or literal reversing is done by the tptp2X utility, the reversals must be explicitly
noted.
- The header information in each problem may not be used by the ATP system
without explicit notice. Any information that is given to the ATP system,
other than that in the input clauses, must be explicitly noted (including
any system switches or default settings).
Abiding by these rules will allow unambigous identification of the problem,
the arrangement of clauses, and further input to the ATP system which has been
used.
5 Present and Future Activity
5.1 The Present
Users' experiences with the TPTP will tell if we have achieved the design goals
described in Section 1, except for the problem ratings. The problem ratings are
currently being worked on. Note that these ratings should decrease as advances
in automated theorem proving are made. Therefore the long term maintenance
of the individual problem ratings will provide an objective measure of progress
in the field.
An upcoming addition to the TPTP is a set of Prolog programs for generating
arbitrary sizes of generic problems (e.g., the "pigeon holes" problem). This will
implicitly provide any size of such problems, and also reduce the physical size of
the TPTP. These programs are being worked on. In order to ensure the usability
of the programs, they will be tested on a public domain Prolog.
The tptp2X utility will soon be modified to be capabile of applying arbitrary
sequences of transformations to problems. The transformations will include
those currently in tptp2X, and others that become available (e.g., Mark
Stickel has provided a transformation for his upside-down meta-interpretation
approach [Sti94]). Automatic removal of equality axioms will also be provided,
to ease the life of researchers whose ATP systems have built in mechanisms for
equality reasoning.
5.2 The Future
We have several short and long term plans for further development of the TPTP.
The main ideas are listed here.
Performance tables for popular ATP systems.
Extensive experimental results, for publically available ATP systems, are
currently being collected (based on all the TPTP problems). These will be
published in some form.
- ATP system evaluation guidelines.
General guidelines outlining the requirements for ATP system evaluation
will be produced.
- The BSTP Benchmark Suite.
A benchmark suite (the BSTP) will be selected from the TPTP. The BSTP
will be a small collection of problems, and will provide a minimal set of
problems on which an ATP system evaluation can be based. The BSTP will
be accompanied by specific guidelines for computing a performance index
for an ATP system.
Full 1st order logic.
The TPTP will be extended to include problems in full 1st order logic no-
tation. ATP systems with automatic conversion to clausal form then can
derive additional information regarding the problem, such as which functors
are Skolem functors.
- Various translators.
Translators between various logical forms will be provided.
ffl from full 1st order logic to clausal normal form
ffl from non-Horn to Horn form
ffl from 1st order to propositional form
- Non-classical and higher order logics.
In the longer term, the TPTP may be extended to include problems expressed
in non-classical and higher order logics.
5.3

Acknowledgements

We are indebted to the following people and organizations who have contributed
problems and helped with the construction of the TPTP:
Geoff Alexander, the ANL group (especially Bill McCune), the Automated
Reasoning group in Munich, Dan Benanav, Woody Bledsoe, Maria Poala
Heng Chu, Tom Jech, Reinhold Letz, Thomas Ludwig, Xumin Nie,
Jeff Pelletier, David Plaisted, Joachim Posegga, Art Quaife, Alberto Segre, John
Slaney, Mark Stickel, Bob Veroff, and TC Wang.



--R

Available by anonymous ftp from info.
Challenge Problems in Elementary Calculus.
Set Theory in First-Order Logic: Clauses for G-odel's Axioms
Dewey Decimal Classification and Relative Index.
Model Generation Theorem Provers on a Parallel Inference Machine.
Otter 2.0 Users Guide.
Single Axioms for Groups and Abelian Groups with Various Operations.
Problems and Experiments for and with Automated Theorem-Proving Programs
American Mathematical Society
Experiments in Automated Deduction with Condensed Detachment.


Automated Deduction in von Neumann-Bernays-Godel Set The- ory
Problems based on NBG Set Theory.

The TPTP Problem Library (TPTP v1.
A Prolog Technology Theorem Prover.
Schubert's Steamroller Problem: Formulations and Solutions.

SETHEO/PARTHEO Users Manual.

--TR

--CTR
Robert Nieuwenhuis, The impact of CASC in the development of automated deduction systems, AI Communications, v.15 n.2, p.77-78, September 2002
Robert Nieuwenhuis, The impact of CASC in the development of automated deduction systems: Guest-editorial, AI Communications, v.15 n.2,3, p.77-78, August 2002
Monty Newborn , Zongyan Wang, Octopus: Combining Learning and Parallel Search, Journal of Automated Reasoning, v.33 n.2, p.171-218, September 2004
John Slaney, More Proofs of an Axiom of ukasiewicz, Journal of Automated Reasoning, v.29 n.1, p.59-66, 2002
G. Sutcliffe , C. B. Suttner, The CADE-18 ATP System Competition, Journal of Automated Reasoning, v.31 n.1, p.23-32,
Bernd Lchner, Things to Know when Implementing KBO, Journal of Automated Reasoning, v.36 n.4, p.289-310, April     2006
Steven Trac , Yury Puzis , Geoff Sutcliffe, An Interactive Derivation Viewer, Electronic Notes in Theoretical Computer Science (ENTCS), v.174 n.2, p.109-123, May, 2007
Urban , Grzegorz Bancerek, Presenting and Explaining Mizar, Electronic Notes in Theoretical Computer Science (ENTCS), v.174 n.2, p.63-74, May, 2007
Geoff Sutcliffe , Christian Suttner, The CADE-19 ATP System Competition, AI Communications, v.17 n.3, p.103-110, August 2004
Geoff Sutcliffe , Christian Suttner, The CADE-19 ATP system competition, AI Communications, v.17 n.3, p.103-110, July 2004
Justin Ward , Garrin Kimmell , Perry Alexander, Prufrock: a framework for constructing polytypic theorem provers, Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering, November 07-11, 2005, Long Beach, CA, USA
Geoff Sutcliffe, The IJCAR-2004 Automated Theorem Proving Competition, AI Communications, v.18 n.1, p.33-40, January 2005
Geoff Sutcliffe, The IJCAR-2004 automated theorem proving competition, AI Communications, v.18 n.1, p.33-40, January 2005
Francis Jeffry Pelletier , Geoff Sutcliffe , Christian Suttner, The development of CASC, AI Communications, v.15 n.2, p.79-90, September 2002
Francis Jeffry Pelletier , Geoff Sutcliffe , Christian Suttner, The development of CASC, AI Communications, v.15 n.2,3, p.79-90, August 2002
G. Sutcliffe , C. B. Suttner , F. J. Pelletier, The IJCAR ATP System Competition, Journal of Automated Reasoning, v.28 n.3, p.307-320, April 2002
G. Sutcliffe , C. B. Suttner, The CADE-15 ATP System Competition, Journal of Automated Reasoning, v.23 n.1, p.1-23, July 1999
Geoff Sutcliffe, The CADE-16 ATP System Competition, Journal of Automated Reasoning, v.24 n.3, p.371-396, April 2000
Geoff Sutcliffe, The CADE-20 Automated Theorem Proving Competition, AI Communications, v.19 n.2, p.173-181, April 2006
Geoff Sutcliffe, The CADE-20 automated theorem proving competition, AI Communications, v.19 n.2, p.173-181, January 2006
Dominique Pastre, Strong and weak points of the MUSCADET theorem prover - examples from CASC-JC, AI Communications, v.15 n.2,3, p.147-160, August 2002
G. Sutcliffe, The CADE-17 ATP System Competition, Journal of Automated Reasoning, v.27 n.3, p.227-250, October 2001
J. Avenhaus , Th. Hillenbrand , B. Lchner, On using ground joinable equations in equational theorem proving, Journal of Symbolic Computation, v.36 n.1-2, p.217-233, July
Josef Urban, MPTP -- Motivation, Implementation, First Experiments, Journal of Automated Reasoning, v.33 n.3-4, p.319-339, October   2004
Jia Meng , Claire Quigley , Lawrence C. Paulson, Automation for interactive proof: first prototype, Information and Computation, v.204 n.10, p.1575-1596, October 2006
Andreas Meier , Volker Sorge, Applying SAT Solving in Classification of Finite Algebras, Journal of Automated Reasoning, v.35 n.1-3, p.201-235, October   2005
Evaluating general purpose automated theorem proving systems, Artificial Intelligence, v.131 n.1-2, p.39-54, September 2001
Stephan Schulz, E - a brainiac theorem prover, AI Communications, v.15 n.2,3, p.111-126, August 2002
Stephan Schulz, E - a brainiac theorem prover, AI Communications, v.15 n.2, p.111-126, September 2002
Geoff Sutcliffe , Christian Suttner, The state of CASC, AI Communications, v.19 n.1, p.35-48, January 2006
Geoff Sutcliffe , Christian Suttner, The state of CASC, AI Communications, v.19 n.1, p.35-48, January 2006
Jens Otten , Wolfgang Bibel, leanCoP: lean connection-based theorem proving, Journal of Symbolic Computation, v.36 n.1-2, p.139-161, July
Simon Colton , Stephen Muggleton, Mathematical applications of inductive logic programming, Machine Learning, v.64 n.1-3, p.25-64, September 2006
Jose Hernandez-Orallo, Beyond the Turing Test, Journal of Logic, Language and Information, v.9 n.4, p.447-466, October 2000
Christian Suttner , Geoff Sutcliffe, The CADE-14 ATP System Competition, Journal of Automated Reasoning, v.21 n.1, p.99-134, August 1998
Christoph Weidenbach, Combining superposition, sorts and splitting, Handbook of automated reasoning, Elsevier Science Publishers B. V., Amsterdam, The Netherlands, 2001
R. Sekar , I. V. Ramakrishnan , Andrei Voronkov, Term indexing, Handbook of automated reasoning, Elsevier Science Publishers B. V., Amsterdam, The Netherlands, 2001

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'finite automata'
 'zero storage biometric authentication' 'parallel programming']
marked:['experimental evaluation', 'TPTP', 'problem library']
--T
What You Always Wanted to Know about Rigid E-Unification.
--A
This paper solves an open problem posed by a number of researchers: the
construction of a complete calculus for matrix-based methods with rigid
E-unification. The use of rigid E-unification and simultaneous rigid
E-unification for such methods was proposed by Gallier et al., in 1987.
After our proof of the undecidability of simultaneous rigid E-unification in
1995. (Degtyarev and Voronkov, 1996d), it became clear that one should look
for more refined techniques to deal with equality in matrix-based methods.
In this article, we define a complete proof procedure for first-order logic
with equality based on an incomplete but terminating procedure for rigid
E-unification. Our approach is applicable to the connection method and the
tableau method and is illustrated on the tableau method.
--B
Introduction
Algorithms for theorem-proving based on matings or tableaux in first-order logic
without equality comprise two kinds of rules. Rules of the first kind construct
matrices or tableaux from a given formula using a suitable amplification. Rules
of the second kind try to close paths or branches using substitutions making the
paths or branches inconsistent. These substitutions are unifiers of some atoms
laying on a path or branch. Until recently, most approaches to introducing equality
in such matrix-based methods tried to generalize such algorithms by a suitable
modification of the notion of a unifier.
Such a modification using simultaneous rigid E-unification was introduced in
(Gallier et al., 1987) for the method of matings (Andrews, 1981) or the connection
method (Bibel, 1981). It can be easily represented in the tableau formalism. The
method of matings interleaves two steps: amplification by quantifier duplication
and search for mating for a given amplification. For formulas in disjunctive normal
form this method was formulated earlier in (Prawitz, 1983). In this case amplification
is represented by a matrix and mating is represented by a set of simultaneously
satisfiable substitution conditions (mated pairs). Prawitz proposed a procedure for
constructing substitution conditions one by one, closing the corresponding paths
in the matrix through search with backtracking. Procedures of this kind were used
in later formalizations and implementations of the tableau method, the method
of matings or the connection method for formulas without equality. For example,
in (Fitting, 1990) the search for mating consists of repeated applications of MGU
atomic closure rule.
Supported by grants from INTAS, TFR and the Swedish Royal Academy of Sciences
y Partially supported by a TFR grant
(Gallier et al., 1987) tried to describe a similar procedure for logic with equality.
For example, (Gallier et al., 1992) describe such a procedure in which a substitution
condition is formalized via rigid E-unification, and the set of substitution
conditions via simultaneous rigid E-unification.
Simultaneous rigid E-unification can be formulated as follows. Given equations
finite sets of equations find a substitution oe such
that
means provability in first-order logic
with equality). The corresponding instance of the simultaneous rigid E-unification
problem is denoted by the system of rigid equations
EXAMPLE 1.1. In this and further examples we shall often omit parentheses in
terms with unary function symbols, for example we write ffb instead of f(f(b)).
Assume that we want to prove the formula
several applications of
tableau expansion rules to the negation normal form of :' (ff-, fi- and fl-rules in
the terminology of (Smullyan, 1968; Fitting, 1990) we obtain the following tableau
(we only consider the part of the tableau containing literals, omitting non-literal
a ' b c ' d
Collecting formulas lying on the two branches in this tableau we obtain the
following two rigid equations expressing inconsistency of this tableau:
This system of rigid equations has one solution ffa=x; fb=y; fc=u; fd=vg. This
substitution can be found by applying the functional reflexivity rule and MGU
replacement rule of (Fitting, 1990) (in fact, the reformulation of the paramodulation
rule for tableaux). Obviously, this tableau cannot be closed without the use
of the functional reflexivity. 1
1 The system of clauses corresponding to this example improves a result proved in (Plaisted,
1995) by using a more complicated example. In this system of clauses
a
a
there is no refutation even by unrestricted rigid paramodulation (i.e. using non-ordered rigid
paramodulation and paramodulation into variables), while (Plaisted, 1995) gives an example
showing incompleteness of ordered rigid paramodulation only.
What You Always Wanted to Know About Rigid E-Unification 3
Since the invention of simultaneous rigid E-unification by (Gallier et al., 1987),
there have been a number of publications on simultaneous rigid E-unification itself
and its use in theorem proving, for example (Gallier et al., 1988; Gallier et al., 1989;
Gallier et al., 1990; Gallier et al., 1992; Baumgartner, 1992; Beckert and H-ahnle,
1992; Becher and Petermann, 1994; Beckert, 1994; Goubault, 1994; Petermann,
1994). Some of these articles were based on the conjecture that simultaneous rigid
E-unification is decidable. There were several faulty proofs of the decidability of
this problem (Gallier et al., 1988; Gallier et al., 1990; Goubault, 1994).
The refutation procedure for first-order logic with equality using simultaneous
rigid E-unification (Gallier et al., 1992) was based on a faulty assumption that
solutions to simultaneous rigid E-unification can be found by consecutive combination
of finite complete sets of solutions for (non-simultaneous) rigid E-unification
(Gallier et al., 1990; Gallier et al., 1988). Later, (Degtyarev and Voronkov, 1996d;
Degtyarev and Voronkov, 1996e) proved that simultaneous rigid E-unification is
undecidable, which implied that Gallier et.al.'s procedure cannot, in general, find
solutions to simultaneous rigid E-unification. However, it is not clear whether this
implies incompleteness of their procedure for first-order logic with equality: there
are examples when the procedure cannot find a solution for a given amplification in
spite that such a solution exists, but can find a solution for a bigger amplification.
Completeness of Gallier et.al.'s procedure or existence of a procedure complete for
first-order logic with equality based on some set of solutions to rigid E-unification
was an open problem (Petermann, 1994; Beckert, 1995). Our paper gives a positive
solution to this problem.
An advantage of Gallier et.al.'s procedure is that it allows one to extend the
proof-search technology developed for tableaux without equality to the case with
equality, using solutions to rigid E-unification instead of most general unifiers. In
particular, for a given amplification Gallier et.al.'s procedure always terminates.
A procedure of this kind is used in the theorem prover 3 T A P (H-ahnle et al., 1994,
R.H-ahnle, private communication).
In this paper we define a procedure extending the tableau method to logic with
equality based on an incomplete procedure for rigid E-unification. Nevertheless,
our procedure is complete for first-order logic with equality. Hence, we rehabilitate
Gallier et.al.'s program for adding equality to semantic tableaux. Moreover, our
procedure solves rigid equations laying on different tableau branches independent-
ly. This strongly improves Gallier et.al.'s procedure which uses solutions of some
rigid equations to solve rigid equations on other branches.
A similar approach has already been defined in (Kanger, 1983) based on a more
straightforward way of variable instantiation. As a method for finding a closing
substitution, Kanger proposed an algorithm which can now be characterized as
an incomplete (but terminating) algorithm for simultaneous rigid E-unifiability.
Variables in a matrix (or a tableau) could be consecutively substituted by ground
terms already occurring in the matrix. This procedure does not solve simultaneous
rigid E-unifiability, but it gives a procedure complete for first-order logic with
total.tex - Date: June 3, 1997
4 Anatoli Degtyarev and Andrei Voronkov
equality. In the terminology of (Fitting, 1990) it means that a closing substitution
can be found after a sufficiently high (but not necessarily minimal) number of
applications of the fl-rule. The approach to substitutions based on this idea has
been characterized as minus-normalization in (Matulis, 1963; Maslov, 1967).
However, for a language with function symbols minus-normalization is interesting
mostly theoretically. Even in simplest cases, minus-normalization requires
a huge number of instantiations. For example, in the tableau of Example 1.1, we
have to consider 8 4 possible instantiations of variables x; by terms in the
set fa; b; c; d; fa; f b; f c; fdg. Moreover, it has been proved that the use of minus-
normalization can lead to considerable growth of derivations. Some results on
minus-normalization are proved by (Norgela, 1974).
In this paper we describe a logical calculus BSE for rigid E-unification based
on the rigid basic superposition rule that is an adaptation of basic superposition
of (Bachmair et al., 1992; Nieuwenhuis and Rubio, 1992), for "rigid" variables. For
a given rigid E-unification problem (called rigid equation in this paper), there is
only a finite number of BSE-derivations for this problem. Thus, BSE gives us an
algorithm returning a finite set of solutions to this rigid equation. We use these
solutions to close a tableau branch in the same way as most general unifiers are
used to close a branch in the MGU atomic closure rule of (Fitting, 1990).
Preliminaries
We present here a brief overview of notions and preliminary definitions necessary
for understanding the paper. We assume basic knowledge of substitutions and
unification.
Let \Sigma be a signature, and X be a set of variables. T (\Sigma; X) denotes the set of
all terms in the signature \Sigma with variables from X. The set of all ground terms
in the signature \Sigma is denoted by T (\Sigma).
A literal is either an atomic formula or a negation of an atomic formula. An
equation is a literal s ' t, where s; t 2 T (\Sigma; X). We do not distinguish equations
Literals of the form :(s ' t) are denoted by s 6' t and called
disequations. For simplicity, we assume that ' is the only predicate symbol of our
first-order language. As usual, arbitrary first-order languages can be represented in
such language by introducing a sort bool and replacing any non-equational atom
A by A ' true (for details see e.g. Bachmair et al., 1995).
By a ground expression (i.e. term or literal) we mean an expression containing
no variables. For any expression E, var(E) denotes the set of all variables occurring
in E. For a sequence of variables -
x, we shall sometimes denote - x also the corresponding
set of variables. We write A[s] to indicate that an expression A contains
s as a subexpression and denote by A[t] the result of replacing this occurrence of
s in A by t. By Aoe we denote the result of applying the substitution oe to A. If A
is a formula, we can as usual rename bound variables in A before applying oe. We
total.tex - Date: June 3, 1997
What You Always Wanted to Know About Rigid E-Unification 5
shall denote '(x) a formula ' with zero or more free occurrences of a variable x
and write '(t) to denote the formula 'ft=xg.
A substitution ' whose domain is a subset of fx is denoted
A substitution oe is called grounding for a set of variables V if for every variable
the term voe is ground.
be two substitutions with disjoint domains, The union of ' 1 and
is the substitution oe defined as follows. For every variable v we
have
Note that we use the union notation only for substitutions with disjoint
domains.
The inference systems used in this paper are defined with respect to a reduction
ordering, denoted by - which is total on ground terms. Our results are valid for
any such ordering.
A formula is in the Skolem negation normal form if it is constructed from
literals using the connectives - and the quantifier 8. There is a satisfiability-
preserving structure-preserving translation of formulas without equivalences into
formulas in Skolem negation normal form consisting of the standard skolemization
and a translation into negation normal form used e.g. in (Andrews, 1981). In order
to prove an arbitrary formula ', we translate :' in Skolem negation normal form
obtaining a formula / and try to establish unsatisfiability of /. For this reason,
theorems in this paper are formulated in terms of unsatisfiability.
For an equation s ' t and a multiset of equations E we write E ` s ' t to
denote that the formula ( V
is provable in first-order logic with
equality. For such formulas provability can be tested by the congruence closure
algorithm (Shostak, 1978). For the inclusion of multisets we shall use notation
3 Rigid basic superposition
The term "rigid paramodulation" has already been used in (Becher and Peter-
mann, 1994; Plaisted, 1995) for systems of inference rules in which all variables
are treated as "rigid". For example, rigid clause paramodulation of (Plaisted, 1995)
is essentially paramodulation and resolution over a set of clauses, where all substitutions
are applied to the whole set of clauses. A similar system for resolution has
been proposed earlier in (Chang, 1972) as V-resolution and in (Lee and Chang,
for resolution with paramodulation as V-resolution and V-paramodulation.
6 Anatoli Degtyarev and Andrei Voronkov
We shall use the term "rigid basic superposition" to denote a "rigid" version of
basic superposition. We formalize rigid basic superposition using constraints that
is close to the presentation of (Nieuwenhuis and Rubio, 1995).
DEFINITION 3.1 (Constraints). By an (ordering) constraint we mean a set of
expressions which can be of two kinds: an equality constraint s ' t, or an inequality
constraint s - t, where s; t are terms. A substitution ' is a solution to a constraint
(respectively, a constraint s - t) if ' is grounding for
coincides with t' (respectively, s' - t').
A substitution ' is a solution to a constraint C if ' is a solution to every equality
or inequality constraint in C. A constraint C is satisfiable if it has a solution.
Constraints C 1 and C 2 are called equivalent if they have the same sets of solutions.
We assume that there is an effective procedure for checking constraint satisfiability.
For example, there are efficient methods for solving ordering constraints for lexicographic
path orderings given by (Nieuwenhuis, 1993; Nieuwenhuis and Rubio,
1995).
DEFINITION 3.2. A rigid equation is an expression of the form E ' 8 s ' t,
where E is a finite multiset of equations and s; t are terms. Its solution is any
substitution ' such that E' ' s' ' t' 2 .
Below we shall introduce a system BSE for solving rigid equations. The derivable
objects of BSE are constraint rigid equations:
DEFINITION 3.3 (Constraint rigid equation). A constraint rigid equation is a
pair consisting of a rigid equation R and a constraint C. Such a constraint rigid
equation will be denoted R \Delta C.
DEFINITION 3.4 (Calculus BSE). The calculus BSE of constraint rigid equations
consists of the following inference rules:
Left rigid basic superposition:
(lrbs)
Right rigid basic superposition:
2 The term "rigid equation" could be more adequately expressed as "instance of a (non-
simultaneous) rigid E-unification problem", but this would be too lengthy.
What You Always Wanted to Know About Rigid E-Unification 7
Equality resolution:
Application of the rules is restricted to the following conditions:
1. The constraint at the conclusion of the rule is satisfiable;
2. The right-hand side of the rigid equation at the premise of the rule does not
have the form q ' q.
3. In the basic superposition rules, the term p is not a variable.
4. In the left basic superposition rule, s[r] 6= t.
The basic restriction in BSE is formalized by representing most general unifiers
through equality constraints. Condition 1 has two purposes. The satisfiability of
equations in constraints is needed to preserve correctness of the method. The
satisfiability of inequality constraints is needed to ensure termination (Theorem 3.9
below). Conditions 3-4 are not essential, they are added as standard optimizations
used in paramodulation-based methods. Condition 2 prohibits to apply any rules
to rigid equations of the form E ' 8 q ' q.
We denote by R \Delta C the fact that R 0 \Delta C 0 is obtained from R \Delta C by
an application of one of the inference rules of BSE. The symbol ;   denotes the
reflexive and transitive closure of ;.
3.5. Consider the rigid equation ha ' a; hx ' a; hb ' fy ` 8 y '
gfy. The ordering - is the Knuth-Bendix ordering (see Martin, 1987) in which all
weights of symbols are equal to 1 and which uses the precedence relation f ? h ?
a. Under this ordering we have ht - a and f t - hb for every ground term t.
The following is a BSE-derivation for this rigid equation:
fy ` 8 y ' gfy
fy ` 8 y ' ghb \Delta ffy - hb; gfy - fy ' fyg
fy ` 8 y ' ga
\Deltaff y - hb; gfy - fy ' fy; hx - a; ghb -
fy ' fy; hx - a; ghb -
By using constraint simplification, i.e. replacement of constraints by equivalent
"more simple" constraints we can rewrite this derivation as
fy ` 8 y ' gfy
fy ` 8 y ' ghb
fy ` 8 y ' ga \Delta fghb -
8 Anatoli Degtyarev and Andrei Voronkov
THEOREM 3.6 (Soundness of BSE). Let R substitution
satisfying C is a solution to R. In particular, R is solvable.
Proof. For any constraint C, denote by C ' the constraint obtained from C be
removing all inequality constraints. First we note that for every application of
an inference rule of BSE of the
By induction on the number of inference steps and using
the fact C i ' C i+1 , we prove the same statement for multi-step derivations
Let R have the form E Applying the obtained statement to the
multi-step derivation R t.
Hence, any solution to C. We have
Any constraint in C ' ` has the form u ' u. Hence,
solution to
This theorem leads to the following definition:
DEFINITION 3.7 (Answer constraint). A constraint C is called an answer constraint
for a rigid equation R if for some rigid equation
We note that BSE is an incomplete calculus for solving rigid equations. It
means that there are solvable rigid equations R that have no answer constraint.
For instance, consider the rigid equation 3 x ' a ` 8 gx ' x. It has one solution
fga=xg. However, no rule of BSE is applicable to x ' a ` 8 gx ' x \Delta ;.
This means that BSE can yield less solutions to a rigid equation than any other
known procedure, for example that of (Gallier et al., 1992) because all these procedures
are existentially complete. At the same time, BSE can yield more solutions
than the procedure of (Gallier et al., 1992) as the following example shows. For
the rigid equation a ' fa ` 8 x ' fa the procedure of (Gallier et al., 1992) will
find one solution fa=xg, but there are two answer constraints whose solutions are
the substitutions fa=xg and ffa=xg respectively.
In order to show that there is only finite number of derivations in BSE from a
given constraint rigid equation, we prove an auxiliary statement.
be an infinite sequence of terms in a finite signature all
whose variables belong to a finite set. Then there are numbers
and the constraint t i - t j is unsatisfiable.
Proof. Following (Kruskal, 1960) we introduce a partial ordering - on terms as
the smallest reflexive and transitive relation satisfying
1.
2. if s - t then r[s] - r[t].
3 Suggested by G.Becher (private communication).
What You Always Wanted to Know About Rigid E-Unification 9
By Kruskal's Tree Theorem (Kruskal, 1960) there exist
It is easy to see that the constraint t i - t j is unsatisfiable.
Similar statements have been proven in (Dershowitz, 1982; Plaisted, 1986).
THEOREM 3.9 (Termination of BSE). For any constraint rigid equation R \Delta C,
there exists a finite number of derivations from R \Delta C.
Proof. Suppose that there exists an infinite number of derivations from R \Delta C.
Then, by K-onig's lemma there exists an infinite derivation R \Delta
consisting of superpositions. Consider any application of superposition
it have the form
(lrbs)
(the case of right rigid basic superposition is similar). We prove that for every
the constraint C n is equivalent to C n [fs[p] - s[r]g. Indeed, the constraint
(and hence the constraint C n ) contains pg. By the definition of
reduction orderings, if p - r, then s[p] - s[r]. This implies that C n is equivalent
to s[r]g.
Since every application of rigid basic superposition replaces a literal s[p] '
t (or s[p] 6' t) in R i by a literal s[r] ' t (respectively, s[r] 6' t), there is an
infinite sequence of terms t and an increasing sequence of natural numbers
with the following property. For every positive natural number k the
constraint C n k
is equivalent to C n k
. Since all terms t k are in the same
finite signature and have variables in the same finite set, by Lemma 3.8, there are
and the constraint t
for
all k - j, the constraint C n j
is equivalent to C n j
, for all k - j. Hence,
the constraint is equivalent to C n j . Thus, C is
satisfiable. But satisfiability of C implies satisfiability of t i - t j . Contradiction.
Note. In (Degtyarev and Voronkov, 1996f) the left rigid basic superposition has
been formulated incorrectly in the following way:
(lrbs)
With this formulation, termination is not guaranteed as the following example
shows. Consider the rigid equation fgx ' gx; gx ' a ' 8 a ' b and arbitrary
reduction ordering - total on ground terms. The following is an infinite sequence
of applications of (lrbs):
ag
(lrbs)
\Deltafgx ' gx; fgx - gx; gx - a; gx - fag
(lrbs)
ag
It is easy to see that the constraint
ag
is satisfied by the substitution ff a=xg.
Inequality constraints are not needed for soundness or completeness of our
method. The pragmatics behind inequality constraints is to ensure that the search
for solutions of a rigid equation is finite. In addition, the use of ordering constraints
prunes the search space.
To illustrate this theorem, we consider Example 3.5. The rigid equation of this
example has an infinite number of solutions including fb=x; gh n a=yg, for every natural
number n. However, all possible BSE-derivations starting with ha ' a; hx '
fy ` 8 y ' gfy \Delta ; give only two answer constraints, one is
ffy - hb; gfy - fy ' fy; hx - a; ghb -
shown in Example 3.5, another is ffy - hb; gfy - fy ' fy; y ' ghbg obtained
from the following derivation:
fy ` 8 y ' gfy
fy ` 8 y ' ghb \Delta ffy - hb; gfy - fy ' fyg
fy ' fy; y ' ghbg
This answer constraint can be simplified to fy ' ghbg.
Theorem 3.9 yields
THEOREM 3.10. Any rigid equation has a finite number of answer constraints.
There is an algorithm giving for any rigid equation R the set of all answer constraints
for R.
What You Always Wanted to Know About Rigid E-Unification 11
In the rules (fl) the variable y does not occur in the premise.
Fig. 1. Tableau Expansion Rules
4 Answer constraints and the tableau method
In this section we consider how to use the system BSE for theorem proving by the
tableau method. Since we only consider skolemized formulas, we have no ffi-rules
in tableau calculi.
DEFINITION 4.1 (Branch and tableau). A branch is a finite multiset of formulas.
A tableau is a finite multiset of branches. A tableau with branches
be denoted by \Gamma . The tableau with is called the empty tableau
and denoted by #.
Often, tableaux are represented in tree form. Representation of tableaux as
multisets of branches is more convenient for us for several reasons. For this representation
we introduce a logical system allowing to expand tableaux:
DEFINITION 4.2 (Tableau expansion rules). The rules (ff), (fi) and (fl) of Figure
are called tableau expansion rules.
DEFINITION 4.3. Let \Gamma be a branch of a tableau. The set of rigid equations on
\Gamma is defined in the following way. A rigid equation t is on \Gamma if E is the
multiset of all equations in \Gamma and (s 6' t) 2 \Gamma.
We extend the notion of answer constraints to tableau branches:
DEFINITION 4.4. A constraint C is an answer constraint for a tableau branch \Gamma
if C is an answer constraint for some rigid equation on \Gamma.
By Theorem 3.10, we obtain
THEOREM 4.5. Any tableau branch has a finite number of answer constraints.
There is an algorithm which for any tableau branch \Gamma computes the set of all
answer constraints for \Gamma.
The following theorem states soundness and completeness of the tableau method
with answer constraints:
THEOREM 4.6 (Soundness and completeness). Let - be a sentence in Skolem negation
normal form. Then - is unsatisfiable if and only if there is a
obtained from - by tableau expansion rules and answer constraints C
respectively, such that
Proof. Soundness follows from soundness of BSE .
The proof of completeness is quite lengthy and is given in Appendix A. It
is based on the completeness of the equality elimination method (Degtyarev and
Voronkov, 1994; Degtyarev and Voronkov, 1995a; Degtyarev and Voronkov, 1996b).
To illustrate this theorem, consider the formula of Example 1.1. Assume that
we want to prove the formula
d oe g(u; x; y) ' g(v; fa; fb))). The negation normal form of :- is 8xyzu((a '
The ordering - is the lexicographic path ordering (see e.g. Nieuwenhuis and
Rubio, 1995) based on the precedence g ? f ? a d. For purely
illustrative purpose, we shall display tableaux in the tree form. After one quantifier
duplication (application of a fl-rule) and some other tableau expansion rules we
obtain the following tableau:
a ' b c ' d
There is one rigid equation on each branch of the tableau:
Rigid basic superposition is applicable to none of this rigid equations. Rigid equation
(1) has one answer constraint fg(x; u; v) ' g(y; f c; fd)g obtained by an application
of the equality resolution rule:
Similarly, rigid equation (2) has one answer constraint fg(u; x; y) ' g(v; fa; fb)g.
The union of these constraints fg(x; u; v) ' g(y; f c; fd); g(u; x; y) ' g(v; fa; fb)g is
total.tex - Date: June 3, 1997
What You Always Wanted to Know About Rigid E-Unification 13
unsatisfiable. Thus, our method does not find solution after one quantifier dupli-
cation. After three quantifier duplications and some other tableau expansion steps
we obtain the following tableau:
a ' b c ' d
a ' b c ' d
a ' b
It has four branches:
Consider the following rigid equations R 1 -R 4 on the branches \Gamma 1 -\Gamma 4 , respectively

We can apply the following BSE-derivations to R 1
a ' b; a ' b ' 8
a
a
14 Anatoli Degtyarev and Andrei Voronkov
a
a
ag
The union of the answer constraints of these derivations is
a - b; a ' a;
This constraint is satisfiable. To check this, we can consider the following substitution

5 Tableau basic superposition
As a simple consequence of our results, we prove a completeness result for a
paramodulation rule working on tableaux. A paramodulation rule working directly
on tableaux was proposed in (Loveland, 1978) in the context of model elimination
and later in (Fitting, 1990). However, their formulations have all disadvantages of
the early paramodulation rule of (Robinson and Wos, 1969):
1. Functional reflexivity rule is used;
2. Paramodulation into variables is allowed;
3. Increasing applications of paramodulation are allowed (for example, x can be
rewritten to f(x).
As a consequence, for a given tableau expansion there may be an infinite sequence
of paramodulations, in particular due to the use of functional reflexivity or increasing
applications of paramodulation. Since the publication of the book (Loveland,
1978), no improvements of the paramodulation-based tableau calculi have been
described except for (Plaisted, 1995) who has shown how to transform derivations
with resolution and paramodulation to tableaux by introducing a tableau factoring
rule.
Here we show that paramodulation is complete under the following restrictions:
What You Always Wanted to Know About Rigid E-Unification 15
1. No functional reflexivity is needed;
2. Paramodulation into variables is not allowed;
3. Orderings are used so that there are no increasing applications of paramodulation

4. Basic restriction on paramodulation prohibits paramodulation into terms introduced
by unification.
All these refinements are a consequence of our main result (Theorem 4.6).
In order to formalize the basic strategy, we keep the substitution condition as
a set of constraints, as before. Thus, we work with constraint tableaux:
DEFINITION 5.1 (Constraint tableau). A constraint tableau is a pair consisting
of a tableau T and a constraint C, denoted T \Delta C.
Now we adapt the tableau rules of (Fitting, 1990) to the case of constraint
tableaux. For simplicity, we only consider signatures whose only predicate symbol
is '. When we prove a formula ', we construct the Skolem negation normal form
/ of :' and, starting with the constraint tableau / try to derive the empty
tableau # with some satisfiable constraint.
DEFINITION 5.2 (Calculus T BSE). The free-variable tableau calculus T BSE
is shown in Figure 2.
DEFINITION 5.3 (Constraint tableau expansion rules). The rules (ff), (fi) and
(fl) of T BSE are called constraint tableau expansion rules.
The calculus T BSE has the required completeness property:
THEOREM 5.4 (Soundness and completeness). Let ' be a formula in the Skolem
negation normal form. It is unsatisfiable if and only if there is a derivation from
the constraint tableau ' \Delta ; of a constraint tableau # \Delta C.
Proof. Straightforward from Theorem 4.6 by noting that the rules of BSE can
be simulated by the corresponding tableau rules.
This logical system has one more pleasant property:
THEOREM 5.5 (Termination). For any constraint tableau T \Delta C there is only a
finite number of derivations from T \Delta C not using constraint tableau expansion
rules.
(lrbs)
In the rules (fl) the variable y does not occur in the premise. The conditions on
the rules (lrbs) and (rrbs) are the same as for the corresponding rules of BSE.
Fig. 2. Calculus T BSE
Proof. Similar to that of Theorem 3.9.
This means, that for a given amplification, we cannot have infinite search. Infinite
search without any expansion steps is possible in Fitting's system.
To illustrate the connection between the tableau rigid basic superposition rule
and rules of BSE, we reconsider the example of Section 4. On the branch containing
the literal g(x fd) and the equation c ' d, we can apply rigid
basic superposition that adds g(x fd) to the branch. Similarly,
we can apply rigid basic superposition to the branch containing
This results in the
tableau given in Figure 3 (the picture does not include the constraint, it is discussed
below).
After four application of the (er) rules all branches of this tableau become
closed. The resulting constraint of this derivation is the same as the union of the
answer constraints shown at the end of Section 4.
6 Related work
The problem of extending tableaux with equality rules is crucial for enriching the
deductive capabilities of the tableau method. Despite the fact that this problem
has been attacked by a growing number of researchers during the last years, known
solutions are not yet convincing. At the same time tableau methods of automated
deduction play an important role in various areas of artificial intelligence and
computer science - see e.g. special issues of the Journal of Automated Reasoning,
v. 13, no. 2,3, 1994. These issues contain a survey (Schumann, 1994) of implemen-
What You Always Wanted to Know About Rigid E-Unification 17
a ' b c ' d
a ' b c ' d
a ' b
Fig. 3.
tations of tableau-based provers. Among 24 systems mentioned in the survey only
two are able to handle equality.
The systems PROTEIN (Baumgartner and Furbach, 1994) and KoMeT (Bibel
et al., 1995) implement the modification method of (Brand, 1975). This method
transforms a set of clauses with equality into a set of clauses without equality.
This transformation usually yields a considerably larger set of clauses. In partic-
ular, the symmetry and the transitivity axioms must be explicitly applied to all
positive occurrences of the equality predicate. Recently, (Degtyarev and Voronkov,
1996c) proposed a new translation method based on the so-called basic folding
demonstrated for Horn clauses.
According to (Schumann, 1994), the system 3 T A P uses the method of (Beckert
and H-ahnle, 1992). Paper (Beckert and H-ahnle, 1992) claims the completeness of
the method, but this claim is not true. The method expands the tableau using
the standard tableau rules, including fl-rules. For finding a closing substitution,
an analogue of linear paramodulation without function reflexivity has been pro-
posed. As it is well known, linear paramodulation is incomplete without function
reflexivity. The same is true for the method of (Beckert and H-ahnle, 1992), as the
following example shows. Suppose that we prove the formula 9x(a ' b-g(fa; fb) '
In order to prove it using paramodulation, we need
to paramodulate a ' b into g(fa; fb) ' h(fa; fb). The method of (Beckert and
H-ahnle, 1992) only allows for paramodulation into copies of g(x; x) ' h(x; x)
obtained by applications of the fl-rule. Thus, this (provable) formula cannot be
proved using the method of (Beckert and H-ahnle, 1992).
Consider now approaches based on simultaneous rigid E-unifiability by (Gallier
et al., 1987; Gallier et al., 1992) and related methods. We do not consider numerous
total.tex - Date: June 3, 1997
works dedicated to (non-simultaneous) rigid E-unifiability. This problem is NP-complete
and there exist a number of complete algorithms for its solution (Gallier
et al., 1988; Gallier et al., 1990; Goubault, 1993; Becher and Petermann, 1994;
De Kogel, 1995; Plaisted, 1995). Since simultaneous rigid E-unification is undecidable
(Degtyarev and Voronkov, 1996d; Degtyarev and Voronkov, 1996e), these
completeness results are useless from the viewpoint of general purpose theorem
proving as proposed in (Gallier et al., 1987; Gallier et al., 1992). Our system BSE
can easily be extended to a calculus complete for rigid E-unifiability, but such
completeness is not our aim. We tried to restrict the number of possible BSE-
derivations preserving completeness of the general-purpose method of Section 4.
It is not known whether the procedure described in (Gallier et al., 1992) is
complete for theorem proving 4 . Even if it is complete, our procedure based on
BSE has some advantages over Gallier et.al.'s procedure. For example, for every
tableau branch with p equations and q disequations, we consider q rigid equations,
while Gallier et.al.'s procedure checks q \Delta 2 p rigid equations.
(Gallier et al., 1988; Gallier et al., 1990) introduced the notion of a complete
set of solutions for rigid E-unification, proved finiteness of such sets and gave
an algorithm computing finite complete set of solutions. According to this result,
(Goubault, 1994) proposed to solve simultaneous rigid E-unifiability by using finite
complete sets of solutions to the components of the simultaneous problem. Paper
(Goubault, 1994) contained faulty results. The undecidability of simultaneous rigid
E-unification shows that finite complete sets of solutions do not give a solution to
the simultaneous problem. The reason for this is that substitutions belonging to
complete sets of solutions for different rigid equations are minimal modulo different
congruences.
(Petermann, 1994) introduces a "complete connection calculus with rigid E-
unification". Here the completeness is achieved by changing the notion of a complete
set of unifiers so that solutions to all subproblems are compared modulo
the same congruence (generated by the empty equality theory). In this case,
a non-simultaneous problem can have an infinite number of solutions and no
finite complete set of solutions. For example, for the rigid E-unification problem
f(a) ' a ` 8 x ' a the complete set of solutions in the sense of (Gallier et al.,
1992) consists of one substitution fa=xg (and there is only one answer constraint
ag obtained by our method), but the complete set of solutions in the sense
of (Petermann, 1994) is infinite and consists of substitutions ff n (a)=xg, for all
:g. This implies that proof-search by the method of (Petermann, 1994)
can be non-terminating even for a limited number of applications of the fl-rule
(i.e. for a particular tableau), unlike algorithms based on the finite complete sets
of unifiers in the sense of (Gallier et al., 1992) or based on minus-normalization
(Kanger, 1983). The implementation of the method of (Petermann, 1994) uses a
completion-based procedure by (Beckert, 1994) of generation of complete sets of
4 For example, the completeness of Gallier et.al.'s procedure does not follow from our method
because, as noted above, our calculus BSE can give more solutions to some rigid equations.
What You Always Wanted to Know About Rigid E-Unification 19
rigid E-unifiers. This procedure is developed with the aim of solving a more general
problem - so-called mixed E-unification and has been implemented as part of the
tableau-based theorem prover 3 T A P . Complete sets of unifiers both in the sense
of (Gallier et al., 1992) and in the sense of (Petermann, 1994) can be computed
by this procedure in the case when all variables are treated as rigid. However, the
termination is not guaranteed even for complete sets of rigid E-unifiers in the sense
of (Gallier et al., 1992).
(Plaisted, 1995) gives "techniques for incorporating equality into theorem prov-
these techniques have a rigid flavor". His method of path paramodulation guarantees
termination for a given amplification and, in the case of success "solves
the simultaneous rigid E-unification problem, in a sense". However, this does not
solve the problem attacked by a number of researchers: extend the method of
matings to languages with equality by rigid E-unification. First, unlike (Gallier
et al., 1992) the search for solutions for a given amplification is not incremental
(the method does not allow "branch-wise" computation of solutions to rigid E-
unification for separate branches). Second, within a given amplification Plaisted
uses factoring rules which involves two branches (paths). As a consequence, even
when the original formula contains no equality, his method results in the standard
tableau calculus plus the factoring rule.
In fact, path paramodulation of (Plaisted, 1995) simulates resolution-paramodu-
lation inference in a connection-like calculus. Although it is not noted in (Plaisted,
1995), this technique has been demonstrated for resolution in many papers, for
example in (Bibel, 1981; Eder, 1988; Eder, 1991; Mints, 1990; Baumgartner and
Furbach, 1993; Avron, 1993). The generalization of this simulation to paramodulation
is straightforward.
However, this simulation technique is insufficient for proving the results of our
paper since, in particular, it gives no insight on how to avoid factoring in tableaux
with equality. The use of factoring prevents not only from the independent search
for solutions for tableau branches, but even from the incremental solving of rigid
equations on tableau branches as proposed by Gallier et.al.
Our equality elimination method (Degtyarev and Voronkov, 1996b; Degtyarev
and Voronkov, 1995b; Degtyarev and Voronkov, 1995a) is based on extending a
tableau prover by a bottom-up equation solver using basic superposition. Solutions
to equations are generated by this solver and used to close branches of a tableau.
Thus, the method combines (non-local) tableau proof search with the (local) equation
solving. Only completely solved equations are used in the tableau part of the
proof, thus reducing non-determinism created by applications of the MGU replacement
rule of (Fitting, 1990). The equation solution is even more restricted by the
use of orderings, basic simplification and subsumption.
A similar idea: combination of proof-search in tableaux with a bottom-up equality
saturation of the original formula is used in (Moser et al., 1995) for constructing
a goal-directed version of model elimination and paramodulation.
One of advantages of the tableau method is its applicability to non-classical
logics. However, handling equality in non-classical logics seems to be a much more
difficult problem than that in classical logic. For example, it is shown in (Voronkov,
1996) that procedures for intuitionistic logic with equality must handle simultaneous
rigid E-unification. This implies that our method based on BSE does not give a
complete procedure for intuitionistic logic with equality. Other results on relations
between simultaneous rigid E-unification and intuitionistic logic are considered in
(Degtyarev and Voronkov, 1996a; Degtyarev et al., 1996a; Degtyarev et al., 1996b;
Veanes, 1997).
An extensive discussion of equality reasoning in sequent-based systems may be
found in (Degtyarev and Voronkov, 1997).

Acknowledgments

We thank Gerard Becher and Uwe Petermann for helpful discussions, anonymous
referees for their valuable comments and the guest editors Jose Julio Alves Alferes
and Luis Moniz Pereira for their patience.



--R

Theorem proving via general matings.

Basic paramodulation and superposition.

Basic paramodulation.
Consolution as a framework for comparing calculi.
PROTEIN: A PROver with a Theory Extension INterface.

An ordered theory resolution calculus.
Rigid unification by completion and rigid paramodulation.
An improved method for adding equality to free variable semantic tableaux.
A completion-based method for mixed universal and rigid E-unification

Issues in theorem proving based on the connection method.
On matrices with connections.
Proving theorems with the modification method.
Theorem proving with variable-constrained resolution
Rigid E-unification simplified
Equality elimination for semantic tableaux.
Equality elimination for the inverse method and extension proce- dures
General connections via equality elimination.
Decidability problems for the prenex fragment of intuitionistic logic.
Equality elimination for the tableau method.

Handling equality in logic programs via basic folding.
Simultaneous rigid E-unification is undecidable
The undecidability of simultaneous rigid E-unification
What you always wanted to know about rigid E-unification
Equality reasoning and sequent-based methods
Simultaneous rigid E-unification and related algorithmic problems
"Logic in Computer Science"
Orderings for term rewriting systems.
Consolution and its relation with resolution.
First Order Logic and Automated Theorem Proving.
Theorem proving using rigid E-unification: Equational matings
IEEE Computer Society Press
Rigid E-unification is NP-complete
Rigid E-unification and its applications to equational matings
Rigid E-unification: NP-completeness and applications to equational matings
Theorem proving using equational matings and rigid E-unification


The many-valued tableau-based theorem prover 3 T A P
Technical Report 30/94
A simplified proof method for elementary logic.
Well quasi ordering
Symbolic Logic and Mechanical Theorem Proving.
Automated Theorem Proving: a Logical Basis.
How to choose weights in the Knuth-Bendix ordering
Invertible sequential variant of constructive predicate calculus (in Russian).
On variants of classical predicate calculus with the unique deduction tree (in Russian).

Model elimination with basic ordered paramodula- tion
Basic superposition is complete.
Theorem proving with ordering and equality constrained clauses.
Journal of Symbolic Computations

On the size of derivations under minus-normalization in Russian
A complete connection calculus with rigid E-unification
A simple non-termination test for the Knuth-Bendix method
Special cases and substitutes for rigid E-unification
An improved proof procedure.
Paramodulation and theorem-proving in first order theories with equality

An algorithm for reasoning about equality.

On Simultaneous Rigid E-Unification
Proof search in intuitionistic logic with equality
--TR

extracted:['file assignment' 'feedback controls' 'feature weights'
 'fault-tolerant software systems' 'fault-tolerant routing algorithm'
 'fault-tolerant routing' 'fault-tolerant algorithms' 'fault-containment'
 'zero storage biometric authentication' 'temporal logic']
marked:['equality reasoning', 'rigid E-unification', 'tableau method']
--T
Integrating Computer Algebra into Proof Planning.
--A
Mechanized reasoning systems and computer algebra systems have different
objectives. Their integration is highly desirable, since formal proofs often
involve both of the two different tasks proving and calculating. Even more
important, proof and computation are often interwoven and not easily
separable.In this article we advocate an integration of computer algebra into
mechanized reasoning systems at the proof plan level. This approach allows
us to view the computer algebra algorithms as methods, that is, declarative
representations of the problem-solving knowledge specific to a certain
mathematical domain. Automation can be achieved in many cases by searching
for a hierarchic proof plan at the method level by using suitable
domain-specific control knowledge about the mathematical algorithms. In
other words, the uniform framework of proof planning allows us to solve a
large class of problems that are not automatically solvable by separate
systems.Our approach also gives an answer to the correctness problems inherent in
such an integration. We advocate an approach where the computer algebra
system produces high-level protocol information that can be processed by an
interface to derive proof plans. Such a proof plan in turn can be expanded
to proofs at different levels of abstraction, so the approach is well suited
for producing a high-level verbalized explication as well as for a
low-level, machine-checkable, calculus-level proof.We present an implementation of our ideas and exemplify them using an
automatically solved example.Changes in the criterion of rigor of the proof engender
major revolutions in mathematics. H. Poincar, 1905
--B
Introduction
The computer and the development of high-level programming languages
made possible the mechanisation of logic as well as the realisa-
M. KERBER, M. KOHLHASE, V. SORGE.
tion of mechanical symbolic calculations, we could witness in the last
forty years. This has lead to two rather disjoint academic fields: mechanised
reasoning and computer algebra, which each have their own meth-
ods, interests and traditions, even though they share common roots:
none of the two fields is imaginable without the underlying foundation
of mathematical logic or the mathematical study of symbolic calculations
(leading to such algorithms and methods as the determination of
the GCD or the Gau-ian elimination). Only in the last decade we have
seen a move towards an integration of the fields driven by the insight
that real-world formal problems often involve a mixture of both computation
and reasoning, hence an integration of mechanised reasoning
systems and computer algebra systems is highly desirable (cf. [Buc85]).
This is the case in particular, since deduction systems are very weak,
when it comes to computation with mathematical objects, and computer
algebra systems manipulate highly optimised representations of
these objects, but do not yield any formally checkable proof information
(if they give any explanation at all).
In the remainder of this introduction we briefly summarise key points
of mechanised reasoning systems as well as of computer algebra systems
and then give a short preview on the integration approach advocated
in this paper. By its nature, such a short description has to abstract
from many details and to simplify considerably.
1.1. Mechanised Reasoning Systems
Mechanised reasoning systems (for short MRS in the following) are
built with various purposes in mind. One goal is the construction of an
autonomous theorem prover, whose strength achieves or even surpasses
the ability of human mathematicians. Another is to build a system
where the user derives the proof, with the system guaranteeing its cor-
rectness. A third purpose consists in modelling human problem-solving
behaviour on a machine, that is, cognitive aspects are the focus.
Advanced theorem proving systems often try to combine the different
goals, since they can complement each other in an ideal way. Let us
roughly divide existing theorem-proving systems into three categories:
machine-oriented theorem provers, proof checkers, and human-oriented
(plan-based) theorem provers.
Normally all these systems do not exist in a pure form anymore, and
in some systems like our
own\Omega mega system [BCF + 97] it is explicitly
tried to combine the reasoning power of automated theorem provers as
logic engines, the specialised problem solving knowledge of the proof
planning mechanism, and the interactive support of tactic-based proof
development environments. We think that the combination of these
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 3
complementary approaches inherits more advantages than drawbacks,
because for most tasks domain-specific as well as domain-independent
problem-solving know-how is required and for difficult tasks more often
than not an explicit user-interaction should be provided. While such an
approach seems to be general enough to cope with any kinds of logic-level
proofs, it neglects the fact that for many mathematical fields,
the everyday work of mathematicians only partially consists in proving
or verifying theorems. Calculation plays an equally important r-ole. In
some cases the tasks of proving theorems and calculating simplifications
of certain terms can be separated from each other, but very often
the tasks are interwoven and inseparable. In such cases an interactive
theorem proving environment will only provide rather poor support to
a user. Although theoretically any computation can be reduced to theorem
proving, this is not practical for non-trivial cases, since the search
spaces are intractable. For many of these tasks, however, no search is
necessary at all, since there are numerical or algebraic algorithms that
can be used. If we think of Kowalski's equation
purpose procedures do not (and cannot)
provide the control for doing a concrete computation.
1.2. Computer Algebra Systems
Early computer algebra systems (CAS for short) developed from collections
of algorithms and data structures for the manipulation of algebraic
expressions like the multiplication of polynomials, or the derivation
and integration of functions [Hea95]. Abstractly spoken, the main
objective of a CAS can be viewed in the simplification of an algebraic
expression or the determination of a normal form. Today there is a
broad range of such systems, ranging from very generally applicable
systems to a multitude of systems designed for specific applications.
Unlike MRS, CAS are used by many mathematicians as a tool in their
everyday work, they are even widely applied in sciences, engineering
and economics. Their high academic and practical standard reflects the
fact that the study of symbolic calculation has long been an established
and fruitful subfield of mathematics that has developed the mathematical
theory and tools.
Most modern systems [Wol96, CGG have in common that
the algebraic algorithms are integrated in a very comfortable graphical
user interface that includes formula editing, visualisation of mathematical
objects and even an interface to programming languages. As in the
case of MRS the representation languages of CAS differ from system
to system, which complicates the integration of such systems as well
as the cooperation between them. This deficiency has been attacked in
4 M. KERBER, M. KOHLHASE, V. SORGE.
the OpenMath initiative [AvLS96], which strives for a standard CAS
communication protocol. Currently the main emphasis is laid on standardising
the syntax and the computational behaviour of the mathematical
objects, while their properties or semantics are not considered.
That means there is no explicit representation format for theorems,
lemmata and proofs. Some specific systems allow to specify mathematical
domains and theories. For instance in systems like MuPad [Fuc96]
or Axiom [JS92], computational behaviour can be specified by attaching
types and axiomatisations to mathematical objects; but this also
falls short of a comprehensive representation of all relevant mathemat-
ics. Furthermore, almost all CAS fail to give an explanation or proof of
their solution to the problem at hand, even though some mathematical
theories like that of Gr-obner bases can be successfully applied to theorem
proving in elementary geometry [Cho88, Kap88, CGZ94, Wu94].
1.3. Contributions of this Paper
Not only the fact that a mutual simulation of the tasks of an MRS and
a CAS can be quite inefficient, but more that the daily work of mathematicians
is about proving and calculating points to the integration
of such systems, since mathematicians want to have support in both of
their main activities. Indeed two independent systems can hardly cover
their needs, since in many cases the tasks of proving and calculating are
hardly separable. As pointed out by Buchberger [Buc96a] the integration
problem is still unsolved, but it can be expected that a successful
combination of these systems will lead to "a drastic improvement of
the intelligence level" of such support systems.
Our paper addresses two immediate questions occurring in the integration
of automated reasoning and computation systems.
How can the algorithms be integrated, so that the underlying
mathematical knowledge is mutually respected and a synergy effect
is achieved?
How can the correctness problem inherent in any such combination
be addressed? In particular, how can results from the CAS be
integrated into a proof without having to completely trust the
We advocate an integration of computer algebra into mechanised reasoning
systems using the proof planning paradigm. This approach
allows to encapsulate the computer algebra algorithms into meth-
ods, that is, declarative representations of the problem solving knowledge
specific to a certain mathematical domain. The proof planning
paradigm enables a user to guide a proof or to fully hand over the con-
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 5
trol to a planner, which in turn can use computer algebra systems, if
the specifications for the corresponding algorithms are met. The use of
hierarchic proof plans at the method-level gives a suitable granularity
of integration, since it allows to directly use existing (human) control
knowledge about the interplay of computation and reasoning.
A proper integration into the proof planning approach answers the
question about the correctness automatically, since the corresponding
questions are solved for proof planning. In this area a proof plan can
either be rejected (the tactics are not executable, hence the plan cannot
be used to build a proof) or executed. The later results either in a
further planning phase to fill in possible gaps or in an accepted machine-checkable
proof. Hence a proper integration requires that the computer
algebra system produces high-level protocol information that can be
processed by an interface to derive proof plans which themselves can
be seamlessly integrated into the overall proof plan generated in the
problem solving attempt. Since this can be expanded into an explicit,
checkable proof in order to obtain a correctness guarantee for the combined
solution, we have also given a principled answer to the correctness
problem.
The feasibility of the approach advocated in the sequel has been
verified by integrating a simple CAS into
the\Omega mega proof planning
system. Therefore, we organise the paper around this experiment and
describe the relevant features with a system perspective. Our approach
requires a mode of the CAS that generates information from which it
is possible to generate a proof plan. For that reason the integration of
a standard CAS makes major adaptations unavoidable (in particular it
is necessary to change the source code of these systems). Our approach
is not committed to the particular systems involved, in particular, the
work reported here should be understood rather as a proof of principle
than as the development of a state-of-the-art integrated system.
Moreover, we will make the details of the approach more concrete by
explaining them by means of an example that cannot easily be solved
by either a mechanised reasoning system or a computer algebra system
alone, but that needs the combined efforts of systems of each kind.
2. Related Work
We give a short description of some of the experiments to combine MRS
and CAS and roughly categorise them into three classes with respect
to the treatment of proofs that is adopted, that is, with respect to the
correctness issue. In doing so we only describe in detail the approaches
of integrating CAS into MRS, that is, essentially the MRS is the master
KeKoSo.tex; 24/06/1998; 14:49; no v.
6 M. KERBER, M. KOHLHASE, V. SORGE.
and the CAS the slave, since our approach is also of this kind. With
the same right, one can of course follow the converse direction, namely
to approach the integration from the point of the CAS and indeed such
approaches are also successfully undertaken (see e.g. [CZ92, Buc96]).
The question about the granularity of integration is treated uniformly
by all these experiments. The application of the CAS is treated
as another (derived) rule of inference at the level of the (tactic) calcu-
lus, so the granularity of integration depends on the granularity of the
calculus or the tactics involved.
In the first category of attempts (see e.g. [HT93b, BHC95]) one
essentially trusts that the CAS properly work, hence their results are
directly incorporated into the proof. All these experiments are at least
partly motivated by achieving a broader applicability range of formal
methods and this objective is definitively achieved, since the range of
mathematical theorems that can be formally proved by the system combinations
is much greater than that provable by MRS alone. However,
CAS are very complex programs and therefore only trustworthy to a
limited extent, so that the correctness of proofs in such a hybrid system
can be questioned. This is not only a minor technical problem, but will
remain unsolved for the foreseeable future since the complexity (not
only the code complexity, but also the mathematical complexity) of a
CAS does not permit a verification of the program itself with currently
available program verification methods. Conceptually, the main contribution
of such an integration is the solution of the software-engineering
problem how to pass the control between the programs and translating
results forth and back. While this is an important subproblem, it does
not seem to cover the full complexity of the interaction of reasoning and
computation found in mathematical theorem proving. In an alternative
approach that formally respects correctness, but essentially trusts CAS,
an additional assumption standing for the CAS is introduced, so that
essentially formulae are derived that are proved modulo the correctness
of the computer algebra system at hand (see e.g. [HT93b]).
The second category (for which [HT93a] is paradigmatic) is more
conscious about the r-ole of proofs, and only uses the CAS as an oracle,
receiving a result, whose correctness can then be checked deductively.
While this certainly solves the correctness problem, this approach only
has a limited coverage, since even checking the correctness of a calculation
may be out of scope of most MRS, when they don't have additional
information. Indeed from the point of applicability, the results of the
CAS help only in cases, where the verification of a result is simpler
than its discovery, such as prime factorisations, solving equations, or
symbolic integration. For other calculations, such as symbolic addition
or multiplication of polynomials and differentiation, the verification is
just as complex as the calculation itself, so that employing the CAS
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 7
does not speed up the proof construction process. Typically in longer
calculations, both types of sub-calculations are contained.
A third approach of integrating computer algebra systems into a
particular kind of mechanised reasoning system, consists in the metatheoretic
extension of the reasoning system as proposed for instance
in [BM81, How88] and been realised in Nuprl [Con86]. In this approach
a constructive mechanised reasoning system is basically used as its
own meta-system, the constructive features are exploited to synthesise
a correct computer algebra system and due to bridge rules between
ground and meta-system it is possible to integrate the so-built CAS
that it can be directly used as a component. The theoretical properties
of the meta-theoretic extension guarantee that if the original system
was correct then the extended system is correct too. This method is
the most appealing one from the viewpoint of correctness, although
the assumption that the original (also rather complex) system must be
correct can hardly be expected to be self-evident for any non-trivial
system. A disadvantage compared to the other two approaches is that
it is not possible to employ an existing CAS, but that it is necessary to
(re)implement one in the strictly formal system given by the basic MRS.
Of course this is subject to the limitations posed by the (mathematical
and software engineering) complexities mentioned above.
The main problem of integrating CAS into MRS without violating
correctness requirements is that CAS are generally highly optimised
towards maximal speed of computation but not towards generating
explanations of the computations involved. In most cases, this is dealt
with by meta-theoretic considerations why the algorithms are adequate.
This lack of explanation makes it not only impossible for the average
user to understand or convince himself of the correctness of the com-
putation, but leaves any MRS essentially without any information why
two terms should be equal. This is problematic, since computational
errors have been reported even for well-tested and well-established
CAS. From the reported categories of approaches only the last one
seriously addresses this problem.
3.\Omega mega as an Open System for Integrating Computation
\Omega mega is a proof development system, based on the proof planning
paradigm. In this section we describe its architecture and components
and show how this supports the integration of computer algebra sys-
tems. Since the goal of this paper is not to present a system description
of\Omega mega, but to document the integration of computer algebra into
it, we try to be as concise as possible and introduce the relevant parts
KeKoSo.tex; 24/06/1998; 14:49; no v.
8 M. KERBER, M. KOHLHASE, V. SORGE.
only, the general architecture, the proof planner, and the integration
possibilities for external reasoners.
3.1. The Proof Development
Environment\Omega\Gammamen
The entire process of theorem proving
in\Omega mega can be viewed as
an interleaving process of proof planning, execution and verification
centred around a hierarchical proof plan data structure.
Several integrated tools support the user in interacting with the
system. Some of them are also available to the proof planner.
Theory Database
Since methods and control knowledge used in proof planning are most-
ly
domain-specific,\Omega mega organises the mathematical knowledge in a
hierarchy of theories. Theories represent signature extensions, axioms,
definitions, and methods that make up typical established mathematical
domains. Each theorem has its home theory and therefore has access
to the theory's signature extensions, axioms, definitions, and lemmata
without explicitly introducing them. A simple inheritance mechanism
allows to incrementally build larger theories from smaller parts.
We give an overview of the part
of\Omega mega's theory database that
is necessary for solving our extended example in Figure 1.
Proof Explanation
Proof presentation is one important feature of a mathematical assistant
that has been neglected by traditional deduction
systems.\Omega mega
employs an extension of the Proverb system [HF96] developed by our
group that allows for presenting proofs and proof plans in natural lan-
guage. In order to produce coherent texts that resemble those found in
mathematical textbooks, Proverb employs state-of-the-art techniques
of natural language processing.
Due to the possibly hierarchical nature
of\Omega mega proofs, these can
be verbalised at more than one level of abstraction, which can be selected
by the user.
To summarise our view of proofs, for every theorem an explicit proof
has to be constructed so that on the one hand it can be checked by
a proof checker, on the other hand the system provides support to
represent this proof in a high-level form that is easily readable by
humans [HF96]. Neither the process of generating proofs nor that of
checking them is fully replaced by the machine but only supported. If
a human mathematician wants to see a proof, he/she can do so at an
appropriate level of abstraction.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 9
3.2. Proof Planning
The central data structure for the overall process is the Proof plan Data
Structure (PDS). This is a hierarchical data structure that represents
a (partial) proof at different levels of abstraction (called proof plans). It
is represented as a directed acyclic graph, where the nodes are justified
by (LCF-style) tactic applications. Conceptually, each such justification
represents a proof plan (the expansion of the justification) at a lower
level of abstraction that is computed when the tactic is executed 1 . In
\Omega mega, we explicitly keep the original proof plan in an expansion hier-
archy. Thus the PDS makes the hierarchical structure of proof plans
explicit and retains it for further applications such as proof explanation
or analogical transfer of plans.
Once a proof plan is completed, its justifications can successively
be expanded to verify the well-formedness of the corresponding PDS.
This verification phase is necessary, since the correctness of the different
components (in particular, that of external ones like automated
theorem provers or computer algebra systems) cannot be guaranteed.
When the expansion process is carried out down to the underlying
ND-calculus (natural deduction), the soundness of the overall system
relies solely on the correctness of the verifier and of ND. This also
provides a basis for the controlled integration of external reasoning
components if each reasoner's results can (on demand) be transformed
into a sub-PDS. The level to which the proofs have to be expanded
depends on the sophistication of the proof checker. As pointed out by
Barendregt [Bar96], a more complex proof-checker that accepts proofs
in a more expressive formalism may drastically reduce the length of the
communicated proofs. If the high-level justifications are not expanded
but accepted as they are, our approach reduces to one in which the computer
algebra system is fully trusted. In short, the hierarchical nature
of the PDS supports the full spectrum of user preferences, from total
trust in the CAS, over partial trust in certain levels to full expansion
of the proofs in a detailed calculus level description that is machine
checkable.
A PDS can be constructed by automated or mixed-initiative plan-
ning, or pure user interaction that can make use of the integrated tools.
In particular, new pieces of PDS can be added by directly calling tac-
tics, by inserting facts from a database, or by calling some external reasoner
(cf. Section 3.3) such as an automated theorem prover or a computer
algebra system. Automated proof planning is only adequate for
1 This proof plan can be recursively expanded, until we have reached a proof plan
that is in fact a fully explicit proof, since all nodes are justified by the inference rules
of a higher-order variant of Gentzen's calculus of natural deduction (ND).
M. KERBER, M. KOHLHASE, V. SORGE.
problem classes for which methods and control knowledge have already
been established.
The goal of proof planning is to fill gaps in a given PDS by forward
and backward reasoning [HKKR94] (proof plans were first introduced
by Bundy, see [Bun88, BSvH 93]). Thus from an abstract point of
view the planning process is the process of exploring the search space
of planning states that is generated by the plan operators in order to
find a complete plan from a given initial state to a terminal state.
\Omega mega's proof planner is an extension of the well-known STRIPS
algorithm that can be evoked to construct a proof plan for a node
(the goal node) from a set I of supporting nodes (the initial state)
using a set Ops of proof planning operators, here called methods. A
method is a (partial) specification of a tactic in a meta-level language.
In\Omega mega planning is combined with hierarchical expansion of methods
and precondition abstraction. The plans found by this procedure are
directly incorporated into the PDS as a separate level of abstraction.
In this model, the actual reasoning competence of the planner and
the user builds upon the availability of appropriate methods together
with meta-level control knowledge that guides the planning. At the
moment,\Omega mega provides user-defined method ratings as a means of
control and can use analogy as a control strategy of the planner. Two
examples of methods are displayed in the section on the extended exam-
ple, Section 3.4.
3.3. Integration of Computer Algebra Systems as
External Reasoners
According to the different modes
of\Omega mega there are different levels
on which an external reasoning system, RSys, can be integrated:
Interactive calls, RSys is represented as a command call-RSys
that invokes the reasoner on a particular subproblem and returns
the result,
Proof planning, RSys is represented as a method whose specification
contains knowledge about the problem solving behaviour
and option settings for RSys.
Justifications, RSys can serve as a justification of a declaratively
given subgoal that is left to be proved by RSys.
In any case, the proof found by RSys must eventually be transformed
into a PDS, since this is the proof-theoretic basis
of\Omega mega. For automated
theorem provers like Otter [McC94], we described the integration
in [HKK + 94] and the necessary proof transformation to PDS
in [HF96], so we will not pursue this matter here. The integration of
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 11
CAS follows the same paradigm and is the main topic of this paper,
so we will develop the paradigm for the case of external computations
in\Omega mega. We will see examples for the three different levels of integrations
of a CAS
into\Omega mega in the example in the next section, so
we will not go into that here. This leaves us with the question of the
transformation of the CAS results into PDS.
If we take the idea of generating explicit PDS seriously also for
computations we can neither just take existing systems nor follow the
approach of meta-theoretic extensions,
since\Omega mega is a classical proof
system and does not use constructive logic. On the other hand we cannot
using them even in cases, where the verification of a calculation
is much easier than the calculation itself (e.g., integration
of functions); the computation needed for verifying alone is in many
cases still much too complicated to be automatically checked without
any guidance. For instance even the proof for the binomial formula
problem for any computer algebra
system) needs more then 70 single steps in the natural deduction
calculus 2 . Thus using theorem provers or rewriting systems to find such
proofs can produce unnecessarily large search spaces and thus absorb
valuable resources. On the other hand such proofs show a remarkable
resemblance to algebraic calculations themselves and suggest the use of
the CAS not only to instantly compute the result of the given problem,
but also to guide a proof in the way of exploiting the implicit knowledge
of the algorithms. We propose to do this extraction of information not
by trying to reconstruct the computation in the MRS after the result
is generated - as we have seen, even in case of a trivial example for a
CAS this may turn out to be a very hard task for an MRS - but rather
to extend the CAS algorithm itself so that it produces some logically
usable output alongside the actual computation. Surely in most cases
a user would not like to see proofs at a level where the binomial formula
is explained (although a novice might want to). This means that a
hierarchical approach to proof generation is appropriate, in which the
abstraction level of the proof presentation can be chosen by the user.
Our approach is to use the mathematical knowledge implicit in the
CAS to extract proof plans that correspond to the mathematical computation
in the CAS. So essentially the output of a CAS should be
transferable into a sequence of tactics, which presents a high-level
description for the proof of correctness of the computation the CAS
has performed. Note that this does not prove general correctness of
the algorithms involved, instead it only gives a proof for a particular
instance of computation. The high-level description can then be used to
2 Proofs of this length are among the hardest ever found by totally automatic
theorem provers without domain-specific knowledge.
M. KERBER, M. KOHLHASE, V. SORGE.
produce a readable explanation or further expanded to a level that can
be automatically checked by proof checkers. The level of abstraction
on which the checking can take place, depends on the level of sophistication
of the proof checker. For a naive proof checker, the proof must
be expanded to an explicit calculus level. The decision to extract proof
plans rather than concrete proofs from the CAS is essential to the goal
of being verbose without transmitting too much detail.
For our purpose, we need different modes, in which we can use the
CAS. Normally, during a proof search, we are only interested in the
result of a computation, since the assumption that the computation
is correct is normally justified for established CAS. When we want to
understand the computation - in particular, in a successful proof - we
need a mode of the CAS that gives enough information to generate a
high-level description of the computation in terms of the mathematics
involved. This is is described in the next section in detail. Before
doing so we describe how the integrated system automatically solves
an extended example from an economics examination.
3.4. Extended Example
The concrete task at hand is to minimise the costs for running a
machine while producing a certain product.
Problem: The output of a machine can range over a certain interval,
the interval I = [1; 7]. The cost of the product prod is determined by
the costs of water and electricity for producing prod , which are given
by the functions
prod ffl r
prod
and the prices for water and electricity
kWh
Determine the output d in I of the machine such that the total costs
are minimal.
This example serves our purposes for several reasons. Firstly, it
allows us to show the interaction of proof planning with symbolic computation
and the extraction of proof plans from calculations. Secondly,
the mathematics involved is simple enough to be fully explained (only
simple polynomial manipulations are necessary). Thirdly, it is not an
example we created, but the problem is a slightly varied version of a
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 13
minimisation problem from a masters examination in economics at the
Universit?t des Saarlandes, Saarbr-ucken [WiW89].
In order to solve problems like this, we have integrated a simple CAS
into\Omega mega, called -Cas 3 .
The -Cas-system is very simple and can at the moment only perform
basic polynomial manipulations and differentiation, but it suffices
for automatically solving the example at hand. Clearly, for a practical
system for mathematical reasoning, a much more developed system
like Maple [CGG + 92], Reduce [Hea95], Axiom [JS92], or Mathematica
[Wol96] has to be integrated. The technicalities of the integration
will be described in Section 4.
For the formalisation of the example, we use the theory mecha-
nism
of\Omega mega to create a theory economy (see Figure 1) that contains
the domain-specific knowledge (both the factual and the method
knowledge) needed for the problem solution. Obviously, we need a
background theory of costs in economics (that handles both numerical
parts and denomination of cost functions) and one of minimisation
of real functions, therefore, our theory inherits material from the theories
costs and calculus. The calculus theory is provided
by\Omega mega
and contains relevant parts of the knowledge of an elementary calculus
textbook: For instance, the real numbers are introduced as a complete,
dense archimedian field (based on elementary algebraic notions such
as groups and rings defined in the respective theories). The set of real
numbers (showing the existence of such a complete, dense archimedian
field) are constructed as the quotient field of the ring of sequences of
rational numbers over the ideal of null-sequences. The rational numbers
in turn are constructed as signed fractions of natural numbers
that are defined from the Peano axioms in theory natural. All of these
mathematical theories are based on the theories function, set, and
relation, that specify naive (simply typed) set theory and the properties
of functions and relations on such sets. Finally, the whole hierarchy
builds on the theory base, which declares the underlying logic
by providing the logical connectives and quantifiers and the basic ND
inference rules.
The theory economy provides a type AE of units that covers the different
units of denominations - in our example
(for work), prod (for product) and DM (for the price). We then formalise
prices as triples consisting of one real number and two units
and cost functions as a real function together with two units (read as
input/output units). Note, that just as in the real world, addition (\Phi)
3 The -Cas system is part of the standard distribution
of\Omega mega which can
be obtained from http://www.ags.uni-sb.de/software/deduktion/omega. The
example is accessible as WiWi-Exam in the theory economy.
14 M. KERBER, M. KOHLHASE, V. SORGE.
base
set relation
struct
ordered
topology
ordered-ring
ordered-field
metric-space
natural
rational
sequences
integer
semigroup
group
ring
field
monoid
real
calculus
economy
costs

Figure

1. Theory hierarchy
in\Omega mega's knowledge base
multiplication
(\Omega ) and comparison of costs and cost functions is defined
as that of their real parts with respect to the denominations. For these
calculations we have the axioms CF1 and CF2. If two denominations
differ, we can relate them by their prices, for this purpose we use axiom
Pr.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 15
Optimisation in economy is formalised by a predicate Opt on a cost
function cf(f; DM ; prod ) and an interval I that is true, whenever f
has a total minimum 4 on I.
O Opt(cf(f; DM ; prod ); I) , 9x TotMin(x; f; I)
Thus we can state the problem as the following formula 5
where H is a set of hypotheses that are needed for the complete proof,
for instance the price axioms
The planner solves the problem by generating a high-level proof
plan consisting of methods from its domain specific method base on
economics exam questions 6 .
We are going to outline this process by describing its major steps.
In particular, we will demonstrate how the proof planner
of\Omega mega
and the -Cas-system interact, and make explicit, on which entries of
a mathematical database this interaction depends. The planner finds
the following simple proof plan:
4 The predicate TotMin and the problem solving knowledge related to it is inherited
from the theory calculus.
5 Actually the formalisation of the problem is not fully correct, since the examiner
is not only interested in the proof that there exists such an x, but he/she wants to
know the value of x as well as a proof that this value fits the requirements. Obviously,
such an answer cannot be obtained from the formula here, but only from a proof
that is constructive for the variable x, where we can extract a witness term. This
is no problem for a CAS nor for an MRS based on constructive logic, but for a
traditional MRS based on classical logic, the proof construction process has to be
refined to guarantee constructivity for x. Note that the arguments, why the witness
for x meets the requirements can still be classical and non-constructive.
this means that the proof planner may only use methods in our proof plan that are
constructive to get the wanted answer as presented here and not a non-constructive
abstract argument. Finally note that this phenomenon is another argument in favour
manipulating explicit proofs. Without this, one may find oneself in the position, that
one is convinced (by meta-theoretic arguments) of the existence of a (constructive)
proof, but in fact without one from which to extract a term witness to answer the
exam question.
6 Questions for certain standard exams are a good example for a very restricted
mathematical domain, since the proofs and calculations involved are highly stan-
dardised. Therefore finding the proof plan in this example is not a big problem for
\Omega mega.
M. KERBER, M. KOHLHASE, V. SORGE.
Mult-by-Price
3 Add-by-Denom
5 TotMin-Rolle
where the first three methods compute the actual cost function by
adjusting the denominations and adding. Method 4 uses Axiom O for
optimisation. As the example only contains polynomials of degree two,
the planner selects a method TotMin-Rolle (cf. Figure 3) for finding
total minima that makes implicit use of Rolle's theorem from the
calculus theory:
Let f be a polynomial of degree two, then f has a total minimum at
has a minimum at x and f(a) - f(x) - f(b).
Formally we get the following equivalence:
TotMin TotMin(x; f; [a; b]) , x 2 [a;
Note that Rolle's theorem is accessible in the current theory and, to
ensure correctness, the database has to contain its formal proof.
Now let us take a closer look at some of the methods in order to get a
feeling of how this initial proof plan can be expanded. In Figures 2 and 3
we have given slightly simplified presentations of the Mult-by-Price
and TotMin-Rolle method 7 .
The declaration slot of the method simply defines the meta-variables
used in the body of the method. The premises, conclusions, and the
constraint describe the applicability of the method. In the example of
Mult-by-Price, for instance, line L 4 has to be present and to be an
open subgoal, while L 1 and L 3 are lines that can be used in order to
has to be given already, whereas L 3 is generated by the
application of the method (indicated by the \Phi). Since the method is
intended to prove L 4 , after the application of the method, this line can
be deleted from the current planning state (we indicate this by the \Psi).
In the constraint slot further applicability criteria are described, which
cannot be formulated in terms of proof line schemata. Declarations,
premises, constraints, and conclusions form the specification part of
the method. In order to be able to mechanically adapt methods the
tactic part is further subdivided into the declarative content and the
procedural content. (However, this particular feature is not important
for the purpose of this paper.) In our examples the procedural content
7 We have especially adjusted the syntax of the constraint in a way that is more
comprehensive for the reader.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 17
Declarations
just
f;
Premises
Constraint
Conclusions \PsiL 4
Declarative
Content
(=subst L 3
Procedural Content schema \Gamma interpreter

Figure

2. The Mult-by-Price method from theory cost.
consists of a schema-interpreter, which essentially inserts the declarative
content (using the bindings made in the planning phase) at the
correct place in the current partial proof tree. In the concrete example
the lines L 1 through L 4 are inserted (Note that we adopted a linearised
version of ND proofs as introduced in [And80]).
In order to understand to which piece of actual proof these methods
evaluate, we have to examine the declarative content and the bindings
performed in particular in the constraint. The constraint of the
Mult-by-Price-method states a rather simple computation: if there is
a cost function in the given open line which has a denomination other
than DM, it is multiplied with the appropriate price. The multiplication
of the real parts is carried out by the CAS and the corresponding cost
function is constructed. As this point is crucial for understanding the
working scheme of a method we will view the bindings in the constraint
step by step: When applied to the current plan the method is matched
with the open goals of the planning state. The first pass of the planner
yields that L 4 can be matched with our theorem THM. Thus its formula
Opt([cf(-d 0:5d
is bound to the meta-variable OE. It is then examined to find an occurrence
of a cost function. If such a subterm exists its arguments are
bound to g; v; w and by matching line L 1 we receive the numerical part
of price in f (if the appropriate price is not provided the application of
the method would fail here). Afterwards the new cost function is com-
M. KERBER, M. KOHLHASE, V. SORGE.
puted (according to axiom Pr) in / 0 and finally OE 0 contains the result
of replacing the old cost function in OE by / 0 . Hence in the first plan
step the optimisation formula stored in OE 0 contains the cost function
cf(-d prod ) as a subterm.
With all these meta-variables instantiated the subproof contributed
by the Mult-by-Price-method consists of lines L 2 and L 3 in the
declarative content. Here we observe that L 2 results from applying the
price-axiom Pr (which is fetched from the database) to line L 1 . Furthermore
note that in L 3 we have a call to the CAS as a justifying
method for the line. This means that at this point in the proof planning
procedure, the CAS is called in order to compute the product of
price and original cost function. The line resulting from this calculation
is then used as the new open subgoal in the planning state.
Summarising the effects of the method Mult-by-Price can be
observed in two steps. First the goal line THM is justified with the
method yielding the following subproof:
Then the method in the justification of line THM (which has been
abbreviated due to a lack of space) could be expanded thereby inserting
the intermediate steps as described above by instantiating the macro
steps of the method. Note that the following expanded subproof is at
a more detailed level of abstraction in the PDS. In particular, the
justification of THM itself is different at this level.
cf(-d
In the proof of THM, the method Mult-by-Price is applied twice
in order to normalise both summands. To preserve space we will
not present the next two methods of our proof plan as extensively
as the Mult-by-Price-method. Add-by-Denom is very similar to
Mult-by-Price and applies axiom CF1 inside the optimisation function
Opt to compute the final cost function. In its course the CAS is called
once to perform a polynomial addition. Then the Optimise-method
simply introduces the definition for the Opt function of axiom O.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 19
Declarations
just
Premises
Constraint degree(OE)
y / compute with CAS(minimum; OE)
Conclusions \PsiL 12
Declarative
Content
(a
Procedural Content schema \Gamma interpreter

Figure

3. The TotMin-Rolle method from theory calculus.
Far more interesting than these two methods is the TotMin-Rolle-
method as it contains a different example for the use of a CAS in
\Omega mega. Again the presentation of the method in Figure 3 is simplified.
The TotMin-Rolle method is applied at a stage of the proof where
the actual minimum of the cost function has to be introduced. This task
is fulfilled within the constraint of the method. The compute with CAS
statement actually calls the CAS in quiet mode to compute the minimum
of the function OE and store it in the meta-variable y. At this
stage, the CAS is used as an oracle here, just as in [HT93a]. In our
example the minimum of the cost function is at and the ND-line
of the form
will be transformed by eliminating the existentially quantified variable:
M. KERBER, M. KOHLHASE, V. SORGE.
The rest of the proof plan is devoted to proving that the result is actually
a total minimum. This is done by using the definition for TotMin
from the database and furthermore by using the definitions for minimum
and interval which correspond to line L 1 and L 2 in the method
TotMin-Rolle. These definitions are introduced in lines L 9 through L 11
by applying them to the correct assertions given in lines L 3 through
L 8 . This is expressed by the justifications in the corresponding lines; for
instance, the justification of line L 10 states that we can infer y 2 [ff; fi]
from the lines L 5 and L 6 with the definition of interval in line L 2 .
A closer look at the justifications of lines L 3 through L 8 reveals that
these contain methods themselves. Lines L 3 and L 4 again depend on
calculations of the CAS which computes the first and second derivative
of our cost function. The justifications Simplify correspond to a method
performing basic arithmetic simplifications and comparisons.
Consisting of only 5 methods the above proof plan gives the impression
of a small proof and on an abstract level it is indeed; an experienced
mathematician might not want to see more. But expanding the plan
into a partially grounded ND proof gives it a length of 90 lines, containing
lines justified by the CAS. The proof on this level may roughly
correspond to a proof that a novice would like to see and that would
form a reasonable solution of the exam problem once it is presented in
natural language by the Proverb system. By rerunning the CAS in a
proof plan generating mode on the CAS-justifications and extracting
proof plans, the proof can be expanded to a more detailed proof plan
containing an account of the mathematics behind the calculations. This
proof plan already contains 135 plan steps and - if the user does not
feel comfortable with the level of detail yet - can then be expanded
to a calculus-level ND proof of length 354. Note that even this proof
is not a stand-alone proof of the minimisation theorem, but depends
on the proofs of a number of lemmata from a database. Furthermore,
in these proofs the simplification of ground arithmetic expressions is
not expanded, for instance, into a representation involving zero and
the successor function either, which would be necessary to obtain a
detailed logic-level proof.
4. Integrating Computations into Explicit Proofs
In this section we describe Sapper (System for Algorithmic Proof
Plan Extraction and Reasoning), which generates proof plans from
CAS output. As mentioned in Section 3.3, for the intended integration
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 21
it is necessary to augment the CAS with mathematical information for a
proof plan generating mode in order to achieve the proposed integration
at the level of proofs. For the -Cas system, which we have developed
to demonstrate the feasibility of the approach, this was rather simple, as
we will demonstrate below. Enriching a state of the art CAS with such
a mode for producing the necessary additional protocol information,
would of course require a considerable amount of work.
4.1. Architecture
The Sapper system can be seen as a generic interface for connecting
\Omega mega (or another proof plan-based mechanised reasoning system)
with one or several computer algebra systems (see Figure 4). An incorporated
CAS is treated as a slave
to\Omega mega which means that only
the latter can call the first one and not vice versa. From the software
engineering point of
view,\Omega mega and the CAS are two independent
processes while the interface is a process providing a bridge for com-
munication. Its r-ole is to automate the broadcasting of messages by
transforming output of one system into data that can be processed by
the other 8 .
Unlike other approaches (see [HC95, GPT96] for example) we do
not want to change the logic inside our MRS. In the same line, we
do not want to change the computational behaviour of the computer
algebra algorithms. In order to achieve this goal the trace output of the
algorithm is kept as short as possible. In fact most of the computations
for constructing a proof plan is left to the interface. The proof plans
can directly be imported
into\Omega mega.
This makes the integration independent of the particular systems,
and indeed all the results below are independent of the CAS employed
and make only some general assumptions about the MRS (such as being
proof plan-based). Moreover, the interface approach helps us to keep
the CAS free of any logical computation, for which such a system is not
intended anyway. Finally, the interface minimises the required changes
to an existing CAS, while maintaining the possibility of using the CAS
stand-alone. The only requirement we make for integrating a particular
CAS is that it has to produce enough protocol information so that a
proof plan can be generated from this information. The proof plan in
turn can be expanded by the MRS into a proof verifying the concrete
computation.
The interface itself can be roughly divided into two parts; the translation
part and the plan generator. The first performs syntax translation

between\Omega mega and a CAS in both directions while the latter
8 This is an adaptation of the general approach on combining systems in [CMP91].
22 M. KERBER, M. KOHLHASE, V. SORGE.
Hypotheses
Theorems
Methods
Tactics Tactics
Methods
Theorems
Hypotheses
Interface
Structured Database
Theory 1
Theory 1.1 Theory 1.2
Theory 2
result
access
access
plan tactics
knowledge
call
result
Plan Generator
Translator
Function Mappings
call
args.

Abstract

-CAS

Figure

4. Interface
between\Omega mega and computer algebra systems
only transforms verbose output of the CAS
to\Omega mega proof plans.
Clearly only the translation part depends on the particular CAS that
is invoked.
For the translations a collection of data structures - called abstract
CAS 9 - is provided each one referring to a particular connected CAS
(or just parts of one). The main purpose of these structures is to specify
function mappings, relating a particular function
of\Omega mega to a corresponding
CAS-function and the type of its arguments. Furthermore
it provides functionality to convert the given arguments of the mapped
\Omega mega function to CAS input. In the same fashion it transforms results
of algebraic computations back into data that can be further processed
by\Omega mega. The functionality in this part of our interface offers us the
possibility of connecting any CAS as a black box system, as in the first
approach we have described in Section 2. For instance, we may want to
use a very efficient system without a mode for generating proof plans in
9 In a reimplementation of Sapper we would probably use the OpenMath protocol
[AvLS96] as a lingua franca on the CAS side.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 23
proof search as black box system, and then another less efficient system
with such a mode for the actual proof construction, once it is clear what
the proof should look like. This corresponds to recent techniques used
in knowledge based systems, where the explanation component is not
just a trace of the rules applied during the search, but the explanation
is reconstructed by an independent component.
The plan generator solely provides the machinery for our main goal,
the proof plan extraction. Equipped with supplementary information
on the proof
by\Omega mega it records the output produced by the particular
algebraic algorithm and converts it into a proof plan. Here the
requirements of keeping the CAS side free of logical considerations and
on the other hand of keeping the interface generic seem conflicting at
the first glance. However, this conflict can be solved by giving both sides
of the interface access to a database of mathematical facts formalising
the mathematics behind the particular CAS algorithms. Conceptually,
this database together with the mappings governing the access, provides
the semantics of the integration
of\Omega mega with a particular CAS. Thus
expanding the plan generator is simply done by expanding the theory
database by adding new tactics.
While\Omega mega itself can access the complete database, Sapper's
plan generator in the interface is only able to use tactics and lookup
hypotheses of a theory (cf. Figure 4). The CAS does not interact with
the database at all: it only has to know about it and references the
logical objects (methods, tactics, theorems, or definitions) in the proof
plan generating mode. Thus knowledge about the database is compiled
a priori into the algebraic algorithms in order to document their calculations

4.2. Proof Plan Extraction
Let us now take a closer look at the implementation of the proof plan
generation in -Cas and at the expansion process of its output. This
should demonstrate how proofs can be extracted from computer algebra
calculation and provide an intuition on the requirements that our
approach poses on the CAS side.
As an example we will consider a polynomial addition from the
example above. Normally, an experienced mathematician would not
like to see any proof at all for that, while a high-school student
would like to. As we have seen in our example, the main purpose
of the Add-by-Demon-method is to compute the final cost function
prod ). This is done in -Cas by adding
the two polynomials -d d 3. In the remainder
of this subsection we will expand this addition in several steps and
thereby obtain a calculus level proof for the computation.
M. KERBER, M. KOHLHASE, V. SORGE.
Before examining this example in detail, let us consider the general
scheme of the proof plan generation inside the polynomial addition
algorithm of -Cas. We first take a look at the different representations
of a polynomial p in the variables x
r . The
logical language
of\Omega mega is a variant of the simply typed -calculus
(indeed we use a stronger type system, but here we want to keep things
as simple as possible), so the polynomials are represented as polynomial
functions, that is, as -expression where the formal parameters
are -abstracted (mathematically, p is a function of r argu-
For the notation, we use a prefix notation; the symbols +,   and "
denote binary functions for addition, multiplication and exponentiation
on the reals. In this representation, we can use fi-reduction for the
evaluation of polynomials.
In -Cas, we use a variable dense, expanded representation as an
internal data structure for polynomials (as described in [Zip93], for
instance). Thus every monomial is represented as a list containing its
coefficient together with the exponents of each variable. Hence we get
the following representation for p:
Let us now turn to the actual -Cas algorithm for polynomial
addition. This simple algorithm adds polynomials p and q by a
case analysis on the exponents 10 with recursive calls to itself. So let
r and
r . We have presented the
algorithm in the jth component of p and the kth component of q in a
Lisp-like pseudo-code in Figure 5. Intuitively, the algorithm proceeds
by ordering the monomials, advancing the leading monomial either of
the first or the second arguments; in the case of equal exponents, the
coefficients of the monomials are added.
Obviously, the only expansions of the original algorithm needed for
the proof plan generation are the additional (tactic.) statements 11 .
We assume a lexicographic monomial order and employ it for ordering the
exponents. Thus we make use of the operators ?, !, and = in an intuitive sense.
Furthermore we can define the rank of a monomial as the vector given by its exponents
and the rank of a polynomial as the maximum rank of its monomials with
respect to the lexicographic monomial order.
11 Observe that in this case, the called tactics do not need any additional argu-
ments, since our plan generator in the interface keeps track of the position in the
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 25
(poly-add (p q)
(tactic "mono-add")
(cons-poly
er j
r
(poly-add
r
r
(tactic "pop-first")
(cons-poly ff j x
er j
r
(poly-add
r
r
(tactic "pop-second")
(cons-poly
r
(poly-add
r
r )))

Figure

5. Polynomial addition in -Cas.
They just produce the additional output by returning keywords of tactic
names to the plan generator and do not have any side effects. In
particular, the computational behaviour of the algorithm does not have
to be changed at all.
If we now apply this algorithm to the two polynomials
we obtain the following proof plan:
(mono-add, pop-second, mono-add)
First the two quadratic monomials from p and q are added, then the
linear term of q (the second argument) is raised, since it only appears
in one argument, and finally the remaining monomials are added up.
In the case of the polynomial addition, each of the methods (proof
plan operators) directly corresponds to a tactic with the same name,
that is, the list of the three methods above directly represents a concrete
proof plan for polynomial addition of the concrete polynomials p and
proof and thus knows on which monomials the algorithm works when returning a
tactic. This way we need not to be concerned which form a monomial actually has
during the course of the algorithm.
26 M. KERBER, M. KOHLHASE, V. SORGE.
q. (In the following representation we omitted the context in which the
polynomials are embedded in the actual proofs.)
These four lines correspond to a step-by-step version of the basic
High School algorithm. So far the expansion of the call-cas-method
has been exclusively done by -Cas proof plan generation mode. But
at this stage -Cas cannot provide us with any more details about the
computation and the subsequent expansion of the next hierarchic level
can be achieved without further use of a CAS.
Let us for instance take a look at the pop-second tactic to understand
its logical content. The tactic itself describes a reordering in a
sum that looks in the general case as follows:
For the current example we can view a and c as arbitrary polynomials
and b as a monomial of rank greater than that of the polynomial a.
It is now obvious that the behaviour of pop-second is determined by
the pattern of the sum it is applied to. If in equation (1) the polynomial
c does not exist, pop-second is equivalent to a single application
of the law of commutativity. Otherwise, like in our example, the
tactic performs a series of commutativity and associativity steps. The
pop-second step above can thus be expanded in a plan which reflects
the single step applications of the laws of commutativity and associativity

Assuming we have expanded the two mono-add tactics as well, we
have constructed a representation of the proof at a level where it only
needs the axioms in the polynomial ring. To finally expand this to a fully
explicit calculus level proof, we further expand all three justifications
of the above lines. This leads to a sequence of eliminations of universally
quantified variables in the corresponding hypothesis, the axioms
of commutativity and associativity. In our example the commutativity
axiom would be transformed in the following fashion:
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 27
Here, the justification (THM) in the first proof line indicates that
the commutativity of was imported from the theory real
in\Omega megas
mathematical database, where it was established as a theorem. The
remaining lines are natural deduction inferences: universal eliminations
that instantiate a with the number 6 and b with the term \Gamma12x.
Altogether this single application of the pop-second-tactic is equivalent
to a calculus-level proof of 11 inference steps. The length of the
subproof for this trivial polynomial addition is 43 single steps. This
example shows how it is possible to mechanically construct a proof
verifying the correctness of any particular CAS computation without
verifying the CAS algorithm (or their implementation) in the general
case.
However, the calculus level proofs for the computations are very long
and rather boring and therefore hardly any human user might actually
want to see much less read them. Therefore the Proverb proof explanation
system
in\Omega mega provides a more realistic alternative, since it
gives the user access to representations of the parts of the proof on
various levels of abstractions making use of the hierarchical structure
of the underlying PDS. For instance, it is then possible to present
the computations with some intermediate steps, as it is customary in
textbooks. For example, we could include the three steps of the High
School algorithm mentioned above, to illustrate the polynomial addi-
tion. (The decision which steps should be included and which omitted,
depends of course on the expertise of readers for which a particular
proof presentation is intended.)
Despite all these abstractions in both developing and presenting the
proof, we can still use any proof checker for ND-calculus to verify all
steps including computations. Furthermore, if we assume we have a
more sophisticated proof checker, for example one that works modulo
the axioms of polynomial rings, it is also possible to check the proof
on an abstract level. As already mentioned, the more sophisticated the
proof checker is, the more concise the communicated proofs can be.
We have tested proof plan extraction from simple recursive and iterative
CAS algorithms, where it works quite well, since these algorithms
closely correspond to the mathematical definitions of the corresponding
concepts. However, more complicated schemes like divide-and-conquer
algorithms (for instance, the polynomial multiplication of Karatsuba
and Ofman [KO63]) cannot be adapted to our approach so easily without
extending the mathematical knowledge base by corresponding lemmata

28 M. KERBER, M. KOHLHASE, V. SORGE.
The example of the polynomial addition is surely a trivial one, we
have chosen it solely for presentation reasons. In particular it is very
likely to be correct in any real-world implementation, since it is well
tested and does not depend on sophisticated mathematical theorems
for which fuzzy boundary cases must be considered. For the sake of
argument, let us assume an error in the implementation, for instance,
in the second case of the polynomial addition algorithm in Figure 5
the cons-poly statement was forgotten, so that the algorithm has the
following (incorrect) form
(tactic "pop-first")
(poly-add
r
In the computation of ((x 2 that we have discussed
above, the second case is never used, and the computation would be
correct although the program is not.
If we now change the order of addition of our polynomials p and q to
we get the following incorrect result from the changed algorithm:
Inserting the proof plan generated by the faulty algorithm then yields
In checking, the proof checker would see that the pop-first step is
not justified, since the expansion corresponds to the application of the
law of associativity. This would yield
and thus would not be applicable during the expansion. Thus the proof
plan and consequently the calculation would be rejected
by\Omega mega.
Note that in a large system with literally millions of possible cases,
the correctness of a calculation like depends only
on a tiny subset of the whole program. It is a strength of our approach,
that only the calculations that are necessary for a given proof would be
checked. This has the advantage that errors on different levels can be
detected (in particular, on the levels of algorithms, of compilers, and
of processors). Of course, for very long computations checking can be
pretty expensive. Moreover, highly elaborated and efficient algorithms
in state of the art CAS might be hard to augment with proof plan
KeKoSo.tex; 24/06/1998; 14:49; no v.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 29
generation modes. As we have seen in the example above, the mathematical
knowledge in the database has to reflect the mathematical
knowledge in the algorithm in order to easily decorate the algorithms
by a proof plan generation mode. However, to extend and prove corresponding
lemmata is not a trivial task for sophisticated algorithms.
In particular such an approach would go very much in the direction of
program verification.
Even if it proves practically impossible to extract the information
that is valuable at the conceptual, mathematical level, it is always possible
to reserve these elaborated techniques for the quiet mode used in
proof discovery, and use more basic algorithms, for which the mathematics
is easier and that are more easily decorated by a proof plan generation
mode, for the proof extraction phase. Systems like Axiom [JS92]
or MuPAD [Fuc96] seem to come closest among standard CAS to the
needs for a proof plan generation, since one can already attach axiomatisations
to algorithms.
5. Conclusion
In this work we have reported on an experiment of integrating a computer
algebra system into the interactive proof development environ-
ment\Omega mega, not only at the system level, but also at the level of
proofs. The motivation for such an integration is the need for support
of a human user when his/her proofs contain non-trivial computations.
We have shown that the proof planning paradigm in general and the
\Omega mega system in particular provide an open environment for such an
extended integration that supports different integration levels.
In our approach it is not possible to use a standard CAS for the
integration as it is, since such a system provides answers, but no directly
usable justifications from which proof plans can be extracted. This,
however, turned out to be essential in an environment that is built to
construct communicable and checkable proofs.
In order to achieve a solution that is compatible with such a strong
requirement, we have adopted a generic approach, where the only
requirement for the CAS is that it has a proof plan generation mode for
the generation of communicable and checkable proofs. Since we want to
achieve the two goals simultaneously, namely to have high-level descriptions
of the calculations of the CAS for communicating them to human
users as well as low-level ones for mechanical checking, we represent
the protocol information in form of high-level hierarchical proof plans,
which can be expanded to the desired detail. Fully expanded proof
plans correspond to natural deduction proofs which can be mechani-
M. KERBER, M. KOHLHASE, V. SORGE.
cally checked by a simple proof checker. In the case that the CAS has
made a mistake the proof checker will detect it.
The general idea and the fundamentals of the integration of a CAS
into an MRS are independent from the concrete proof development
environment\Omega mega and the concrete computer algebra system -Cas.
It can be realised in any plan-based theorem prover. Proof extraction
can even be realised on any tactic-based system and with any CAS
that can protocol its calculations in form of tactics. Axiom [JS92] and
MuPAD [Fuc96] seem to be best suited for a corresponding extension
since one can already attach axiomatisations to algorithms. If in addition
the algorithms could be enriched in a way that they produce protocol
information in every computation step, that is, state which of the
attached axioms are used and what the particular instantiations are,
the system would probably fit in with our approach pretty well.
A useful extension of our approach would consist in the usage of
various algorithms for the same computation, for instance, one as a
fast and efficient algorithm that is not suitable for knowledge extraction
while searching for a proof. Afterwards, when actually documenting the
whole proof a less efficient algorithm, which is optimised to find short
proofs, can provide a complete proof plan.
Although the correctness issue can be achieved by a tactic-based
approach as well and does not need the specifications that are used in
proof planning, the full strength of an integration where considerable
automated support is provided cannot be achieved on this level, since
it is not possible to perform mechanical reasoning about the tactics.
Such an automation can, however, be achieved by the proof planning
approach, where the proof planner can automatically call a CAS pro-
cedure, when the conditions in the corresponding method are met. The
usefulness of an integration on this level can already be seen in the case
of our simple -Cas: After the integration we are able to prove optimisation
problems which were out of reach without such a support. On
the other hand, the system is able to give explanations of the involved
computations at various levels of abstraction. A feature that is missing
from todays CAS.
From our experiments we expect that the successful integration of
any powerful computer algebra systems would considerably enhance
the reasoning power of any mechanised reasoning system.

Acknowledgements

The work presented in this paper was supported
by the "Deutsche Forschungsgemeinschaft" in SFB 378, project
OMEGA. It benefited a lot from discussions in the Calculemus interest
group.
INTEGRATING COMPUTER ALGEBRA INTO PROOF PLANNING 31
The authors would like to thank Lassaad Cheikhrouhou for his help
with\Omega mega's proof planner and with coding the methods for our
examples. Furthermore, we would like to thank Deepak Kapur and the
anonymous referees for carefully reading earlier versions of the paper
and for their detailed comments that helped us to improve the presentation
considerably.



--R

Transforming matings into natural deduction proofs.

Objectives of OpenMath.
Technical Report 12
Computations and formal proofs in type theory.


Theorems and algorithms: An interface between Isabelle and Maple.


Rippling: A heuristic for guiding inductive proofs.
Symbolic Computation (An Editorial).
Using Mathematica for Doing Simple Mathematical Proofs.
"Intelligenzniveaus"

The use of explicit plans to guide inductive proofs.

Machine Proofs in Geome- try: Automated Production of Readable Proofs for Geometry Theorems


Mechanical geometry theorem proving.
Integrated software compo- nents: A paradigm for control integration
Implementing Mathematics with the Nuprl Proof Development System.


Reasoning theories - towards an architecture for open mechanized reasoning systems
An open environment for doing mathe- matics
Reduce user's manual: Version 3.6.
Computational metatheory in Nuprl.


Adapting methods to novel tasks in proof planning.
Presenting machine-found proofs
Extending the HOL theorem prover with a computer algebra system to reason about the reals.
Reasoning about the reals: The marriage of HOL and Maple.
AXIOM: The Scientific Computation System.
A refutational approach to theorem proving in geometry.



Otter 3.0 reference manual and guide.

The Mathematica Book: Version 3.0.
Mechanical Theorem Proving in Geometries: Basic Principles.
Texts and monographs in symbolic computation.
Effective Polynomial Computation.
Address for correspondence: Manfred Kerber School of Computer Science The University of Birmingham Birmingham B15 2TT
--TR

--CTR
Andreas Meier , Martin Pollet , Volker Sorge, Comparing approaches to the exploration of the domain of residue classes, Journal of Symbolic Computation, v.34 n.4, p.287-306, October 2002
Paul Cairns , Jeremy Gow, Integrating Searching and Authoring in Mizar, Journal of Automated Reasoning, v.39 n.2, p.141-160, August    2007
Micheal Kohlhase , Andreas Franke, MBase: representing knowledge and context for the intergration of mathematical software systems, Journal of Symbolic Computation, v.32 n.4, p.365-402, October 2001

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-based testing'
 'zero storage biometric authentication' 'model checking']
marked:['proof checking', 'computer algebra', 'hierarchical proof planning', 'mechanized reasoning']
--T
Ordered Semantic Hyper-Linking.
--A
The ordered semantic hyper-linking strategy is complete for first-order logic and accepts a user-specified natural semantics that guides the search for a proof. Any semantics in which the meanings of the function and predicate symbols are computable on ground terms may be used. This instance-based strategy is efficient on near-propositional problems, is goal sensitive, and has an extension to equality and term rewriting. However, it sometimes has difficulty generating large terms. We compare this strategy with some others that use semantic information, and present a proof of soundness and completeness. We also give some theoretical results about the search efficiency of the strategy. Some examples illustrate the performance of the strategy.
--B
Introduction
There are at least two basic approaches to the study of automated deduction. One
approach concentrates on solving hard problems, especially those of interest to
human mathematicians. There have been some notable successes in this area, and
even some proofs of hitherto unproven conjectures. This has served to give the
field of automated deduction some respectability among mathematicians and the
general public. Such proofs may be done with or without human interaction. For
this approach, it is of secondary interest whether the prover that solves these hard
problems is very weak for other, easy problems. Another approach to automated
deduction concentrates on building provers that perform well on a broad range of
problems, with a minimum of human interaction. In this approach, it does not make
sense to try hard problems if one's prover still has difficulty with easy problems.
The philosophy is to develop as far as possible theorem provers that have general
problem solving ability. The advantage of this approach is that one is likely to obtain
provers that are more well-rounded, and in the long run possibly more powerful; a
disadvantage is that the results may appear less spectacular, especially in the early
stages.
We have concentrated on the latter approach. In this endeavor, we have developed
a number of provers over the recent years, including the modified problem
reduction format of [Pla88] and its extensions, the clause linking method of [LP92],
and clause linking with semantics [CP94a], among others. We have emphasized
first-order logic without equality. These provers have become increasingly more
powerful, each able to solve a considerable range of problems out of reach of its pre-
decessor, with little or no human guidance. In addition, these provers can compete
respectably with well-known powerful theorem provers on certain types of problems.
For this, we have emphasized Prolog implementation, as a way to rapidly implement
and test a wide variety of ideas with little manpower. This means that our
provers have a disadvantage with respect to provers carefully implemented in C or
LISP, since the underlying language is less efficient; despite this, the performance
has been impressive. We now would like to carry this investigation a step further,
and incorporate some kind of ordering methods.
hyper-linking [CP94a] was developed to retain the propositional advantages
of hyper-linking [LP92] while adding natural semantics and goal-sensitivity
[Pla94a]. We discuss the issues of propositional efficiency and goal-sensitivity in
[Pla94a, Pla94b], thereby highlighting what we feel are some inefficiencies in many
common theorem proving strategies. Even hyper-linking without semantics performs
much better than resolution and similar strategies on some hard problems,
particularly non-Horn problems. We show in [CP94a] that hyper-linking with semantics
is sometimes much better still. However, there are still some problem with
semantic hyper-linking that we would like to solve. The cooperation between semantic
hyper-linking and rough resolution does not seem as clean as it could be.
Rough resolution [CP93b] is a version of ordered resolution developed to eliminate
large literals from proofs; this is helpful because semantic hyper-linking has difficulties
generating such large literals. Therefore the cooperation of these two methods
seems attractive, and indeed, we show in [CP93b] that this cooperation improves
the effectiveness of semantic hyper-linking on a number of problems. However, the
definition of rough resolution seems arbitrary; the logical way to eliminate large literals
is to use ordered resolution, as described in [BG90, HR91]. Also, the manner
in which the semantic tree is constructed and searched seems to have an arbitrary
element to it; this also makes this part of the method harder to describe formally. In
addition to semantic hyper-linking and rough resolution, UR (unit resulting) resolution
is a component of the prover described in [CP94a]. The motivation for this is
that rough resolution eliminates large literals from proofs, UR resolution eliminates
the Horn parts of proofs, and what remains is a non-Horn proof with small literals;
clause linking performs well on such problems. However, the Horn property is really
irrelevant here, since clause linking performs well on problems with small literals,
whether they are Horn problems or not. Therefore it seems logical to eliminate
UR resolution. Also, the choice of which rough resolutions and UR-resolutions are
performed seems to be arbitrary, to some extent; we prefer small clauses and small
proofs, essentially. Our motivation in the development of ordered semantic hyperlinking
is to simplify semantic hyper-linking as much as possible, and in this way
hopefully to extend its peaks of strength to a wider class of problems. This should
also allow for a small and easily understood implementation.
In addition, we are interested in removing some of the propositional inefficiencies
from term-rewriting based theorem proving strategies. Such strategies essentially
reduce to A-ordering [Sla67] on first-order logic without equality. However,
in [Pla94b] we show some simple sets of clauses on which A-ordering produces an
exponential search space, regardless of the ordering chosen. On the other hand,
term-rewriting methods are often very efficient, and extend naturally to other specialized
theories [BG94]. Furthermore, certain sets of clauses are easily decided by
strategies based on ordering [FLTZ93]. That is, these ordering based strategies are
a decision procedure, always terminating and indicating whether the set of input
clauses is satisfiable or not. However, these sublanguages of first-order logic are
not decidable in this way by clause linking. Therefore, we would like to present
semantic clause linking in a format that facilitates the transition to term-rewriting
based theorem proving strategies, thereby also providing a way to remove some of
their notable inefficiencies, while preserving some of their advantages.
The idea of semantic hyper-linking is to show that a set S of clauses is unsatisfiable
by the failure of a systematic search for a model of S. Ordered semantic
hyper-linking is similar, but it organizes the search a little differently. The basic
principle behind ordered semantic hyper-linking is the following: Suppose we have
a set of independent choices to make in a process of examining a set
of possibilities. Thus there are potentially 2 n combinations of choices altogether.
Then we assume that there is an ordering on these choices, and we make the simplest
choice first. Suppose p 1 is the simplest of the choices; then we first consider
the alternatives p 1 and not(p 1 ). For each such alternative, we recursively attempt
to solve the problem. The reasoning is that we may solve the problem before the
more complex choices are even seen, thereby saving effort. This seems to be a natural
strategy from the standpoint of human problem solving. In the application to
theorem proving, the set of choices is infinite, and the order in which they are made
has other implications, but the idea is still the same.
Furthermore, a problem with semantic hyper-linking is that sometimes the enumeration
of ground terms is necessary. We would like to have a method that is
based on unification instead of on the enumeration of ground terms. The proposed
method incorporates unification in a natural way, and for certain kinds of semantics
we show that the enumeration phase can be done in polynomial time. There is
an additional reason to believe that this new version will have better performance.
The work required by semantic hyper-linking is strongly influenced by the number
of "eligible literals" that are generated. The proposed method should reduce
this number, thereby making the method more efficient and permitting proofs that
require more rounds of search.
We also consider some complexity issues, as discussed in part by [Gou94]. His
approach is non-clausal, but the same analysis applies to a clausal framework. He
shows that the fundamental problem associated with the mating [And81] or connection
[Bib87] approach is \Sigma p
2 complete. These approaches first choose a number
of copies of each of the input clauses, and then seek to find a single substitution
that makes the given set of copies of the clauses propositionally unsatisfiable. (For
a general set of clauses, an arbitrary number of copies may be needed.) Goubault's
result has the consequence that the effort to prove a theorem is at worst exponential
in the size of a minimal proof, using a suitable implementation of these approaches,
where proof size is measured by the number of instances of the input clauses that
appear in the proof. More precisely, if S is the set of input clauses and each clause
appears at most n times in the proof, then the effort is exponential in n times the
size of S, written as a character string. This is actually not a bad bound; many
other methods are considerably worse, at least relative to this measure of proof size.
For example, clause linking can be double or even triple exponential in this measure
of proof complexity. The reason is that a proof containing n copies of the input
clauses may involve a number of unifications proportional to n; each such unification
can increase the size of the terms by a constant factor. Thus terms that are
exponentially large can be generated, the number of terms within this exponential
size bound is double exponential, and the time to handle a double exponential set
of ground clauses can be triple exponential. By altering the measure of term size,
so that repeated subterms are counted only once, this can be reduced to double
exponential. We consider the behavior of ordered clause linking and argue that
although the worst-case bound is double exponential, there is reason to believe that
in many cases this performance will be single exponential or even better. Of course,
there may be proofs in which many copies of the clauses are needed but the term
sizes are all small; for such clauses, clause linking would probably be faster than
the mating approach.
It is interesting that our proposed theorem prover incorporates a number of
well-known AI techniques, including case analysis, explanation-based generaliza-
tion, procedural semantics (to describe the semantics of the set of clauses), back-
tracking, ordering criteria, and of course unification and first-order logic. Therefore
this prover may have some independent interest from the standpoint of artificial
intelligence.
2 Orderings on Interpretations and Clauses
The idea of ordered clause linking with semantics is to place a total ordering on
the set of atoms, and then based on this to define a lexicographic total ordering
on the set of interpretations of these atoms. Now, semantic hyper-linking can
be seen as a systematic search for a model of a set S of clauses; if this search
fails, then we know that S is unsatisfiable. During this search, a semantic tree
is essentially constructed. Ordered semantic hyper-linking works in a similar way.
In fact, both strategies are somewhat similar to model elimination in this respect
[Lov69]; this similarity may be more apparent for ordered semantic hyper-linking
than for semantic hyper-linking. However, instead of a semantic tree, we have a
transfinite semantic tree, as in [HR91]. Also, in ordered semantic hyper-linking we
specify more precisely how this tree is constructed. That is, the interpretations
are examined in a sequence I consistent with the total ordering; the first
interpretation I 0 to be considered is the one that is least in this ordering. For this
interpretation we find a clause C 0 not satisfied by I 0 ; this clause is a minimal such
contradicting clause in a specified ordering on clauses. The next interpretation I 1
considered is the smallest interpretation that is not "obviously" contradicted by the
clauses found so far (in this case, C 0 ). The search continues in this manner, so we
have I only is the sequence in which the
interpretations are examined completely determined in this way, but the choice of
which clauses C i are found is also largely determined (up to minimality). If S is
unsatisfiable, eventually the set fC will be unsatisfiable; this will
be detected by the prover, and the search will stop.
We now define these orderings on atoms and interpretations more precisely.
We assume that some first-order language is specified in terms of a finite set F
of function and constant symbols, a finite set P of predicate symbols, and a list
X of variables. Then we are interested in the satisfiability problem of sets S of
first-order clauses over this language. Let T be the set of terms formed from
the function symbols F and the variables X , and let T (F) be the set of ground
terms (terms without variables) formed from T and F . Let A be the set of atoms,
that is, expressions of the form P
For orderings !, we define x ? y as equivalent to y ! x. A partial ordering
is said to be well-founded if there are no infinite sequences x
We assume that there is a total well-founded ordering ! on A. If
the ordering is order-isomorphic to !, then A can be enumerated as A 1
this will not be possible if the ordering corresponds to
a higher ordinal. The ordering on A may be based on the size (number of symbol
occurrences) in the atoms A i , or it may be one of the orderings used to show the
termination of term-rewriting systems [DJ90, Pla93]. Of course, there are also other
possibilities.
We now specify the ordering on interpretations. A literal is an atom or an atom
preceded by a negation sign :. A literal without a negation sign is called positive
and one with it is called negative. The literals L and :L are called complementary.
We write L for the complement of L; thus if L is positive then L is :L, and :L is
L. If A 2 A, then we call A and :A literals over A. An interpretation I is (for our
purposes) a subset of A, or, equivalently, a mapping from A to ftrue; falseg. If I
maps A to true then we write I j= A and say that I satisfies A. Otherwise, I 6j= A
and we say that A contradicts I. We say that I j= :L iff I 6j= L. We say that two
interpretations I and J agree on a literal L if (I
assign L true or they both assign L false. If I and J are two distinct interpretations
of A, let d(I; J) be the least atom A (with respect to !) such that I and J do not
agree on A. Such a least atom must exist, because the ordering on atoms is well-
founded. Let I 0 be a special interpretation called the initial interpretation; this
is typically supplied by the user. We order the interpretations with respect to I 0
as follows: Let I and J be two different interpretations of A. Let A be d(I; J).
Then if I agrees with I 0 on A, we say I I. Thus the smallest
literals have the greatest influence on the ordering, and the interpretations that
agree with I 0 are smaller in the ordering than interpretations that disagree with I 0 ,
other things being equal. Note that I 0 is the minimal interpretation in this ordering.
This ordering on interpretations is not well-founded, if A is infinite. Still, it turns
out that certain sets of interpretations of interest to us have minimal elements.
We will mainly be interested in interpretations that differ from I 0 in finitely many
places; we now develop some of their properties.
Definition 2.1 Given an interpretation I of A and literals L i over A, let I[L
be defined as follows:
(a) Lng or if
(b) L 62 fL Lng and I
Thus like I except for the finite list [L
ceptions;" for an earlier use of this notation see [CP94a].
Theorem 2.2 Suppose I is an interpretation over A and
Proof. If I ! J , then I and J are unequal; thus there must be a literal d(I; J). If
agrees with I 0 on the atom d(I; J), so J ! I. 2
This result classifies the interpretations that are smaller than I
into n distinct groups, depending on which L i is equal to d(I; J).
We now specify the ordering on literals and clauses. We order literals so that
if A and B are atoms and A ! B then A ! B (as literals), A ! :B, :A ! B,
and :A ! :B. However, the literals A and :A are not ordered with respect to
one another. A clause is a finite set of literals, regarded as their disjunction. A
clause is a tautology if it contains a literal and its negation. A clause is a ground
clause if it contains no variables (and similarly for literals, atoms, and terms). We
define a partial ordering ! on non-tautologous clauses by the multiset extension
of the ordering on literals: If C is the empty clause (which contains no literals),
then C ! D for every non-empty clause D. Also, if C and D are clauses, then let
and M be their maximum respective literals, which exist if C and D are non-
tautologous. Then if
fMg. If L and M are
complementary, then C and D are not ordered. We note that this multiset ordering
on clauses is well-founded, because the underlying ordering on literals is. We will
not further use this ordering on clauses, and present it here mainly to make this
point clear. An interpretation I satisfies a ground clause C if I satisfies some (that
is, at least one) literal in C, and in this case we write I Otherwise, we say
that C contradicts I and write I 6j= C. A substitution is a mapping from variables
to terms. If \Theta is a substitution and C is a clause then C \Theta represents the clause
obtained by replacing variables of C by terms, as specified by \Theta. Such a clause
C \Theta is called an instance of C. A similar terminology applies to terms, atoms, and
literals, so we can apply substitutions to them, for example. For our purposes, we
say that an interpretation satisfies a non-ground clause if it satisfies all of its ground
instances. An interpretation I satisfies a set S of clauses if it satisfies every clause
C in S. An interpretation I is a model of a ground clause C if I I is a
model of a set S of ground clauses if I j= C for all C 2 S. Least models of single
ground clauses and finite sets of ground clauses exist, as we now show.
Theorem 2.3 Suppose Lng is a ground clause, and suppose
the least model of C is I 0 [Ln ].
Proof. We note that I 0 [L n ] is a model of C. We show that no smaller interpretation
J is a model of C. Suppose J ! I 0 [L n ]. Then by theorem 2.2, d(I;
This implies that J agrees with I 0 on Ln , and so C contradicts J . 2
Theorem 2.4 Let G be a finite set of ground clauses. If G is satisfiable, then G
has a least model.
Proof. Let A An be the atoms that appear (positively or negatively)
in clauses in G. There are only finitely many interpretations of these atoms; at
least one of them, say I, is a model, since G is satisfiable. Let I min be the least
such model, in our ordering on interpretations. This must exist, since there are
only finitely many such finite interpretations. Extend I min to an interpretation
I of A where each L i is either A i or :A i and I min
i. Then J is a model of G, since I min is, and it is a least model, because I min is
as small as possible among the interpretations of the A i and the interpretations of
the other literals have been chosen as small as possible. Any smaller interpretation
would have to differ from J on some literal L i , by theorem 2.2, which is not possible
by the way I min was chosen. 2
Definition 2.5 If G is a set of ground clauses over A, let lm(G) be its least model,
that is, the smallest interpretation I in the ordering on interpretations such that I
satisfies all elements of G.
In ordered clause linking with semantics, we accumulate such a set G and in
the process keep track of its least model lm(G). However, we do not store G in
its original form, but apply certain simplifications to it so that only the features
relevant for the current minimal model are retained. For this, we not only make use
of least models but also least contradicting clauses of interpretations. However, this
necessitates the introduction of another ordering on clauses. We would have liked to
define ordered semantic hyper-linking entirely in terms of the ordering ! on clauses,
but it turns out that this is incomplete if the ordering ! has order type greater than
!. Therefore we introduce another ordering ! cl on clauses and use it to choose
contradicting instances. This means that for purposes of completeness, ordered
semantic hyper-linking may need to use two different orderings, which seems to be
somewhat remarkable. For this purpose, we assume that ! cl is a partial ordering
such that for all ground clauses C, all but finitely many ground clauses D satisfy
cl D. An example of such an ordering is to order the clauses by size, that
is, the total number of occurrences of symbols, with two clauses unordered if they
have the same number of symbols. Since there are only finitely many non-variable
symbols that can appear in a clause, this ordering satisfies the finiteness property
given above. We note that ! cl is well-founded. We say a clause C is minimal in a
set of clauses if there is no clause D in the set such that D ! cl C; note that a set
of clauses can have more than one, but at most finitely many, minimal elements.
We now show that minimal contradicting clauses always exist; that is, if I is not
a model of a set S of (non-ground) clauses, then there is a minimal ground instance
D of a clause C of S such that D contradicts I.
Theorem 2.6 Suppose S is a set of (possibly) non-ground clauses over A and I is
an interpretation of A that is not a model of S. Then there is a ground instance D
of some clause C of S such that for all other ground instances D 0 of clauses in S,
if I 6j= D 0 then either D ! cl D 0 or D and D 0 are unrelated by ! cl .
Proof. The set of such ground clauses D 0 such that I 6j= D 0 is non-empty, since
I 6j= S. Since the ordering ! cl on ground clauses is well-founded, this set of ground
clauses has a minimal element D, as claimed. Note that there may be more than
one such minimal D, but at most only finitely many, by the way ! cl is defined.
This result holds even if S is infinite, by the way. 2
Definition 2.7 Let mi(S; I) be some such clause D, that is, a ground instance D
of some clause in S such that I 6j= D and such that there is no other ground instance
C of a clause in S such that C ! cl D and such that I 6j= C. If there is more than
one such instance D, we assume that mi(S; I) is one of them, chosen in an arbitrary
manner. We call such a clause D a minimal contradicting instance for I.
Later we will discuss algorithmic aspects of computing mi(S; I).
3 The Search Procedure
The task now is to devise a procedure that will search through the set of all interpretations
in a manner consistent with the ordering, finding clauses that contradict
each interpretation. For this, we maintain a finite list C of "relevant" ground clauses
that record the progress made in the search so far; this list contains instances of
clauses from S as well as instances of clauses derived from S by A-ordering resolu-
tion. The goal of the search is to continually increase the least model lm(C) of C,
that is, to make lm(C) larger and larger in the ordering on interpretations. This
least model is called the current interpretation. At the beginning C is empty and
lm(C) is I 0 . As elements are added to C, and sometimes removed, this least model
becomes larger and larger in our ordering on interpretations. If S is unsatisfiable,
then eventually C will contradict all interpretations, and there will not be a least
model any more. At this point, the empty clause will be in C and the search will
stop. The invariant that C possesses is captured by the following definitions.
Definition 3.1 If C is a (non-tautologous) clause, let max(C) be the maximal
literal in C in the ordering !. This exists because clauses are finite.
Definition 3.2 A list C of clauses is ascending if it is of the form C
contradicts the least model of C 1 , and so on. The
literals are called eligible literals, in harmony with the use of this term in
[CP94a].
Theorem 3.3 If then so are all its prefixes. Also,
Furthermore, this latter interpretation disagrees
with I 0 on the literals max(C j ).
Proof. The part about prefixes is immediate. Let I be I 0 [max(C 1
max(Cn )]. For the rest, we use induction on i. We show that I 0 6j=
i. By induction, lm(fC
Since the literal max(C i ) is larger than any literal [max(C 1
agrees with I 0 on max(C i ). By the definition
of ascending, lm(fC
To show that I and I 0 disagree on the literals max(C i ), we have just shown that
I 0 6j= max(C i ) for all i, but I by the way it is constructed.
To show that I is lm(C), we show that I is a model of C but no smaller interpretation
J is a model of C. Now, I is a model of C since it satisfies all the literals
I then J must differ from I on some eligible literal, by theorem 2.2.
Suppose d(I; J) is max(C I and
J differ on max(C i )). It remains to show that J does not satisfy the other literals
of C i . We know that lm(fC by the definition of ascending.
By induction, we know that lm(fC
does not satisfy C i .
Therefore J does not satisfy C i , since J agrees with I 0 [max(C 1
on all literals smaller than max(C i ), and thus the literals of C i other
than are not satisfied by J either. 2
Now, when the search procedure begins, the set C is empty, and this set is then
successively modified by adding to it some clause instance D contradicting its least
model. Whenever this is done, it is necessary to do some processing on C to preserve
the ascending property. This processing involves performing certain resolutions, as
well as deleting certain elements from C, in a manner that will be described. Letting
simp(C; D) denote the result of this processing, we have the following overall
algorithm for ordered semantic hyper-linking:
while fg 62 C do
if lm(C)
else D / mi(S; lm(C));
return "unsatisfiable"
3.1 Processing the list C of relevant clauses
There are two kinds of operations that take place during the processing of C involved
in the call to "simp." The first kind is to perform ordered resolutions between D
and the last clause of C, when possible. For this, we define a res(C,D) as follows:
Definition 3.4 Suppose C and D are ground clauses and suppose there is a literal L
such that
Now, a res(C,D) will be an (A-ordering) resolution involving the maximal literals
in C and D. We note that A-ordering resolution is in itself a complete theorem
proving method for propositional logic, and has natural extensions to first-order
logic. The second kind of operation that takes place during "simp" is to eliminate
elements from C that are made irrelevant by these A-ordering resolutions, that is,
elements of C that can be eliminated without affecting lm(C). We have the following
procedure for "simp":
procedure
if max(Cn ) and max(D) are complementary then
return
else if max(Cn ) ? max(D) then
return
else return [C
To show correctness, we define an interpretation I   (C; D) where C is an ascending
list of clauses and D is a clause contradicting lm(C). This interpretation will
have the property that lm(C) ! I   (C; D) and if I  I   (C; D) then I 6j= C [ D.
Therefore, if C [ D is satisfiable, then I   (C; D) ! lm(C [ D). Thus this definition
gives a lower bound on the least model (if it exists). We show that each processing
step in "simp" does not decrease I   (C; D). And, at the beginning, I   (C; D) is larger
than lm(C). Therefore, at the end we will obtain an ascending set of clauses whose
least model is larger than it was at the beginning. Or, if I   (C; D) is the maximal in-
terpretation, that is, the interpretation that disagrees with I 0 everywhere, then this
property is preserved, and we will eventually derive the empty clause fg. I   (C; D)
can be the maximal interpretation only if C [D is unsatisfiable.
Definition 3.5 Suppose C is an ascending list of clauses. Let Cn be the last clause
in the list C. Suppose D is a ground clause contradicting lm(C). Define I   (C; D) as
follows:
I   (C; D)
1. L ? max(D) and I 0 6j= L or
2. L  max(D) and lm(C)
We note that since I   (C; D) imitates lm(C) for literals less than or equal to
max(D), and max(D) contradicts lm(C), therefore I   (C; D) does not satisfy D.
Also, for literals larger than max(D), I   (C; D) is chosen to be as large as possible
in the ordering on interpretations, and for smaller literals, I   (C; D) agrees with
lm(C). Therefore, I   (C; D) ? lm(C). (We cannot have equality because I   (C; D)
differs from I 0 on infinitely many literals, assuming that A is infinite.) Also, if
J is an interpretation and J ! I   (C; D) then J does not satisfy C [ D. If J !
lm(C) this is immediate. If contradicts J . If J ? lm(C)
then d(J; lm(C)) ? max(D) since I   (C; D) and lm(C) agree on literals not larger
than max(D). Therefore D contradicts J in this case, too. Thus lm(C [ D) ?
I   (C; D). And if there is no least model for C [D, then we can at least say that all
interpretations J less than or equal to I   (C; D) fail to satisfy C [ D.
To show correctness, we need to show that every processing step in "simp" preserves
the property that D contradicts lm(C) and does not decrease I   (C; D). Let
Cn be the last element of C. If max(D) ? max(Cn ) then the list C with D added
to the end, is ascending, because D contradicts lm(C). If max(D)
then Cn does not enter into the definition of I   (C; D) at all, so deleting Cn does
not affect I   (C; D). Also, D still contradicts lm(fC since this cannot
be distinguished from lm(C) on literals smaller than max(Cn ). If max(D)
and max(Cn ) are complementary (the only remaining case), then the A-ordering
resolution replaces D by a resolvent containing all the literals of D and Cn except
the two maximal ones. This resolvent will still contradict the least model of
since Cn did (by the definition of ascending) and all the literals of
D except max(D) also contradicted this least model (since D contradicted lm(C)).
Also, I   (fC because the maximal literal
of a res(Cn ; D) is smaller than the maximal literal of D.
We now consider the case in which C [ D is unsatisfiable. If at some stage in
this processing, I   (C; D) becomes the maximal interpretation, then no ascending
list can be produced, since all ascending lists of clauses are satisfiable. This means
that the empty clause fg is derived, and the search terminates.
3.2 An example
Suppose that the set A of atoms is
and that S contains the following clauses:
Suppose I 0 interprets P i as true for all i, that is, I 0
the following sequence of current interpretations I and the corresponding
clauses mi(S; I i
I 0 (now the clause :P 5 ; :P 8 is chosen)
I
(resolvent , from the above two clauses)
I
I
I
is generated from
(resolvent P 6 is generated from P 10 and P 6
(resolvent :P 5 is generated from P 6 and :P 5
I
(resolvent fg is generated from :P 5 and P 5
We show the five ascending lists of clauses that are generated, too:
(after "simp" is called)
3.3 Completeness
We can argue the completeness of this method in a manner similar to the completeness
proof in [PACL92]. However, the fact that our partial orderings may have order
type greater than ! complicates the argument a little. Also, the fact that clauses
D that are chosen at one point may be discarded later complicates the proof. The
general idea is to show that if S is unsatisfiable then there are only a finite number
of ground instances that can ever be chosen as minimal contradicting instances.
Therefore there are only a finite number of atoms that ever appear in C. This essentially
reduces the problem to one involving finite interpretations. Since there are
only finitely many finite interpretations, and by the ordering on interpretations, the
same one cannot be seen more than once, eventually the method must stop; if S is
unsatisfiable, then the only way for the method to stop is to generate the empty
clause.
We now show that only a finite number of clauses can be chosen.
Theorem 3.6 Suppose S is an unsatisfiable set of clauses. Then the set fmi(S; I)
I is an interpretation over Ag is finite.
Proof. Let T be a finite unsatisfiable set of ground instances of clauses in S; such
a set T exists, by the so-called Herbrand's theorem. Then for every interpretation I
there is a clause D in T such that I 6j= T . Note that mi(S; I) is minimal in ! cl among
clauses contradicting I; thus we cannot have D ! mi(S; I). However, for all but
finitely many ground clauses C,
which is finite. 2
Definition 3.7 Suppose we arbitrarily choose some finite unsatisfiable set T of
ground instances of S. Then we say a ground instance C of S is small if it is in the
set [D2T fD it is large otherwise. We say that an atom is small
if it appears (positively or negatively) in a small clause, and it is large otherwise.
Also, a literal :A is small if A is small, otherwise :A is large.
Theorem 3.8 Ordered semantic hyper-linking is complete, that is, if a set S of
clauses is unsatisfiable, then eventually the empty clause fg will be derived.
Proof. As above, we observe that the clauses mi(S; I) will be small for all current
interpretation I, and there are only finitely many such clauses. It follows that all the
current interpretations I constructed will be of the form I 0 [L], where L is a subset of
the small literals. This is a finite set of interpretations. Each current interpretation
constructed is larger than its predecessor in the ordering ? on interpretations; thus
the same interpretation cannot be seen twice, and the search must eventually stop.
The only way that this can happen is for the empty clause to be generated, if S is
unsatisfiable. 2
3.4 Efficiency
We briefly note one advantage of this approach over semantic hyper-linking as described
in [CP94a]; that is that the growth in the number of eligible literals is
better controlled. The number of eligible literals has a strong effect on the work
required to find a minimal clause D contradicting the current interpretation. The
procedure "simp" will automatically perform A-ordering resolutions when the maximal
literal of the contradicting clause D is the complement of an existing eligible
literal; each such resolution has the effect of removing an eligible literal from C.
In semantic hyper-linking without an ordering, it is rare for eligible literals to be
removed. Typically the set of eligible literals grows rapidly, making the search procedure
time-consuming after a few rounds of search and making it difficult to find
proofs that require more than a few rounds of search. However, the other parts
of semantic hyper-linking are powerful enough so that many proofs can be found
within three or four rounds of search. In addition, the way that the literals are
ordered in semantic hyper-linking makes the propositional satisfiability test fast.
Still, we think it would be an advantage to be able to handle many rounds of search
efficiently.
Finding Minimal Contradicting Instances
The preceding discussion has not dealt with the practical aspects of how the minimal
contradicting instance mi(S; I) is found. We have dealt with this to some extent in
[CP94a]. The problem is, given an interpretation of the form I 0 [L
a set S of (possibly) non-ground clauses, to test whether I 0 [L
if not, to find a minimal ground instance D of some clause C in S such that D contradicts
I (if such a clause D exists). For this purpose, as we shall
see, it is helpful to choose an I 0 that is decidable, that is, given a non-ground clause
C we can decide whether I 0 There are a number of kinds of interpretations
I 0 that are decidable; among them are the syntactic interpretations, interpretations
with a finite domain, and interpretations over the reals in which all the functions
and predicates can be expressed in terms of linear arithmetic with inequality. We
say that an interpretation I is syntactic if for any two atoms A and B with the
same predicate symbol, I depending only on
the signs and predicate symbols of literals, are fairly limited in expressiveness, but
are sometimes useful anyway, as we show in [CP94a]. As an example of an interpretation
in terms of linear arithmetic with inequality, we might interpret P (x; y)
as 2x ? y and we might interpret f(x; y) as if (x ? y) then x else y. Now, if
I 0 6j= C, then we will need to find a ground instance C \Theta of C such that I 0 6j= C \Theta.
A problem is that even if I 0 6j= C, such a ground instance may not exist; the reason
is that some of the elements of the domain of I 0 may not be values of any finite
ground terms. The question, given clause C, does there exist a substitution \Theta such
that C \Theta is a ground clause and I 0 6j= C \Theta, seems to be harder than deciding if
I 0 We say an interpretation I 0 is Herbrand decidable if this question about
\Theta is decidable. We note that syntactic interpretations and interpretations with
a finite domain are Herbrand decidable. We don't know whether interpretations
over the reals in which the functions and predicates can be expressed using linear
arithmetic with inequality, are Herbrand decidable. If I 0 is not Herbrand decid-
able, then we may have a current interpretation I that satisfies S without being
able to detect this, and we may then spend an infinite amount of time fruitlessly
searching for a ground instance C \Theta that contradicts I. However, this will not affect
the completeness of ordered semantic hyper-linking, because if such a C \Theta exists,
it will eventually be found. If I 0 is Herbrand decidable, then the question whether
I decidable, as we will now show.
We first specify in more detail how such an instance C \Theta can be found, if it
exists, and moreover an instance that is minimal, assuming that I 0 is decidable. Our
approach is to construct a set Z of literals with the following property: The set of
ground instances of literals in Z is exactly the set of ground literals L such that I 0 6j=
L and neither L nor its complement L appear in the list [L of eligible
literals. Then one can show that D contradicts I 0 [L literal
of D is either the complement of an eligible literal or an instance of a literal in Z.
For, literals of D that are complements of some L i will be false in I 0 [L
since I 0 [L Also, literals that are instances of elements
of Z will be false in I 0 [L agrees with I 0 on
such literals. To obtain such instances D, then, we can take a clause C of S and
apply a substitution \Theta such that all literals L of C \Theta are instances of some literal
in Z or complements of some eligible literal. That is, we find most general \Theta such
that for all L in C \Theta, there exists a literal M in Z or in the set of complements of
eligible literals such that L and M are identical.
We note that finding such a \Theta is reminiscent of the hyper-linking method of
[LP92]. If all the literals in Z are ground literals, this can be done by a matching
procedure, in which the literals of C are matched one by one; if some literals in Z
are non-ground, we may need to do successive unifications. Then if the resulting
instance D is non-ground, it is necessary to instantiate the variables with ground
terms in some manner. Since we are only interested in minimal ground instances, we
can replace each variable by a ground term that is minimal in the clause ordering,
assuming that the clause ordering ! cl is monotone. For our purposes, we say that
the clause ordering is monotone if it can be extended to a partial ordering on terms
such that for terms s and t, s ! cl t implies C[s] ! cl C[t] for a clause C[s] containing
an occurrence of s. Furthermore, we require that for any term t there are at most
finitely many terms s for which :(t ! cl s). Note that this implies that there are at
most finitely many minimal terms, by the condition just given. However, in order to
find a minimal instance, it may be necessary to replace each variable by all minimal
terms in all possible ways, which can be expensive. For this purpose, we write s#t
to mean :(s ? t)":(t ? s) and we write C#D similarly for clauses C and D. Then
we can choose an ordering ! cl so that s#t implies C[s]#C[t]; for example, ordering
terms by their size satisfies this condition. For such an ordering, we can obtain
a minimal instance of C by replacing all variables by arbitrarily chosen minimal
terms.
Now, the problem is to generate Z. Without knowing more about I 0 , not much
can be said. If the test I 0 j= L is decidable for ground L, then one can enumerate all
ground literals L, discard eligible literals and their complements, and test whether
I 0 L, and in this way generate, or at least enumerate, Z. The fact that Z is
infinite need not be a problem, because one only needs to enumerate Z in ascending
order in the ordering ! cl in order to find a minimal contradicting instance D. As
soon as one instance D has been found, literals L such that D ! cl L need not
be examined, and this eliminates all but finitely many literals. For this we need
to assume that if L 2 C and L 6= C then L ! cl C. In [CP94b] we indicate how
specialized decision procedures can be used to aid in the generation of such sets Z
of literals, in some cases. Of course, such an enumerative method cannot detect if
the current interpretation satisfies C (or S). Here we choose a different approach
that permits a relatively small set Z 0 to be generated independent of I 0 , and delays
the consideration of I 0 to a later stage. This approach also permits us to detect if
the current interpretation satisfies S, if I 0 is Herbrand decidable.
We make some comments about the complexity measures used. These are defined
in terms of the sizes of various structures, considered as character strings.
Alternatively, the size of a clause, set of clauses, etc. is the number of occurrences
of symbols in it. So if we say that something can be computed in time polynomial
in a set S of clauses, we mean that the running time is bounded by a polynomial
in the length of S, written out as a character string; this is the usual complexity
measure.
4.1 Disunification
Let Z 0 be a set of literals such that a ground literal L is an instance of a literal in
exactly when neither L nor its complement appear in the set of eligible literals.
Thus we have a kind of disunification problem. It turns out that a finite set Z 0
always exists, since the eligible literals are ground and there are finitely many of
them. We are using the fact that there are only finitely many function and predicate
symbols in all. Also, Z 0 can be computed in time polynomial in the list of eligible
literals. To see this, let Z 0 (E) be a desired Z 0 of literals as desired where E is
the set of literals that are either eligible literals or their complements. We first
note that the positive and negative literals can be handled separately. Let L pos be
the set of all positive literals and let L neg be the set of all negative literals. Let
pos be the positive literals in E and let E neg be the negative literals in E. Then
We can then extend this further, to
consider the sets of positive and negative literals having specified predicate symbols,
and solving for Z 0 for each such subclass. If there are n predicate symbols in all, we
obtain a total of 2n subproblems to solve, whose solutions can then be combined to
obtain the desired Z 0 . If a predicate symbol with a specified sign does not appear
in E, the subproblem has a simple solution; then P
is in Z 0 (E) for suitable P , since none of its instances will be in E. Otherwise, we
can consider the sets of literals having various function or constant symbols in some
chosen position. Each such further division reduces the problem into a number of
subproblems equal to the number of function and constant symbols, and partitions
the set E further into disjoint subsets; eventually we obtain trivial problems, whose
solutions can be combined.
4.2 Generating eligible instances
Suppose the set Z 0 of literals has been generated, as specified above. We generate
instances of the input clauses as follows: For each clause C in S, we partition C into
two disjoint sets of literals C ff and C fi . This is done in all possible ways. For each
such partition, we unify the literals in C ff with complements of eligible literals, and
those in C fi with literals from Z 0 . This also must be done in all possible ways. In
this way, we obtain an instance D of C which can be expressed as D ff [D fi where the
literals in D ff are complements of eligible literals and the literals in D fi are instances
of literals of Z 0 , that is, they do not unify with eligible literals or their complements.
We call such an instance an eligible instance. Note that there are only finitely
many eligible instances, and they can be found by a simple enumeration procedure.
In order to ensure that D contradicts I 0 [L it is necessary to find a
substitution fl such that I 0 6j= (D fi )fl. We also want this instance to be minimal in
the clause ordering ! cl . If fl is as specified, then I 0 [L reasoning
as above: The literals in D ff are not satisfied by the current interpretation because
they are complements of eligible literals. The literals in (D fi )fl are not satisfied by
the current interpretation I 0 [L because they are not satisfied by I 0 . If
I 0 is Herbrand decidable, then the test whether such a fl exists is decidable, and
in this way we can test if the current interpretation satisfies S. If such a fl exists,
we want to find one such that Dfl is minimal in the ordering ! cl . The search for
depends of course on I 0 and ! cl . If the clause ordering is based on the directed
acyclic graph size of the terms, that is, the size is the number of distinct subterms
that appear, independent of how often they appear, then this computation seems
to be rather complicated, despite the favorable theoretical properties of this size
measure. However, for certain types of interpretations and literal orderings, the
search for such a fl producing a small clause can be done quickly, as we will show
in the next section. This is also dealt with to some extent in the paper [CP94b]. It
is likely also that this search can always be done fast if I 0 is an interpretation with
a (small) finite domain and if the clause ordering ! cl is monotone. However, we
can say something more in case I 0 is syntactic. If I 0 is syntactic, then I 0 6j= (D fi )fl
iff I 0 6j= D fi , which latter condition can be checked just by examining the signs and
predicate symbols of the literals. This permits some clauses to be rapidly eliminated
from further processing.
4.3 Complexity analysis
Now, the generation of all the eligible instances requires an amount of work that
depends on the number and sizes of the eligible literals and the number of literals
in the clauses. However, suppose that there is a constant bound on the number of
literals in each clause of S. Then the generation of the eligible instances can be
done in time polynomial in S and the set of eligible literals, since one has to look
at all combinations obtained by unifying each literal in C with each complement of
an eligible literals or with each element of Z 0 . If the set of eligible literals unioned
with Z 0 has n elements and a clause C has k literals, then there are n k possibilities,
which is still polynomial, for a fixed k. For each possibility, the work is polynomial
(or even linear), using efficient unification algorithms. Note that n is polynomial
because the number of elements of Z 0 is polynomial, by the disunification arguments
given above. The assumption that k is bounded is reasonable, because it is possible
to convert clauses with many literals into clauses with three literals in a satisfiability-
preserving manner by introducing new predicate symbols.
We have just shown that if the number of literals in clauses of S is bounded,
then the eligible instances can be generated in polynomial time, regardless of I 0 .
However, in order to generate a minimal contradicting instance, it is necessary to
instantiate the eligible instances with a substitution fl as specified above. If I 0 is
syntactic and the clause ordering ! cl is monotone and has properties concerning #
as specified in section 4, then such a fl can be obtained simply by checking the signs
and predicate symbols and replacing all variables by a minimal term; the entire
process of generating a minimal contradicting instance can then be done in time
polynomial in S and the set of eligible literals, that is, polynomial in their lengths
written out as character strings. If I 0 has a finite domain, then we can find for each
domain element d a minimal ground term t d whose interpretation under I 0 is d;
such terms can be found by a simple iteration procedure. Then to find a minimal
instance Dfl such that I 0 6j= (D fi )fl, it is only necessary to consider fl replacing
variables by terms of the form t d for various d in the domain. This is a finite
number of possibilities, exponential in the number of variables in D fi . If we use an
ordering based on directed acyclic graphs, that is, counting the number of distinct
subterms that appear, but not counting how often they appear, then the generation
of a minimal contradicting instance can be more complicated. This is because such
directed acyclic graph-based orderings are not monotone, since the contribution of
a subterm to the clause depends on subterms that appear elsewhere in the clause.
However, because of the favorable complexity properties of the directed acyclic
graph-based orderings, it may be worthwhile to use them anyway. In general, it
makes sense to look at the eligible instances in order of their size; then it is possible
that a small contradicting instance may be found early and the instantiation of the
larger eligible instances may be avoided.
5 Additional Ordered Resolutions
Since the basic search procedure performs A-ordering resolution, it seems reasonable
to do additional resolutions; these might have the effect of doing part of the work
in advance, and thereby speed up the search. So we might add to the input clause
set S a set S 0 of clauses generated by A-ordering resolution from S, and consider
these clauses S 0 as additional input clauses. One would expect that these additional
clauses might lead to the finding of certain contradictions earlier. They also have
the effect of eliminating large literals from proofs, as mentioned in [CP94a]. Since
the basic search procedure has difficulty generating large literals, this combination
is reasonable, and we have found that in practice such a combination (with rough
resolution instead of A-ordering resolution to eliminate large literals) often works
well. We can balance the work between ordered semantic hyper-linking and A-
ordering resolution in some way so that both are done in parallel; one method to
use is to divide the total time spent equally between them.
The question remains what ordering to use for these A-ordering resolutions, and
which A-ordering resolutions to do. As for the ordering, we can choose some literal
ordering ! lit compatible with the ordering ! on ground literals. That is, we can
say lit M (for non-ground literals L and M ) if there is a ground substitution
\Theta such that L\Theta ! M \Theta. Note that we really need to use the ordering ! here,
not ! cl , to compare L\Theta and M \Theta. Then we can restrict literals resolved on, to
literals that are maximal in this ordering ! lit . As for which A-ordering resolutions
to perform, we observed in section 4.3 the strong dependence of the efficiency of
ordered semantic hyper-linking on the number of literals in clauses of S, so it is
reasonable to keep the number of literals small. However, we want to perform the
A-ordering search in a complete manner, so that a reasonably large portion of the
search space is explored. For this, it is necessary to consider not just the number
of literals in a clause, but the size (number of symbol occurrences) of the clause.
Preferring clauses of small size can be done in a completenesss-preserving manner,
and will also tend to keep the number of literals small. The question remains exactly
how this can be done. One way to do this is to keep a list of all pairs of clauses
that have not yet been resolved together, and always resolve the pair of clauses
whose sum of sizes is as small as possible. This produces clauses of small size
(number of symbol occurrences), which will tend to have few literals, but requires
a quadratic amount of space to store these possible resolutions. Another method
is the Otter [McC89] approach, in which a small clause is repeatedly chosen and
resolved against all other clauses. This avoids the expensive bookkeeping, but has
the problem that this small clause will also resolve against large clauses, possibly
producing large clauses very early. We propose a compromise, in which a list
of clauses is constructed as follows: The clauses are entered into this
list L smallest first. Whenever a clause is entered into the list, it resolves (using
A-ordering resolution) against all clauses that are already on the list. In this way,
all resolutions will eventually be done, but we have some guarantee that when two
clauses are resolved, both of them are small. Also, the amount of bookkeeping is
kept to a minimum. When we are done resolving C i against C j for all j  i, then
we find the smallest clause D such that D is in S or has been produced by an earlier
resolution, and such that D is not already in the list L. We then enter this smallest
clause D in the list as C i+1 and resolve D against C j for 1  j  i, continuing the
process.
Now, for certain problems, it is important to produce large A-ordering resolvents
early, if they are directly derived from the negation of the theorem; this is the
case for example in Bledsoe's five limit problems [Ble90]. Therefore, we would like
to make this A-ordering resolution procedure sensitive to support criteria as well
as size. For this purpose, we can say that a clause C is semantically supported
if it contradicts the user-given interpretation I 0 . Both input clauses and clauses
generated by resolution can be semantically supported. We then can modify the
above A-ordering search strategy so that on alternate choices of D, a semantically
supported clause is chosen. Thus, we alternate between choosing D as the smallest
clause not yet in L, and the smallest semantically supported clause not yet in L;
this D is then added to the list and resolved against all clauses already in L. This
will tend to favor resolutions involving supported clauses, even if the clauses are
large. This can only be done systematically if I 0 is decidable (or better, Herbrand
decidable).
5.1 Explanation-based generalization
The idea of explanation-based generalization (EBG) is to extract some general principles
from a specific argument, enabling the argument to be applied to a wider range
of situations. This principle can be applied in the ordered resolution phase of ordered
semantic hyper-linking. We note that in the procedure "simp" of section 3.1,
A-ordering resolutions are performed between ground clauses Cn and D; both Cn
and D are instances of clauses in S or clauses produced from S by prior A-ordering
resolutions. Suppose C 0
n and D 0 are the more general (possibly non-ground) clauses
of which Cn and D, respectively, are instances. Then by general properties of res-
olution, it follows that there is an A-ordering resolvent C of Cn and D such that
a res(C n ; D) is an instance of C. Therefore, it seems reasonable to store these more
general clauses C 0
along with their respective instances Cn , D, and
a res(C n ; D). In this way, we produce lemmas that can be added to the set of input
clauses; such lemmas are likely to be relevant to the proof and may be generated
again during the search. By adding them to S, we may avoid sections of the search
in which the same resolutions are performed over and over again. It makes sense
to generate these lemmas, in addition to performing the A-ordering resolutions of
the previous section, since these lemmas may not be generated by the list-based
search method described there. The fact that a uniform ordering is used in the
search may make the lemma mechanism more effective; the search method used in
semantic hyper-linking without ordering can vary the literal ordering for different
interpretations, making it less likely that a lemma found earlier will be useful later
on.
6 Replacement Rules
We found that the use of replacement rules considerably enhanced the performance
of semantic hyper-linking, and so it is reasonable to include them in ordered semantic
hyper-linking, too. However, it is necessary to adapt them to the current
context. Suppose is the current set C of relevant clauses. From this
we construct a set EL of explicit literals; these are the literals satisfied by lm(C)
that actually appear in the clauses C i . Thus, the set EL is defined as the set of
literals L such that either L or L is a member of [ i C i and such that lm(C)
Now, if we can show that EL[S is unsatisfiable, then we have already contradicted
the current interpretation lm(C) and need not search for a minimal contradicting
instance. For, if we can show that EL [ S is unsatisfiable, then we know that the
clause is a logical consequence of S, and can be used as if it were
the minimal contradicting instance. In fact, we typically find a small subset of the
clause that is a consequence of S, which may be more useful for the
search.
To see whether EL[S is unsatisfiable, we use some incomplete but often effective
methods. In the general AI context, these may be viewed as "obvious inferences"
or "associations" that are readily made. One method is to use natural replacement
rules; these are implications of the form L such that all
variables in the literal L appear elsewhere in the implication, and such that some
clause is a member of S. These replacement rules may be used
by unifying the L i with elements of EL; then the corresponding instance of L will
be a ground literal (since all variables of L also appear elsewhere). This ground
literal is a logical consequence of EL and can be (temporarily) added to EL. This
operation may be repeated a number of times, and if complementary literals appear
in EL, then we know that EL [ S is unsatisfiable. From this derivation, a clause
that is a subset of fL : L 2 ELg may be extracted, which is a logical consequence
of S, and can be used in place of the minimal contradicting instance.
Another kind of replacement rules are definitional replace rules; this is a slight
simplification of the minimal replace rules of [CP93a]. The idea of definitional replace
rules is to capture clauses that represent definitions, and the effect of applying
them is to expand the definitions; this is particularly useful in set theory and modal
logic. A definitional replace rule is of the form L such that all
variables in L 1 appear in L and such that some clause fL; L
is a member of S. If one has a definition of the form L j A where L is a literal and
A is a formula involving no quantifiers, one can verify that such clauses will be generated
when this definition is converted to clause form. Sometimes such formulae
are generated even if A contains quantifiers.
Such replacement rules are used in an inverse way to natural replacement rules;
if L has an instance L\Theta in EL then the implication L\Theta
can be added to EL. Also, if L has an instance L\Theta appearing on the right-hand side
of some such implication that has previously been added to EL, then this instance
can be added to EL. In this way, EL is augmented by a set of ground instances
of clauses in S. These instances are the instances that one would consider when
expanding definitions. We can then test if EL together with these ground instances
is unsatisfiable, using something like Davis and Putnam's method [DP60]. If so,
then a clause that is a subset of fL : L 2 ELg may be extracted, which is a logical
consequence of S; thus we know that EL [ S is unsatisfiable.
In order to control the application of replace rules, it seems most reasonable
to use a time bound based on the time used to search for a minimal contradicting
instance. It seems reasonable to first search for a contradiction using natural replace
rules. If this fails, then definitional replace rules can be applied, again controlled by
the time bound. If this fails, then we have generated a set G of ground clauses that
can be processed a little more; this set G contains EL (considered as unit clauses)
together with the ground instances generated by definitional replacement. Since G
consists of ground clauses, it has only finitely many models. We can examine all
these models J of G one by one, and for each such model J , we can again apply
natural replacement to J to see if it can be contradicted. We note that since G is
finite, the models J are essentially finite, too. Each such model J can be considered
as a set U of unit clauses, that is, the set of literals L such that J j= L and such
that L or its complement appears in G. Then we can perform natural replacement
on this set U ; this may find some contradictions that were nearly, but not quite,
found by definitional replacement. Such contradictions (demonstrations that S [U
is unsatisfiable) need to be found for all models J of G in order to demonstrate that
7 Complexity Analysis
We now consider the complexity required by ordered clause linking, in terms of
the complexity of the shortest proof from S. We consider both worst-case bounds
and give plausibility arguments for better performance. In the introduction, we
noticed that clause linking has a triple exponential bound in the complexity of the
proof, which can be reduced to double exponential if a suitable ordering on literals
is used. The same arguments apply to ordered clause linking. However, we have
reason to believe that the performance will be better than this. We know that if S is
unsatisfiable then there is an unsatisfiable set T of ground instances of S. Suppose
we measure the complexity of the proof by the complexity c(T ) of T , that is, its
length when written out as a character string. Or, equivalently for our purposes, we
can measure the complexity of the proof by the complexity of the largest clause in
T . Note that this complexity measure does not economize on repeated occurrences
of the same subterm. Now, suppose our clause ordering is based simply on the
length of the clause, written out as a character string, that is, the sum of the sizes
of the literals in the clause. Then each clause appearing in the proof has complexity
at most c(T ), so we will find the proof when all clauses of complexity c(T ) or less
have been generated (or earlier). The number of such clauses is exponential in c(T ),
and so the time required to test their satisfiability may be double exponential in
c(T ). We note that this measure is independent of how many clauses appear in
T , if the "largest clause" complexity measure is used. This will give our method a
better comparison with the methods of [Gou94], which need to count the number
of elements in T .
Another favorable factor for our method is that the satisfiability of a set of
ground clauses can be tested in expected polynomial time, for many probability
distributions. And in practice, methods similar to Davis and Putnam's method
often decide satisfiability of sets of propositional clauses very fast. We have also
found this to be the case in the clause linking theorem prover. Since our search
procedure is similar to Davis and Putnam's procedure in its systematic search for a
model, we would expect a similar time bound to apply to it, too; thus we may expect
in practice that the time required by our method is single exponential. We note
further that this is based on the assumption that all clauses of complexity c(T ) or less
are generated. Our method is very selective about which clauses are generated, so
that it is reasonable to assume that only a small subset of the clauses of complexity
c(T ) or less will be generated. For example, we can show that our method will only
generate logically minimal ground instances, that is, ground instances that are not
logical consequences of smaller (with respect to ! cl ) ground instances. Equivalently,
a ground instance C is logically minimal if there is some interpretation I such that C
is a minimal instance of S contradicting I. The question whether a clause is a logical
consequence of simpler clauses is also relevant for the methods of [BG90, BG94], it
turns out. One would expect that the number of logically minimal ground instances
of a given size is much smaller than the total number. In fact, we do not even
generate all the logically minimal clauses. The fact that A-ordering resolutions
are done in the "simp" procedure means that many interpretations are not even
examined. That is, we only generate clauses that are logically minimal when such
A-ordering resolvents are also considered, so this can eliminate some clauses from
being logically minimal. But, as a worst case bound, our method is exponential in
the number of logically minimal ground instances; this is always finite when S is
unsatisfiable.
One might ask whether it would be just as efficient to generate all ground instances
of clauses in S, and then consider them in order of size. Thus we would have
a list C of all ground instances of S, with the smallest ones occurring
earlier in the list. We could then test if C i is a logical consequence of C i , for i  j,
and if so, delete C i from the list. This produces a smaller sublist C 0
of ground instances, of which finite prefixes can be tested for satisfiability. Our
method is more efficient in that the non-logically minimal instances are never even
generated. Furthermore, even some of the logically minimal instances are avoided,
as explained above.
7.1 Estimating the size of the tree
We can give additional evidence that the work required by ordered semantic hyperlinking
is often small. We note that it is only the small atoms that influence the
search, as defined in definition 3.7. Let us consider the smallest n atoms in A
and the probability that a clause C over these atoms will contradict an arbitrary
interpretation I. Suppose that there are m 3-literal ground clauses C in all. Then
the chance that a random interpretation I will not satisfy a specific clause C is 1=8
(since each of 3 literals must be mapped to false). Therefore the chance that the
interpretation will satisfy the clause C is 7=8;, and the chance that I will satisfy m
3-literal clauses is (7=8) m , if the clauses are chosen independently. The expected
number of models of m independently chosen clauses (considering only the first n
literals) is then 2 n (7=8) m since there are 2 n interpretations altogether for n atoms.
We note that and the expected
number of models is less than one. This means that the probability is very small
that we will have to search past the first n atoms to find a contradicting instance.
This is evidence that if there are many clauses then the size of the tree is small,
on the average. What we are given is non-ground clauses, in general, instead of
ground instances, so the determining quantity for this analysis is the number of
their ground instances of various sizes, and how they depend on one another.



--R

Theorem proving via general mating.
On restrictions of ordered paramodulation with simplification.
Ordered chaining for total or- derings
Automated Theorem Proving.
Challenge problems in elementary calculus.
Model finding in semantically guided instance-based theorem proving
Rough resolution: a refinement of resolution to remove large literals.
Semantically guided first-order theorem proving using hyper-linking
The use of presburger formulas in semantically guided theorem proving.
Rewrite systems.
A computing procedure for quantification theory.
Resolution Methods for the Decision Problem.
The complexity of resource-bounded first-order classical logic
Proving refutational completeness of theorem-proving strategies: the transfinite semantic tree method
A simplified format for the model elimination procedure.
Eliminating duplication with the hyper-linking strategy
Otter 1.0 Users' Guide.
Conditional term rewriting and first-order theorem proving

Equational reasoning and term rewriting systems.
The search efficiency of theorem proving strategies.
The search efficiency of theorem proving strategies: an analytical comparison.
Automatic theorem proving with renameable and semantic resolution.
--TR

--CTR
Kahlil Hodgson , John Slaney, TPTP, CASC and the development of a semantically guided theorem prover, AI Communications, v.15 n.2,3, p.135-146, August 2002
Kahlil Hodgson , John Slaney, TPTP, CASC and the development of a semantically guided theorem prover, AI Communications, v.15 n.2, p.135-146, September 2002
R. Janvier , Y. Lakhnech , M. Prin, Certifying cryptographic protocols by abstract model-checking and proof concretization, ACM SIGBED Review, v.3 n.4, p.37-57, October 2006
David A. Plaisted , Adnan Yahya, A relevance restriction strategy for automated deduction, Artificial Intelligence, v.144 n.1-2, p.59-93, March
Adnan Yahya , David A. Plaisted, Ordered Semantic Hyper Tableaux, Journal of Automated Reasoning, v.29 n.1, p.17-57, 2002
Carsten Sinz, Visualizing SAT Instances and Runs of the DPLL Algorithm, Journal of Automated Reasoning, v.39 n.2, p.219-243, August    2007
Peter Baumgartner, First-order logic Davis-Putnam-Logemann-Loveland procedure, Exploring artificial intelligence in the new millennium, Morgan Kaufmann Publishers Inc., San Francisco, CA,

extracted:['feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-containment'
 'finite automata' 'zero storage biometric authentication' 'semantics']
marked:['semantics', 'automated theorem proving', 'hyper-linking']
--T
Semi-Thue Systems with an Inhibitor.
--A
A i>semi-Thue system with an inhibitor is one having a special symbol, called an i>inhibitor, that appears on the right side of every rule but does not appear on the left side of any rule. The main result of this paper is that the uniform halting problem is decidable for the class of such systems. The concept of i>inhibitor is related to the concept of i>well-behaved derivation in systems without an inhibitor. The latter concept has received some attention from those interested in the open question as to whether the uniform termination problem for one-rule semi-Thue systems is decidable.
--B
April, 1998 (revised)
1. Introduction, notation and terminology. Semi-Thue systems constitute a
universal model of computation, in the sense that any decision problem about
computations is reducible to a problem about semi-Thue systems. Every recursively
enumerable language is the language of some semi-Thue system. (E.g., see Chapter 7
of [3].)
And yet the concept is simple: A semi-Thue system is an ordered pair (\Sigma; \Pi),
where \Sigma is a finite alphabet of characters and \Pi a finite set of rules (u; v), where in each
case u and v are words over \Sigma. These systems are called "semi-Thue systems" because
the rules are not necessarily reversible, u being referred to as "the left side" and v "the
right side" of the rule (u; v). Semi-Thue systems are distinguished from Thue systems
(named for their originator Axel Thue [15]) whose rules operate in both directions.
The important notion in the study of semi-Thue system is that of a derivation
from one string to another. We write to mean that w 2 is derived in one step
from w 1 , i.e., that there exist words x and y such that w
(u; v) is a rule of the system. We say that z is derivable from w or that there is a
derivation from w to z, if there exist strings w
Each w i is a line of the
derivation, which has lines, and has p steps; p is the length of the derivation. An
infinite derivation is an infinite sequence w
nonnegative integers i.
Although the class of semi-Thue systems is quite powerful, there have been some
applications in which not all this power is appropriate. An example is the context-free
Supported by Grant No. CCR-9500182 from the National Science Foundation.
grammar, which had as its origin the phrase-structure grammar put forth tentatively
and critically by Chomsky [2] for natural languages, but which by now has its chief
application in the domain of formal languages (see [11]). A context-free grammar is a
semi-Thue system that (1) distinguishes between "terminal" and "nonterminal"
characters of the alphabet, (2) restricts rules to those having single nonterminals as
their left sides, and (3) restricts derivations to those whose first lines all consist of a
single occurrence of a particular nonterminal designated as the start symbol (usually S
in the literature) and whose last lines have only terminal characters.
Another application has arisen in the area of computer science concerned with
theorem proving by machine, where semi-Thue systems are used and thought about
extensively. Typically, these semi-Thue systems, often called "rewrite systems," are
used to reduce words to simpler equivalent words (see, e.g., [7] or [1]). Rewrite systems
and context-free grammars are quite different both in their purpose and their
operation. Nevertheless, these enterprises do have in common their use of
computationally weak semi-Thue systems.
The research program into which the present paper fits is not concerned with
particular applications of semi-Thue systems such as grammars and rewrite systems.
Rather it considers various subclasses of semi-Thue systems that appear to be usefully
weaker than the entire class (without regard to any particular application), and then
attempts to assess how useful and weak they are. One method of assessing a subclass is
to determine whether certain decision problems are decidable for it. If so then that is
evidence that the subclass is weak, since most significant decision problems are
undecidable for the full class of all semi-Thue systems. And, if the decision problem is
a computationally important one, it is evidence also that the subclass may be useful.
There are several decision problems that can be used as criteria in this way. Let us
focus on three of them:
(1) The halting problem. Given a semi-Thue system (\Sigma; \Pi) of the
subclass, and given x 2 \Sigma   , is every derivation whose first line is x finite?
(2) The uniform halting problem. Is every derivation in a given
semi-Thue system of the subclass finite?
(3) The derivability problem. Given a semi-Thue system of the subclass,
and given x; y 2 \Sigma   , does there exist a derivation of y from x?
For example, for both the class of context-free grammars and the class of rewrite
systems used by many theoreticians of machine theorem proving, all three of these
problems are decidable. Moreover, any subclass of the semi-Thue systems for which
any of these three problems is solvable is weaker and possibly more useful than the
class of all semi-Thue systems. Weaker because all three problems are undecidable for
the class of all semi-Thue systems. And more useful, albeit in a restricted set of
possible applications, because the ability to tell whether or not derivations will
terminate gives the user an advantage. The importance of termination in the practical
use of rewrite systems is described in detail in [6].
The uniform halting problem seems more difficult than the other two, although
there seems to be no general proof that if that problem is solvable for any subclass of
semi-Thue systems then the other two problems are solvable also. At any rate, several
research workers have decided to focus on the uniform halting problem; in particular,
the question of whether or not that problem is decidable for the subclass of one-rule
semi-Thue systems has received some attention, so far without an answer, in [4], [5],
[8], [9], [10], [12], [13], [14] and [16]. This subclass will be discussed in Section 4.
The subclass that is the main focus of this paper is the subclass of semi-Thue
systems with an inhibitor, i.e., an alphabetic character that occurs at least once on the
right side of every rule, but does not occur on the left side of any rule. The main result
is that the uniform halting problem for this subclass is decidable, proved in Section 3,
where it is also proved that the halting problem and the derivability problem are
decidable. Derivations in semi-Thue systems with an inhibitor will be analyzed in
detail in Section 2.
The subclass of semi-Thue systems with an inhibitor was noticed in the course of
studying the subclass of one-rule systems [13]. The results of the present paper on
semi-Thue systems with an inhibitor lead to a distinction that promises to be fruitful
in analyzing derivations in one-rule systems without an inhibitor. That distinction,
between well behaved derivations and ill behaved derivations, is explained in Section 4.
In this paper lower-case Greek iota (') will be used as an inhibitor. So, in a
semi-Thue system with an inhibitor, ' 2 \Sigma, every right side has at least one occurrence
of ', and no left side has any occurrence of '. Our convention will be that a semi-Thue
system has k rules:
We shall write x ! n y to mean that there is a derivation of n steps (n - 0) from x
to to mean that there is a derivation from x to to mean that
there is such a derivation of at least one step.
For mean that there exist z 1 ; z 2 2 \Sigma   such that x ! z 1 yz 2 . We
shall write x . + y to mean that there exist z 1 ; z 2 2 \Sigma   such that
The well known fan theorem states that an infinite rooted tree in which each node
has only finitely many children nodes has an infinite path. The following generalization
will be useful in Section 2; the proof is left to the reader:
Theorem 1.1. If a forest of finitely many rooted trees has infinitely many nodes,
each of which has only finitely many children nodes, then the forest has an infinite path.
Definition (the y). If x; y and z are words and xyz is a line in a derivation then
it may be that y occurs several times as a factor (i.e., substring) of that line. For
example, then there are
three occurrences of y in xyz, the second of which we shall refer to as "the apparent
occurrence of y in xyz," or "the y in xyz," or (frequently, when the designation xyz is
understood) "the y." The article "the" will indicate that we are referring to a
particular occurrence of y and not to the word y itself. When we wish to talk about the
word apart from any occurrence in a line we shall simply say "y."
Definition (the same occurrence). If x 1 bx 2 u h x 3 th line of a
derivation where b is a letter and x 1 bx 2 v h x 3 is the (j st line of that
derivation, then we talk of the apparent occurrence of b in the j th line as being the
same as the apparent occurrence of b in the (j st line. On the other hand, no
occurrence of a character in the apparent occurrences of u h or v h exists in the other
line: any character occurrence in the apparent u h is destroyed, and any character
occurrence in the apparent v h is created, in going from the j th line to the (j st line.
2. Analysis of derivations. Section 3 will present the decision procedure to
determine whether a given semi-Thue system with an inhibitor is uniformly
terminating. In preparation, this section will develop a method of analyzing derivations
in such systems. To begin, it is stipulated that there be no occurrence of the inhibitor '
in the first line of a derivation. This stipulation will help us establish important
structural concepts. It is justified by the fact that (since ' does not occur on the left
side of any rule) if there is an infinite derivation from x 1 'x then there is an
infinite derivation from one of the x i 's. All semi-Thue systems discussed in this section
and Section 3 will be systems with the inhibitor '.
Definition (vital). For b 2 \Sigma, if xby is a line other than the first in a derivation,
then the apparent occurrence of b is vital if it is not ' and has been created either in
that line or in some preceding line of the derivation (in other words, if that occurrence
of b does not exist in the first line).
Definition (S-occurrence, the set S). (1) If w 6= - and xwy is a line of a
derivation then the w is an S-occurrence in that line if all its character occurrences are
vital, the x does not end in a vital character occurrence, and the y does not begin with
a vital character occurrence. (That is to say, the x must either be null or end in a
nonvital character occurrence. And the y must either be null or begin with a nonvital
character occurrence.) In short, a nonnull S-occurrence is a maximal factor of vital
character occurrences in a line of a derivation. (2) If a line of a derivation begins with
(ends in) ' then the occurrence of - at the left end (right end) of the line is an
S-occurrence. And if bc is a factor of a line where both the b and the c are nonvital
character occurrences, at least one of the two being ', then the - between the b and the
c is an S-occurrence. (3) S is the set of all words having an S-occurrence in some line
of some derivation.
Note that if x's'y !   z and s has no ' then
and y !   y 0 . This follows from the segregating power of the inhibitor ', and shows the
importance of S-occurrences in lines all of whose non-iota character occurrences are
vital. Even more revealing of the importance of S-occurrences is the next theorem.
Definition (S-sequence). The sequence s is an S-sequence if, for each i,
Theorem 2.1. In a semi-Thue system with an inhibitor, there exists an infinite
derivation only if there is an infinite S-sequence.
Proof: If there is an infinite S-sequence s then an infinite derivation
can be defined as follows: For each i, define z i and z 0
i so that s
(guaranteed by the definition of s i . s
is an infinite derivation.
Now assume there is an infinite derivation w
where the character occurrences of w 1 that are never destroyed in the derivation are
precisely those that occur in the apparent factors y . So, for every i,
for some z . Consequently, for some j, 1 - j - m, the sequence
z
is an infinite derivation possibly with repetitions; that is to say, there is an infinite
sequence
For convenience we alter the notation and set z
is an infinite derivation in which every character occurrence of z 1 is eventually
destroyed. Since z 1 is only finitely long, there exists a p such that, for all i - p, the line
z i has no character occurrences in common with z 1 .
Thus for all i - p,
where each s i;h 2 S, and q is the number of "s in z i .
For each line i - p, there is exactly one h such that the s i;h is rewritten by a rule,
becoming (for some in the (i st line; for all
is copied to become the very same S-occurrence in the
st line (but with a different subscript for
to mean that the s i;h 0
is copied to become the s i+1;h 00
in the
st line. Then define C to be the transitive, symmetric and reflexive closure of the
relation C 0 . Thus C(!
and the s i;h is copied, recopied, etc., until it becomes the s i 0 ;h 0
in the (i 0 ) th line, or
is copied, recopied, etc., to become the s i;h .
Clearly, C is an equivalence relation; furthermore if C(! then the
word s
. Note also that, for any i and h - q i , if there is no h 0 - q i+1 such that
for finitely many values of h 0 , s i;h . s i+1;h 0
Consider now the graph H whose nodes are the equivalence classes of the C
relation: The roots of H are the classes having the respective pairs
as members; there is an edge from a node N 1 to a node N 2 of H
if and only if there are and the s i+1;h 0
in the (i st line is part of what is obtained by rewriting from the s i;h in the i th line.
Clearly, H is a forest of rooted trees. Define
Since the z-derivation is infinite, H has infinitely many nodes, each of which has
only finitely many children nodes. Hence, by Theorem 1.1, there is an infinite path
through the Putting s
Definition (v-section, L(v h ); R(v h )). The word w is a v-section if ' does not
occur in w and there is a v h such that, for some x and y, either v
or x'w. Thus the null word is a v-section if and only if some v h either begins in ',
ends in ', or has '' as a factor. If v
In particular, L(v h ) (R(v h only if v h begins
with (ends in) '.
Note that every v-section has an S-occurrence in some derivation. Whenever a
rule applied in a derivation where v the apparent
occurrences of the internal v-sections always become S-occurrences. The apparent
occurrences of L(v h ) and R(v h ) become either S-occurrences or parts of S-occurrences.
E.g., for the system
in the third line of the derivation
a
a'bc'd
a'be'f'g'd
the occurrence of g is an S-occurrence, but the occurrence of of the
S-occurrence be.
An S-occurrence of - must, in its first line, be the result of an application of a rule
having lambda as a v-section. However, if L(v h ) or R(v h the application of the
rule need not result in an S-occurrence of -. E.g., in the system
In the derivation
a
a'bc'd
a'b'f"d
the application of the second rule results in the third line in which R(v 2 ) becomes an
S-occurrence of - but L(v 2 ) does not, being absorbed into the S-occurrence b.
Generally, we can think of a nonnull S-occurrence in a line as a maximal substring
occurrence consisting of consecutive vital character occurrences; it is brought about by
one or more rule applications each of which is responsible for creating some of its vital
character occurrences. The earliest S-occurrences are v-sections, which come about as
a result of rule applications. The more complex S-occurrences are brought about by
the modification of simpler S-occurrences. The theorems that follow will describe this
process in detail.
Theorem 2.2. If the s is an S-occurrence in a line xsy of a derivation, then
either x ends in ' or y begins with ' or both.
Proof: Let s be any word having an S-occurrence in the derivation. Let xsy be
the first line that has that occurrence of s as an S-occurrence. Then
some ff; i; fi, the preceding line being ffu i fi. Let us make three observations.
(1) The v i must have either the rightmost character occurrence of the x, a
character occurrence of the s or the leftmost character occurrence of the y. (Otherwise
s would have an S-occurrence in the preceding line.)
(2) The v i must have a character occurrence in common with either the x or the y.
(Since s has no ' but v i does, s cannot account for all of v i .)
(3) If the v i has any two character occurrences of the line xsy then it has every
character occurrence between.
From (1), (2) and (3) we infer that the v i contains either the rightmost character
occurrence of the x or the leftmost character occurrence of the y.
Case I: is a single character, and the v i contains the b. Then since
the s is an S-occurrence, the b is not vital. And since it is part of the v i , it is not a
character occurrence of the first line. Hence
Case II: is a single character, and the v i contains the c. Then, by
similar reasoning,
We have proved that our theorem is true of the first line having that s as an
S-occurrence. But since "s are indestructible in a derivation, our theorem is true of all
the lines having that s as an S-occurrence.2
We consider two examples. The first is the system with the two rules (ab; aac')
and (c; b'), and the derivation
ab
aac'
whose S-occurrences are underlined. These are the first four lines of an infinite
derivation, which is obtained by applying the two rules alternately. The lines of this
derivation have increasingly long S-occurrences at their left ends, followed by several
S-occurrences of -.
Our second example is the system with the two rules (ab; b'ac) and (c; b'a) and the
derivation
ab
b'ac
b'ab'a
b'b'ac'a
b'b'ab'a'a
Each line of the resulting infinite derivation will have several S-occurrences of b,
followed by an S-occurrence of either ab or ac, and then several S-occurrences of a.
Definition (the sets S L and SR ). S fxjsome derivation has a line x'y in
which the x has no nonvital character occurrencesg. fyjsome derivation has a line
x'y in which the y has no nonvital character occurrencesg. It is not difficult to see that
is the number of
rules.)
It is clear from the definitions that S L [ SR ' S.
Theorem 2.3. If xbsy (xsby) is a line of a derivation in which the s is an
S-occurrence and the apparent letter b is an occurrence from the first line, then
Proof for xbsy: By Theorem 2.2, . The result of deleting all
lines of the derivation after the line xbsy, and then deleting the noted b and all
characters to its left from all lines, is a derivation possibly with repeated lines, having
the line sy = s'y 0 in which the apparent s is an S-occurrence, and hence is vital. Thus
The proof that s 2 SR when the line is xsby is symmetric.2
Theorem 2.4. (a) S the smallest set C such that, for all j, (1) L(v j
the smallest C 0 such that, for all j,
Proof that C ' L from the two-line derivation
y has no ' and there is a derivation of xu j y'z, for some z. This
derivation can be extended to become a derivation of xv j y'z, which shows that
Proof that S L ' C: In any derivation D, put th line does not
begin with a character occurrence of the first lineg. If and D has a j 0th
line then also. For each j 2 J(D), let the j th line be x j 'y j , where x j is
iota-free. Clearly, if J(D) 6= ; and j 0 is the smallest member of J(D) then x
for some h. Also,
j(D)g. We have proved that
L(D) where the union is taken over all derivations D in the
system, we have S L ' C.
Thus S C. The proof that
We shall generally focus on S L knowing that, whatever we prove about S L , an
appropriate similar assertion can be proved about SR .
Definition (The set S Li ). For each
The proofs of the following two theorems are straightforward:
Theorem 2.5. S
Theorem 2.6. If xu j y 2 S Lj 0
then xS Lj ' S Lj 0
Continuing the examples given above, in the system whose rules are (ab; aac') and
(c; b') we have S f-g. In the system whose rules are
(ab; b'ac) and (c; b'a), we have S ag.
Theorem 2.7. the smallest set S 0 such
(1) every v-section is in S
Proof: To prove that S 0 ' S we verify that (1)-(5) are all true of S. Items (1)
and (2) are clear.
The verification for item (3) is as follows: Assume u
some z 0 there is a derivation whose last line is yz'z 0 where the yz is vital. The result of
appending x to the left end of every line is a derivation whose last line is
. We can extend this derivation by adding the line v h z'z 0 , in which the
word R(v h )z will have an S-occurrence, showing that R(v h )z 2 S. The verification for
item (4) is similar.
To verify that (5) is true of S, assume xu h y 2 S. Then, for some z and z 0 , there is
a derivation whose last line is zxu h yz 0 in which the xu h y is an S-occurrence. If we
append to this derivation the line zxv h yz 0 the result is a derivation whose last line has
both xL(v h ) and R(v h )y as S-occurrences, showing that both these words are in S.
(Examples illustrating (3) and (5) are provided after this proof.)
To prove that S ' S 0 we assume an arbitrary derivation D and prove that every
S-occurrence in D is in S 0 . We do so by proving the following proposition by
mathematical induction on i: If D has at least i lines then every S-occurrence in the i th
line is in S 0 .
This proposition is true for since the first line has no S-occurrences.
Assume it is true for the i th line and let the (i st line be xsy in which the s is an
S-occurrence. Assume also that s does not have an S-occurrence in the i th line, and
that is the rule by means of which the i th line is rewritten as the (i st line.
either (a) at the left end of the line, which begins in ', (b) at the
right end of the line, which ends in ', (c) flanked by two consecutive ''s, (d) flanked by
nonvital b 6= ' on the left and ' on the right, or (e) flanked by ' on the left and nonvital
c 6= ' on the right. Since - is not an S-occurrence in the preceding line in the
derivation, the s must be a factor of the v h in the (i st line. In each of the cases (a),
(b), (d) and (e), the ' flanking the s must be part of the v h and - is a v-section of v h .
In case (c) either both "s are part of the v h and hence - is an interior v-section of v h ,
or only one of the two "s is part of the v h and - is an end v-section of v h .
Henceforth we assume s 6= -. Let the noted occurrence of s in the (i st line
consist of the B(s) th through the E(s) th characters of that line, in left-to-right order.
And let the noted occurrence of v h in that line consist of the B(v h ) th through the
E(v h ) th characters. Note that B(v h E(s). To carry through the
proof, we divide into cases based on the relative positions of s and v h in the
st line.
Case I: B(v h ) - B(s) and E(s) - E(v h ). Then s is a factor of v h , and since s is an
S-occurrence of the (i st line, s must be a v-section of v h . By (1), s 2 S 0 .
In the remaining cases either E(v h
v h has an occurrence of ' but s does not, the v h cannot be wholly inside the s. The
possibility E(v h give rise to Cases II and III, while the possibility
give rise to Cases IV and V, which are right-left symmetric to
Cases II and III, respectively.
Case II: E(v h E(s). Then
there would be no change in the vicinity of the s in
going from the i th line to the (i st line, and the s being an S-occurrence in the
st line would also be an S-occurrence in the i th line, contrary to our assumption.
Furthermore, the rightmost character of v h must be ', otherwise that character
occurrence would be vital and the s would not be an S-occurrence in the (i st line.
Thus R(v h since s does not have an S-occurrence in the i th line, the s must
be a proper suffix of an S-occurrence rs in that line.
Case IIa: r is a proper suffix of u h , i.e., u rs is an
S-occurrence in line i preceded by a character occurrence of the first line. (The
rightmost character occurrence of r 0 cannot be ' since it is part of u h , and cannot be
vital since it is not part of the S-occurrence rs.) It follows by Theorem 2.3 that
rs 2 S L . Taking apply (3) getting R(v h )s 2 S 0 . Since
Case IIb: r is not a proper suffix of u h . Then . Here we
apply (5) to get completing Case II.
Case III: E(v h cannot be a substring of s, we
get E(s). Consider the
character occurrence b immediately to the left of the leftmost character occurrence of s.
It must be an ', for otherwise it would be vital and would have to be part of the
S-occurrence s. Furthermore, it must be the rightmost ' in the v h , for otherwise, there
would be an ' inside of the S-occurrence s. Therefore, we can assume that the (i st
line is x y, where v th line is x 0 u h z 3 y.
Case IIIa: all the character occurrences of the u h are vital in the i th line. Then, for
some x 00 ; x 000 where x the x 000 u h z 3 is an S-occurrence in the i th line. By the
inductive hypothesis, x 000 u h z 3 2 S 0 , and by (5),
Case IIIb: not all the character occurrences in the u h in the i th line x 0 u h z 3 y are
vital. Then u all the character occurrences in the u 00 are vital
but the rightmost character occurrence in the u 0 is not vital. This character occurrence
is not an ', so it must be a character occurrence of the first line of the derivation. It
follows that u 00 z 3 is an S-occurrence, since all its character occurrences are vital but it
is not adjacent to a vital character occurrence in the i th line, either on the left or on the
right. Moreover, by Theorem 2.3, u 00 z 3 2 S L . Thus
Case IV: This case is left-right
symmetric to Case II. (It is obtained from Case II by simultaneously interchanging !
and ? and interchanging B and E.)
Case V:
This case is left-right symmetric to Case III.2
We illustrate (3) of Theorem 2.7 by the Thue system
Taking Hence
by (3) we get derivation with an S-occurrence of ga is:
bc
baa'e
h'ga'e
We illustrate (5) by the Thue system
Assume for the moment that abcd 2 S 0 , and take by (5) we
get derivation with S-occurrences of abcd,
ah and jd is
h'abf
Definition (ABC property). A word w has the ABC property if there exist
words A; B; C such that
Theorem 2.8. Every s 2 S has the ABC property.
Proof: Theorem 2.7) we can complete our proof by showing that
every s 2 S 0 has the ABC property.
To this end we note that (1) if s is a v-section then we can take
For (2), if s 2 S L (S R ) then we can take
For (3), if then we can take
y.
For (4), if then we can take
For (5) we should demonstrate that if xu h y has the ABC property then both
it. Accordingly, we assume that xu h
We shall prove that s 1 has the
property, leaving the similar proof for s 2 to the reader. A division into cases is
required according to whether or not x is a prefix of A 0 and, if not, whether or not it is
a prefix of A 0 B 0 .
Case I: A
which satisfies the requirement, since Ax
Case II:
which satisfies the requirement, since A is the same as A 0 , B is
no longer than B 0 and x 2
Case III: y, we have C
y. We now take
2 . Clearly
the requirement, since they are the same as A 0 , x 0
It remains to prove x 2 C 2 S L . Note first that
So, by Theorem 2.4, x 0
our proof is complete.2
Theorem 2.9. If S is infinite then either S L or SR is infinite.
Proof: If S is infinite then fjsjjs 2 Sg is an unbounded set of lengths. By
Theorem 2.8, each accord with the ABC property. Since fjB s jjs 2 Sg is
bounded, either fjA s jg or fjC s jg is unbounded. But each A s (C s ) is a factor of a
member of SR (S L ). It follows that either SR or S L is infinite.2
3. The algorithm. This section presents the algorithm to determine whether a
given semi-Thue system with an inhibitor is uniformly terminating, 1 using the analysis
of the preceding section. At the conclusion of this section, it is proved that the halting
problem and the derivability problem are also decidable.
The algorithm will begin by constructing finite automata for the languages S L
and SR .
Definitions related to finite automata. A nondeterministic finite automaton
(automaton for short) is a finite directed graph each of whose arcs has either a letter or
the symbol - as a label; one node is designated as the initial node and any number of
nodes are designated as accepting nodes. A walk through the graph is a sequence
where the N i 's are nodes and the A i 's are arcs, such that for each i, A i goes from N i\Gamma1
to N i . This walk goes from N 0 to N p . The word spelled out by this walk is the result of
deleting all -'s from the word a 1 a 2 \Delta \Delta \Delta a p where, for each i, a i is the label of N i . In
particular, the word spelled out is the null word itself if a i. The language
of an automaton is the set of all words spelled out by walks from the initial node to an
accepting node.
We begin by constructing, for the finite language fL(v h )j1 - h - kg, a loopless
finite automaton G 0
(1) exactly one initial node N I with no arc entering it;
(2) exactly one accepting node N T with no arc leaving it;
(3) exactly one path from N I to N and exactly one path from N to N T , for
each node N other than N I and N T ;
(4) nodes N Lk such that, for each i, the path from N Li to N T spells
out L(v i ); in particular, if L(v i there is simply a lambda arc, i.e.,
arc labeled -, from N Li to N T ; and
(5) for each i, a lambda arc from N I to N Li .
The construction is done so that
there are no nodes in G 0 other than those required by (1)-(4), and there
are no lambda arcs except those explicitly mentioned in (4) and (5).
From G 0 we construct a finite automaton G L for the language S L by repeating the
following step as often as possible: for a pair of nodes N; N 0 and an integer
I am grateful to Friedrich Otto for pointing out a defect in a previous version of this algorithm
in [13].

Figure

1: G 0

Figure

2: G L
if there is a walk from N to N 0 spelling out u i , insert a lambda arc from N to N Li ,
provided that there is not one already there.
Note that G L and G 0 , having the same set of nodes, differ only in that G L has
certain lambda arcs that G 0 does not have. It follows that the construction step is
repeated only finitely many times and the graph G L so constructed is a
nondeterministic finite automaton.
As an example, G 0 and G L for the semi-Thue system
are shown in Figures 1 and 2, respectively. Thus S
The automaton GR for the language LR is like G L except that N I and N T are
interchanged and all arrows are reversed. More explicitly, we first construct G 0R for
the same as (1),(2),(3);
Rk such that, for each i, the path from N I to N Ri
spells out R(v i this is simply a lambda arc; and
each i, a lambda arc from N Ri to N T .
The construction is done so that
are no nodes in G 0R other than those required by (1 0 )-(4 0 ), and
there are no lambda arcs except those explicitly mentioned in (4 0 ) and (5 0 ).
From G 0R , GR is constructed by repeatedly finding N , N 0 and i such that there is
a walk from N to N 0 spelling out u i ; and then inserting a lambda arc from N Ri to N 0 .
GR is similar enough to G L so that we can carry through detailed reasoning about G L
knowing that corresponding things about GR will also follow.
Theorem 3.1. The language of G L (GR ) is S L (S R ).
Proof for G L : With Theorem 2.4, the proof that S L is a subset of the language of
G L is straightforward and is left to the reader.
For the converse let A q be the lambda arcs in G L other than those of
in the order in which they are added in the construction. For each h, 1 - h - q, let
G h be the graph that results from G 0 by adding the lambda arcs A . Thus for
Where A h+1 goes from N to N Li ,
there is a walk in G h from N to some node N 0 spelling out u i .
Let P (h; n), for q - h - 1 and n - 0, be the following assertion: For all i,
spelled out by a walk in G h from N Li to N T in which the lambda arc
A h occurs at most n times then w 2 S Li .
Our objective will be to prove that P (q; n) is true for all n. First we note that
is true, since for each i there is only one walk in G 1 without the arc A 1 from
N Li to N T , which is a walk in G 0 , and that walk spells out the word L(v i
Next we prove that, for each h - q and n,
n) and let w be spelled out by a walk W in G h from N Li to N T in which A h occurs
times. Let occurs n times in W 1 but does not occur in
respectively. Assume A h goes from node N to
N Lg . By the construction of G h from G h\Gamma1 there is a walk W 3 in G h\Gamma1 from N to N T
spelling out a word u g y for some y. The walk W 1 W 3 spelling out w 1 u g y has only n
occurrences of the arc A h . Thus P (h; n) implies w 1 u . The walk W 2 from N Lg
to N T has no occurrences of the arc A h . Hence w 2 2 S Lg , which, by Theorem 2.6,
implies
From this it follows by mathematical induction that, for all n and h - q, P (h;
implies P (h; n). But, for all h ! q, P (h; n) for all n is equivalent to P
Putting all this together we get the proposition
For all n; P (q; n)
which, by Theorem 2.5, clearly implies that the language of G is included in S L ,
concluding the proof that the language of G L equals S L .
The proof that the language of GR is SR is similar.2
Next it is proved that certain derivations in the semi-Thue system can be obtained
from certain walks in G L and GR . In particular, loops in the automaton graphs will
yield derivation loops in the semi-Thue system. We confine our attention to G L ,
knowing that corresponding results about GR will also be valid. We refine our
consideration of the algorithm in obtaining the graph G L from G 0 , considering the
sequence as defined in the proof of Theorem 3.1.
Definition (w 0 (N)). For any node N other than N I , w 0 (N) is the word spelled
out by the unique walk from N to N T in G 0 .
Theorem 3.2. For each i, 0 - i - q, and for any two nodes N and N 0 in G i other
than N I , if there is a walk from N to N 0 spelling out a word x, then
z. If the walk has at least one lambda arc then
some z. These derivations are obtainable effectively.
Proof: We begin by proving the first sentence by mathematical induction on i.
That sentence is clearly true for 0: in this case w 0 since the only
relevant walks are segments of the walks from the N Li 's to N T , which are disjoint from
one another. We now assume it is true for inductive hypothesis)
and prove it is true for i + 1. This proof is itself by mathematical induction on the
length of the walk from N to N 0 . The proposition is clearly true when this length is 0.
We assume it for walks of length e (the e inductive hypothesis) and we prove it for
walks of length e + 1. Thus let
be this walk in G i+1 , and let the word x a be the word spelled out by it, x e
being the word spelled out by the walk N of length e. By the e inductive
Case I: a 6= -. Then the arc from N e to N e+1 labeled a is on the walk in G 0 from
N e to N T . Consequently, w 0 (N e
Case II: h, and (whether the lambda arc from
N e to N e+1 is the new lambda arc of G i+1 or one already in G i ) there is a walk from N e
to N T in G i spelling out a word u h z 0 , for some z 0 . By the i inductive hypothesis,
. Thus we have, for some z 000 :
since x
This concludes the proof of the first sentence in the statement of our theorem.
The second sentence follows from the fact that a lambda arc in the walk causes Case II
to apply, insuring that the derivation has at least one step. Clearly, all these
derivations are obtained effectively.2
Definition (loop derivation). If w .   y and y . + y then we say the system has a
loop derivation on y from w. Note that a loop derivation provides us with one kind of
infinite derivation: from w we get the infinite derivation
From the proof of Theorem 3.2 we also get
Theorem 3.3. If node N is on a loop in G L then there is a loop derivation on
in the semi-Thue system.
(It should be mentioned that a loop does not imply that S L is an infinite set. It is
possible for the loop to consist entirely of lambda arcs and for S L to be finite. For
example, this happens with the system with the two rules (a; b'c), (b; a'c), for which
bg. Following the constructions we get a . + a, which we could have inferred in
this simple example from the observation that a ! 2 a'c'c.)
Theorem 3.4. If s 2 S and jsj ? (2k 1)m then there is a loop derivation from s.
Proof: That s has the ABC property (by Theorem 2.8) and that jsj ? (2k
together imply that
either jAj ? km or jCj ? km.
Case I: jCj ? km. Since x 2 C 2 S L , there is a walk in G L spelling out C ending at
N T . But, by the construction of G L , all words spelled out by loop-free paths in G L have
length - km. Thus there is a loop in the walk spelling out C, and so by Theorem 3.3
there is a loop derivation on some suffix of C, and hence a loop derivation from s.
Case II: jAj ? km. The proof is similar, using the graph GR for SR :2
Theorem 3.5. There is an algorithm that determines whether both G L and GR
are without loops and, if so, enumerates the finite set S.
Proof: By Theorem 3.1, S It is easy to tell whether
both graphs are without loops.
Assume now that they are without loops. Then the sets S L and SR are finite and,
by Theorem 2.9, so is S. S L and SR are enumerable from G L and GR . Let S 0 be the
smallest set satisfying (1)-(4) of Theorem 2.7. From the finite enumeration of S L and
SR we can enumerate S 0 . Noting that S is the smallest class that contains S 0 and is
closed under (5) of Theorem 2.7, let us recursively define the sets S i+1 , for all i - 0:
Clearly, S i+1 is computable from S i ; and
. But since S is finite by
Theorem 2.9, in computing the successive S i 's, eventually we shall reach an i such that
which implies that, for this i,
Theorem 3.6. If a semi-Thue system with an inhibitor in which S is finite has an
infinite derivation then there exists an s 2 S such that s .
Proof: Since there is an infinite derivation, there is an infinite S sequence
Theorem 2.1. Because S is finite there must exist p and q, q ? p, such
that s
Theorem 3.7 (Main Theorem). There is an algorithm that produces either a
loop derivation in a given semi-Thue system with an inhibitor, or the information that
the system is uniformly terminating.
Proof: The algorithm begins by constructing the automaton G L for S L . If G L has
a loop then from that loop a loop derivation is effectively determined, by Theorem 3.3.
If G L has no loop, the analogous automaton GR for SR is constructed. If GR has a
loop, analogously a loop derivation is effectively determined.
If neither G L nor GR has a loop then, by Theorem 3.5, S is finite and can be
enumerated. The . relation on S is computed and, from this, the . + relation on S. If
there is an s 2 S such that s . + s then we have a loop derivation, effectively.
Otherwise, by Theorem 3.6, the system has no infinite derivation.2
Corollary 1. The uniform termination problem for semi-Thue systems with an
inhibitor is decidable.
Corollary 2. If a semi-Thue system with an inhibitor has an infinite derivation
then it has a loop derivation.
In studying the complexity of the algorithm of Theorem 3.7, we assume that the
expression T naming the semi-Thue system is simply the list of its rules,
The following assertions should be clear to the reader: The
automaton G 0 is constructible in polynomial time. Each G i+1 is constructible from G i
in polynomial time. Since all the automata G have the same set of
nodes and each G i+1 is obtained from G i by adding an arc, q is bounded by a
polynomial in the number of these nodes. Thus the construction of the
nondeterministic finite automaton G L is accomplished in polynomial time, and
similarly for GR . It is possible to determine in polynomial time whether G L (GR ) has a
loop, and if so to produce the loop derivation in polynomial time.
However, I cannot prove that the enumeration of S L (S R ), if it is finite, can be
done in polynomial time, since jS L j (jS R j) may be exponential in the size of G L (GR ).
Thus if G L or GR has a loop, the main algorithm produces a loop derivation in
polynomial time and terminates. But if neither G L nor GR has a loop then the main
algorithm has to enumerate the finite set S. Since there is no polynomial bound on jSj
for those S that are finite, the algorithm as written in the proof of Theorem 3.7 is not a
polynomial-time algorithm. However, this does not imply that the following has a
negative answer:
Open question 1. Is there a polynomial-time algorithm for the problem of
whether a given semi-Thue system with an inhibitor is uniformly terminating?
This section closes by settling the two remaining problems of Section 1, the
halting problem and the derivability problem for semi-Thue systems with an inhibitor.
Theorem 3.8. The halting problem for semi-Thue systems with an inhibitor is
decidable. If such a system has an infinite derivation from a word w then it has a loop
derivation from w.
Proof: For a given w and be the set of
derivations from w of length i, and
let\Omega i be the set of all S-sequences that can be
taken from those derivations (as in the proof of Theorem 2.1). Note that \Delta 1
and each \Delta i+1
and\Omega are readily computable from \Delta i
and\Omega i . Consider three
possibilities:
(1) For some i ? 1, ;. Then there is no infinite derivation from w.
(2) There is an i and an S-sequence
in\Omega i with a repeated S-expression.
Then there is an loop derivation from w.
(3) There is an i and an S-sequence
in\Omega i with an S-expression s such that
m. Then by Theorem 3.4 there is a loop derivation from s, and hence
a loop derivation from w.
If any of these possibilities occurs, then we have the answer to the question after a
finite amount of time. It remains to prove that one of them must occur. Theorem 1.1
can be used to prove that if there is no infinite derivation from w then the set of
lengths of the derivations from w has an upper bound and possibility (1) will occur. If
there is an infinite derivation from w then by Theorem 2.1 there is an infinite
S-sequence \Delta. The proof of that theorem makes it clear that w .   s 1 , and hence
w .   s i , for all i. If that S-sequence has a repeated element then possibility (2) will
occur. If not there will be no bound on the length of the elements occurring in that
S-sequence and possibility (3) will occur.2
Theorem 3.9. The derivability problem for semi-Thue systems with an inhibitor
is decidable.
Proof: We define I(w) to be the number of iotas in the word w. Where
is a derivation and be the weight of the
st step, which equals I(v), (u; v) being the rule used. Let the weighted length of
a derivation be the sum of the weights of all the steps of the derivation. Given x and y,
any derivation of y from x must have a weighted length of I(y) \Gamma I(x).
The algorithm that decides whether y is derivable from x simply enumerates all
derivations from x whose weighted length equals I(y) \Gamma I(x). Because all weights are
positive, no line z such that I(z) ? I(y) can be part of a such a derivation.
Consequently, the list of such derivations can be enumerated readily. Finally, y is
derivable from x if and only if the last line of one of these derivations is y:2
Open question 2. Does there exist an algorithm for the following problem:
Given a semi-Thue system with an inhibitor and words x and y, does x .   y hold?
4. Well behaved derivations. 2 We now turn our attention to semi-Thue
systems without an inhibitor, with an emphasis on those having only one rule. Some
derivations in these systems turn out to be like those in systems with an inhibitor.
Definition (inhibited rule, inhibition system). If u a rule of a
semi-Thue system T without ' then is an inhibited rule of T . (v 0 or v 00 can be
the null string.) The inhibition system of T is the semi-Thue system whose rules are all
the inhibited rules of T . An immediate consequence of this definition is
Theorem 4.1. If x is a finite or infinite derivation in the inhibition
system of the semi-Thue system T then, where each x 0
i is x i with all "s erased,
is a derivation in T .
Definition (well behaved, ill behaved). A derivation D in a semi-Thue
system T without ' is well behaved if there is a derivation in the inhibition system of T
from which D is the result of deleting all "s. Otherwise D is ill behaved.
From Theorems 4.1 and 3.7 we get
Theorem 4.2. There is an algorithm that produces, given a semi-Thue system
without ', either a well behaved loop derivation in the system or the information that
the system has no well behaved infinite derivation.
Example 1. The inhibition system T 0 of the system T whose one rule is (cb; bbcc)
has five rules:
This section is based on material from [13].
This system T 0 has an infinite derivation. In fact, all we need for this infinite
derivation is the one rule (cb; bb'cc). The infinite derivation is based on the following
loop of length 2:
ccb
cbb'cc
bb'ccb'cc
(In this and the examples to follow, the part of the line with an underscore is the
occurrence of u that is rewritten as the v in the next line, which has an overscore.) The
th line in the infinite derivation is (bb') st line is
Accordingly, the original system also has an infinite well behaved derivation based
on the loop
ccb
cbbcc
bbccbcc
The (2n) th line of this infinite derivation is (bb) st line is
Example 2. The following is an ill behaved derivation in the system with the one
rule (ccb; bbccc):
ccccbb
ccbbcccb
bbcccbcccb
bbcbbccccccb
bbcbbccccbbccc
bbcbbccbbcccbccc
To prove that this derivation is ill behaved we note that the inhibition system has
six rules, whose right sides are, respectively, 'bbccc, b'bccc, bb'ccc, bbc'cc, bbcc'c and
bbccc'. Thus the second line in the corresponding derivation in the inhibition system
has six possibilities. It is left to the reader to verify that in each of these six cases the '
will inhibit the replacement of the occurrence of bbc in one of the lines below. (For
example, if the ccb in the first line is rewritten as bb'ccc in the second line then the ccb
in the fifth line cannot be rewritten.)
However, the first five lines of the above derivation form a well behaved derivation,
which can be verified by considering the following derivation in the inhibition system:
ccccbb
ccbbc'ccb
'bbcccbc'ccb
'bbc'bbcccc'ccb
If all the "s from this derivation are deleted, what remains are the first five lines of the
above derivation in T .
The system with the single rule (ccb; bbccc) has an infinite derivation. This is clear
from the first five lines of our original derivation, which shows that
ccccbb . 4 ccccbb
The sixth line of this infinite derivation is the sixth line of the above derivation, which
shows that the infinite derivation is ill behaved.
This system has no infinite well behaved derivation. To verify this fact we can
refer to Theorem 4.1 and prove that its inhibition system has no infinite derivation.
Using the algorithm of Section 3 for this is tedious as it involves enumerating S.
Rather than do this we work with a superset of S:
3g. Then it is rather simple to verify using
Theorem 2.4 that
Thus in particular, for each rule (u; v), we have L(v); R(v) 2 T .
Next, we use Theorem 2.7 to verify that
by proving the following:
(1) every v-section is in TT ;
(3) (since all rules have the same left side ccb) if
then R(v)z 2 TT (for all right sides v);
Parts (1) and (2) are clear. For (3), yz 2 S L implies
. The reasoning for (4) is similar to the
reasoning for (3).
For (5), implies that . So, a fortiori,
From L(v); R(v) 2 T , we then get xL(v); R(v)y 2 TT .
Having proved that S ' TT , we complete the proof that the inhibition system has
no infinite derivation by proving there is no infinite S sequence (invoking Theorem 2.1).
The proof is by contradiction. Assume there is an infinite S sequence s
Then each s h
with
and 3. The following must be true:
nothing could be derived from u h .)
(b) Either
(Part (b) can be verified by first noting that there is only one occurrence of ccb in
. Thus if s h ! w, then w is a word resulting from
by placing a single ' anywhere between the two slashes and deleting the slashes; s h+1 is
either the word to the left of the ' or the word to its right. If to the left then
if to the right, k
Taking
, we have by (b) either
1. By (a) again we get k
Where s
we get by (b) either j
By (a) we then see that s 4 cannot exist, which completes our proof that the system
with the one rule (ccb; bbccc) has no infinite well behaved derivation, ending our
discussion of Example 2.
In [13] there is a much more expeditious algorithm for the problem of whether a
given one-rule semi-Thue system has an infinite well behaved derivation. That
algorithm does not require consideration of the inhibition system of the given system,
but involves a structural analysis of one-rule systems that is well outside the purview of
this paper. That structural analysis having been established, the proof in [13] that the
one-rule system (ccb; bbccc) has no infinite well behaved derivation takes one quarter
the space used in the proof given above as part of Example 2.
This example generalizes. Zantema and Geser [16] prove that a system with one
rule
in which (1) p ? n, either p is a multiple of n or q is a multiple of
m, has an infinite derivation. In [13] it is proved that such a system in which either
2m has no infinite well behaved derivation. From these two results it
follows that any such system in which either 2n in for i an integer - 2
or else 2m ? in has an infinite ill behaved derivation but no infinite well
behaved derivation.
Interestingly, it is proved in [16] that there is no infinite derivation at all in any of
the following cases: (1) q is not a multiple of m,
is not a multiple of n. (Senizergues [14] has extended these results of
Zantema and Geser.) In [13] it is proved that if p - 2n and q - 2m then there is an
infinite well behaved derivation.
The two examples discussed in this section illustrate the distinction between well
behaved infinite derivations and ill behaved infinite derivations. They are intended to
suggest the importance of this distinction to the question of whether the uniform
halting problem for one-rule semi-Thue systems is decidable. It is generally conjectured
that this problem is decidable, and some progress has been made in proving partial
results along that line. However, the question of whether the uniform halting problem
for one-rule semi-Thue systems is decidable is very much open. It seems to me that
further progress on this question will come only if research workers achieve a structural
understanding of ill behaved derivations. There are partial results towards this end
in [13].
The halting problem is open for one-rule semi-Thue systems. On the other hand,
we have
Theorem 4.3. The derivability problem is decidable for one-rule semi-Thue
systems.
Proof: Given x, y and a semi-Thue system whose one rule is (u; v) our algorithm
to determine whether y is derivable from x divides into three cases according to the
relative lengths of u and v: Case I, jvj ! juj; Case II,
In Cases I and II, the finite set of words derivable from x can be enumerated, and the
presence or absence of y in the set easily determined. In Case III, the finite set of
words derivable from x whose length does not exceed that of y can be enumerated,
again yielding an answer to the question.2
For a class of semi-Thue systems an interesting question is, does every semi-Thue
system in the class with an infinite derivation have a loop derivation? The result in
Section 3 shows that this question has an affirmative answer for the class of semi-Thue
systems with an inhibitor. When restricted to well behaved infinite derivations, it has
an affirmative answer for all semi-Thue systems. However, it is an open question for
one-rule semi-Thue systems; which means that the question restricted to infinite ill
behaved derivations is open for one-rule semi-Thue systems.



--R


"Three models for the description of language,"

"Termination of rewriting,"
"Topics in termination,"
"On proving uniform termination and restricted termination of rewriting systems,"
Monographs on Theoretical Computer Science
Termination und Konfluenz von Semi-Thue-systemen mit nur einer Regel
"Explanations to the text,"
"One-rule semi-Thue systems with loops of length one, two or three,"
"The development of formal language theory since 1956,"
"The uniform halting problem for one-rule semi-Thue systems: progress report,"
"Well behaved derivations in one-rule semi-Thue systems,"
''On the termination problem for one-rule semi-Thue systems"
"Probleme - uber Ver-anderungen von Zeichenreihen nach gegeben Regeln,"
"A complete characterization of termination of 0
--TR

--CTR
Matiyasevich , Graud Snizergues, Decision problems for semi-Thue systems with a few rules, Theoretical Computer Science, v.330 n.1, p.145-169, 31 January 2005

extracted:['file assignment' 'feedback controls' 'feature weights'
 'fault-tolerant software systems' 'fault-tolerant routing algorithm'
 'fault-tolerant routing' 'fault-tolerant algorithms' 'flow analysis'
 'zero storage biometric authentication' 'computational complexity']
marked:['termination', 'semi-Thue system', 'string rewriting', 'uniform termination', 'well behaved derivation']
--T
Efficient Algorithms to Detect and Restore Minimality, an Extension of the Regular Restriction of Resolution.
--A
A given binary resolution proof, represented as a binary tree, is said to be i>minimal if the resolutions cannot be reordered to generate an irregular proof. Minimality extends Tseitin"s regularity restriction and still retains completeness. A linear-time algorithm is introduced to decide whether a given proof is minimal. This algorithm can be used by a deduction system that avoids redundancy by retaining only minimal proofs and thus lessens its reliance on subsumption, a more general but more expensive technique.Any irregular binary resolution tree is made strictly smaller by an operation called i>Surgery, which runs in time linear in the size of the tree. After surgery the result proved by the new tree is nonstrictly more general than the original result and has fewer violations of the regular restriction. Furthermore, any nonminimal tree can be made irregular in linear time by an operation called i>Splay. Thus a combination of splaying and surgery efficiently reduces a nonminimal tree to a minimal one.Finally, a close correspondence between clause trees, recently introduced by the authors, and binary resolution trees is established. In that sense this work provides the first linear-time algorithms that detect minimality and perform surgery on clause trees.
--B
Introduction
The regular restriction of binary resolution [14] states that a resolution
step resolving on a given literal should not be used to deduce a clause
containing that literal. In other words, that resolution step should not
be an ancestor of such a clause in the binary derivation tree. We extend
this restriction so that it applies also when a reordering of the resolutions
brings such a clause below that step. We use rotations of edges
in the binary tree to reorder the resolution steps, and require that the
rotations neither weaken what is proved nor increase the size of the
tree. If a binary resolution proof cannot be made irregular by such
rotations, we call it minimal.
This extension of regularity depends on sequences of rotations, and
thus appears to be expensive to compute. However we characterize it
with a condition that can be checked efficiently by examining the static
tree. The condition is stated in terms of history paths in the binary
resolution tree. Each history path tells the story of a given literal. The
tail of the history path is a leaf of the tree and it tells where the
literal was introduced in an input clause. The history path is said to
close at the node where its literal is resolved away. When one history
path closes at a node that occurs in another history path, and the two
paths are disjoint, we say the first history path directly precedes the
other. The precedes relation on history paths is the reflexive transitive
closure of directly precedes. History paths and the precedes relation are
basic in our understanding of how binary resolution trees behave when
rotations are performed. We provide simple conditions on history paths
that characterize when one node can be rotated below another (we say
it is visible from the other) and when one node cannot be rotated from
below another (we then say it supports the other.) By examining the
history paths in a static tree, we can decide much about what can and
cannot be accomplished by sequences of rotations.
In particular, we can say whether rotations can convert a regular tree
to an irregular one. A theorem prover can use this ability to screen the
proofs it builds, and retain only the minimal ones. Since every clause
with a non-minimal proof is subsumed by some clause with a minimal
proof, this theorem prover uses minimality to lessen its reliance on sub-
sumption. The runtime of full subsumption grows with the number of
retained clauses, which may become very large, while detecting minimality
depends linearly on the size of each proof tree, which is typically
much smaller.
Instead of using minimality simply as a filter, a theorem prover can
convert a non-minimal proof to a minimal one, using operations defined
in this paper. Consider the branch of an irregular derivation tree that
makes it irregular. This branch contains the node where some given
literal is resolved and, further down, contains a clause in which that
literal occurs. Why resolve the literal away, only to have it reappear
later? We can remove this resolution but otherwise reconstruct the
branch as closely as possible to the original, with this literal appearing
additionally in some of the clauses. The constructed tree is smaller
and the result it proves is at least as general as the original one. We
call this operation surgery, and define a second operation, splay, which
rearranges a non-minimal tree so that it is irregular. Both operations
run in time linear in the size of the tree. A combination of both operations
will eventually reduce any non-minimal tree to some minimal
one.
The first section below presents the regular restriction on binary
resolution trees. In that section we introduce the surgery operation for
irregular binary resolution trees. Then minimal binary resolution trees
are introduced. This section also discusses our rotations and the set of
rotation equivalent trees, which is all of the trees that can be generated
from a given tree by a sequence of these rotations. In the following sec-
tion, we discuss the "precedes" relation of history paths, and use that to
define the "holds" relation on nodes. We then relate holds and visibility
by showing that if the nearest common descendant of two given nodes
holds one of them, then that one is not seen by the other, i.e. it cannot
be rotated below the other. Based on this condition, we give an efficient
algorithm for deciding visibility. We show how a theorem prover can
be restricted to retain only minimal proofs and disregard non-minimal
ones, decreasing the number of proofs that must be considered. Then
we show the splay operation for efficiently converting a non-minimal
proof into an irregular one, so that surgery can further convert it to a
minimal one. Thus another theorem prover can efficiently convert its
non-minimal results to minimal ones if desired. In the next section we
use history paths to characterize support: the condition where one node
in a binary resolution tree must be a descendant of another after any
sequence of rotations.
Both minimality and surgery were first developed for clause trees [5].
The clause tree is a tool for developing ideas in automated reasoning,
and the binary resolution tree is an efficient, compact data structure
to implement clause trees. In the second last section we show the close
relation between binary resolution trees and clause trees. Thus, this
paper provides the first efficient algorithm for surgery in clause trees.
We close with some remarks on the relation between clause trees and
binary resolution trees, and related work.
This paper is an extension of [13].
2. Binary Resolution Trees
We use standard definitions [2] for atom, literal, substitution, unifier
and most general unifier. In the following a clause is an unordered disjunction
of literals. We do not use set notation because we do not want
multiple occurrences of a literal to collapse to a single literal automat-
ically. Thus our clauses can be viewed as multisets. An atom a occurs
in a clause C if either a or :a is one of the disjuncts of the clause.
The clause C subsumes the clause D if there exists a substitution '
such that C' ' D (as sets, not as multisets). A variable renaming
substitution is one in which every replacement of a variable maps to
another variable, and no two variables map to the same variable. Two
clauses C and D are equal up to variable renaming if there exists a
variable renaming substitution ' such that Two clauses are
standardized apart if no variable occurs in both. Given two parent clauses
which are standardized
apart (a variable renaming substitution may be required) their resolvent
is the clause (C 1 - C 2 )' where ' is the most general unifier of
g. The atom resolved upon is a 1 ', and the set of
resolved literals is g.
It is convenient to define a mapping ae of literals for the resolution
operation. This is used later to define history paths.
(Resolution mapping). For each resolution operation we
define the resolution mapping ae from each occurrence of a literal c in
each parent clause to either the atom resolved upon if c is a resolved
literal, or otherwise to the occurrence of c' in the resolvent.
The reader may be missing the usual factoring operation on a clause,
which consists of applying a substitution that unifies two of its literals
with the same sign and then removing one of these literals. This
operation is not needed in binary resolution trees, however, since if a
clause contains two identical or unifiable literals, both can be resolved
upon whenever the clause is used in a resolution. By allowing several
literals to be resolved on, instead of merging them before the resolu-
tion, we have just one type of internal node in our binary resolution
tree, instead of two. (De Nivelle uses resolution nodes and factorization
nodes [3].) Moreover, an implementation is free to merge or factor
literals if desired. Factoring may be seen as an optimization if the factored
clause can be used in several resolution steps, since the factoring
is done only once.
A binary resolution derivation is commonly represented by a binary
tree, drawn with its root at the bottom. Each edge joins a parent node,
drawn above the edge, to a child node, drawn below it. The ancestors
(descendants) of a node are defined by the reflexive, transitive closure of
the parent (child) relation. The proper ancestors (proper descendants)
of a node are those ancestors (descendants) not equal to the node itself.
If T and H are nodes in a tree then path(T ; H) is the unique path from
T to H . Here, T is called the tail and H the head.
Definition 2. A binary resolution tree on a set S of input clauses is
a labeled binary tree. Each node N in the tree is labeled by a clause
label, denoted cl(N ). Each node either has two parents and then its
clause label is the result of a resolution operation on the clause labels
of the parents, or has no parents and is labeled by an instance of an
input clause from S. In the case of a resolution, the atom resolved upon
is used as another label of the node: the atom label, denoted al(N ).
Any substitution generated by resolution is applied to all labels of the
tree. The clause label of the root of the binary resolution tree is called
the result of the tree, result(T ). A binary resolution tree is closed if its
result is the empty clause, 2.
a-d
a:b-d-e
d:b-c-e
e:b-c-f-g
c:a- b-b-f-g
a: b-b-f-g
b:f-g
-a-b-e
c-d
e-f-g
a-b-c
h:-a
-b
-g
-a-h
-h

Figure

1. An irregular binary resolution tree.
For the binary resolution tree in Figure 1,
:gg. The labels of a node N are
displayed beside the name of the node and separated by a colon, e.g.
the node N 4 has atom label c, and clause label a - b - b - f - g. The
order between the parents of a node is not defined.
If instead of labelling the internal nodes by atoms, one labels each
edge with the complement of the literal resolved upon, a binary resolution
tree would become a semantic tree, and the leaves of the binary
resolution tree would become failure nodes of the semantic tree.
Using the resolution mapping ae for each resolution operation in the
tree, we can trace what happens to a literal from its occurrence in the
clause label of some leaf, down through the tree until it is resolved
away. Clearly if all literals are eventually mapped to the atom label
of some internal node, the clause label of the root is empty. In this
case by soundness of resolution, the clause labels of the leaves is an
b:
a-d
d:a-c
c:a-a-b
a:b
c-d
a-b-c
-b
-a-h
-h

Figure

2. Surgery: Operation 5 on Figure 1.
unsatisfiable set. Thus we are primarily concerned about tracing the
"history" of a literal starting from its appearance in a leaf.
Definition 3 (History Path). Let the nodes (N occur in a
binary resolution tree T such that N 0 is a leaf whose clause label contains
a literal a, and for each is a parent of N i . Let
ae i be the resolution mapping from the parents of N i to N i , and let
occur in cl(N i ). Suppose N n either is the root of T , or has
a child N such that ae a is mapped by the resolution at N to the
atom resolved upon at N . Then (N is a history path for a.
The history path is said to close at N if N exists, and then N is written
as close(P ). The node N n is the head, the leaf N 0 is the tail and a is
the literal of P , written head(P ); tail(P ) and literal(P ), respectively.
For example in Figure 1, (M for c which
closes at N 4 . The two history paths for b in Figure 1, corresponding to
the two occurrences of b, are (M 3
Both of these close at N 6 . The only which does not close is the one for
f , which is
Definition 4 (Regular). A binary resolution tree T is regular if there
does not exist a node N of T and a descendant M of N , such that
al(N) occurs in cl(M ).
The tree in Figure 1 is irregular because al(N 1 ) is a and a occurs
in cl(N 4 ). Irregular trees are never necessary. Why resolve away the a
twice? One could choose to leave out the resolution at N 1 , leaving the
a in the clause, do the other resolutions as necessary (not all will be
necessary) and later resolve a away, as was done at N 5 . Operation 5
makes this idea more formal.
A new binary resolution tree T 0 is constructed from T in which M 0
and all of its ancestors, and possibly other M i are removed. However
all leaves of T 0 are also leaves of T . Thus every history path in T 0
has a corresponding history path in T . The converse cannot be true,
since the history paths of T through M 0 do not exist in T 0 . A new
sequence (N 0
n ) is defined in which either N 0
M i is removed) or N 0
i corresponds to N i in T .
Operation 5 (Surgery on irregular trees). Let T be an irregular binary
resolution tree with being the path of
nodes from N 1 to the root N n , and with being the first
node in this path whose clause label contains a. Let N i\Gamma1 and M i\Gamma1 be
the parents of N i for be the parents of
so that a occurs in the clause label of N 0 with the same sign as in
cl(N
Procedure
all ancestors of N 0 be in T 0 . (M 0 and all of its
ancestors are removed.) Let N k be the node at which some history
path P a for a in N j closes, if k exists.
For do
If there is a history path in T 0 containing N 0
for which the corresponding
history path in T closes at N i then
Put M i\Gamma1 and all its ancestors into T 0 .
i as a new node in T 0 which is the child of N 0
Let the resolution at N 0
i be defined so that the history paths of
i that close at N 0
i correspond to history paths of T that close at
Let the history path corresponding to P a also close at N 0
k .
(That is, the same literals as far as possible are resolved at N 0
i as
at N i with one possible addition.)
else
There is no history path containing N 0
which corresponds To a
path that closes at N i .
. (M i\Gamma1 and all of its ancestors are removed.)
endif
endfor
Note that if the occurrence of a in N j is never resolved away, i.e. its
history path continues to the root, then the two literals corresponding
to a may occur in the root of T 0 . However, they are both at least as
general as a.

Figure

2 shows the effect of surgery on Figure 1. Surgery is performed
at using N 4 as N j . M are not needed in T 0 . By insisting
that both occurrences of a are closed at the same node, we ensure that
does not have a in its result when T does not. Thus N 0
5 closes both
history paths for a from N 0
Theorem 6. Let T be an irregular binary resolution tree on a set
S of clauses and T 0 is constructed by Operation 5. Then T 0 is also a
binary resolution tree on S, T 0 is smaller than T and the result of T 0
subsumes the result of T .
Proof. We use the following lemma: If clauses C 1 and C resolve to give
clause C 2 subsumes C 1 then either the resolution of C 2 and
C is not possible and C 2 subsumes R 1 , or it is possible and its result,
This assumes that all literals from C resolved in the
first resolution are also resolved in the second, and furthermore that
literals from C 2 are resolved in the second resolution if they correspond
(in the subsumption) to literals from C 1 resolved in the first resolution.
Each leaf in T 0 has the same label as a leaf in T and therefore T 0
is defined on S. Also, each internal node is defined by a resolution of
its parents, so T 0 is a binary resolution tree. Note that cl(N 0
subsumes cl(N 1 ) - a because cl(N 1 ) contains all the literals in cl(N 0 )
except possibly a. Using repeated applications of the lemma, it follows
that cl(N 0
cl(N j ) since a occurs in cl(N j ). Then cl(N 0
the result of T 0 subsumes that of T . Since M 0 is not in
T 0 and since all other nodes in T 0 are taken at most once from T , it
follows that T 0 has fewer nodes than T . 2
Theorem 7 (Completeness [14]). If S is unsatisfiable there exists a
closed regular binary resolution tree on S. Furthermore the smallest
closed binary resolution tree is regular.
Proof. If S is unsatisfiable, there exists a closed binary resolution tree
[11]. If it is irregular, apply Operation 5 repeatedly until it is regular.
This process must terminate since the tree is smaller at each step.
If the smallest closed binary resolution tree is not regular, surgery
can be applied to it, making a smaller closed tree. 2

Figure

3. A binary tree rotation
3. Minimal Binary Resolution Trees
A rotation of an edge in a binary tree is a common operation, for
example with AVL trees [1]. Before we apply it to binary resolution
trees, we review the operation on binary trees. Given the binary tree
fragment on the left of Figure 3, a rotation is the reassignment of edges
so that the tree on the right of Figure 3 is produced. The parent C of
becomes the child of E and the parent B of C becomes the parent
of E. If E has a child in T , then C takes that child in T 0 . In other
words, the edges (B; C), (C; E) and (E; F ) if it exists, are replaced by
the edges (B; E), (E; C) and (C; F ) if necessary.
Operation 8 (Edge Rotation). Let T be a binary resolution tree with
an edge (C; E) between internal nodes such that C is the parent of E
and C has two parents A and B. Further, suppose that no history path
through A closes at E. Then the result of a rotation on this edge is the
binary resolution tree T 0 defined by resolving cl(B) and cl(D) on al(E)
giving cl(E) in T 0 and then resolving cl(E) with cl(A) on al(C) giving
cl(C) in T 0 . Any history path closed at C in T is closed at C in T
similarly any history path closed at E in T is closed at E in T 0 . Also,
the child of E in T , if it exists, is the child of C in T 0 .
A rotation may introduce tautologies to clause labels of internal
nodes. For instance, if al(C) occurs in cl(D) then cl(E) in T 0 may
be tautological. However the clause label of the root is not changed
(Corollary 11). We prove a slightly more general result first, which is
also used later.
Definition 9. Let T 1 and T 2 be two binary resolution trees defined
on the same set of input clauses. Then T 1 and T 2 close history paths
similarly if there is a one-to-one and onto mapping - from nodes in T 1
to those in T 2 , such that:
1. If N is a leaf then -(N) is a leaf and both are labeled with instances
of the same input clause. Thus there is a natural one to one cor-
respondence, from literals in cl(N) to those in cl(-(N )). Moreover
this mapping of literals provides a mapping from history paths in
T 1 to those in T 2 , defined so that they start from the same literal
in the input clause, up to variable renaming. We represent these
other two mappings also with -. We require for all history paths P
in T 1 that tail(-(P
up to variable renaming.
2. For every history path P of T 1 , P closes at a node N if and only if
closes at -(N ).
Thus two binary resolution trees close history paths similarly if they
resolve the same literals against each other, albeit in a possibly different
order.
Lemma 10. If two binary resolution trees T 1 and T 2 close history
paths similarly, the result of T 1 and the result of T 2 are the same, up
to variable renaming.
Proof. Note that result(T 1 ) and result(T 2 ) are composed entirely of
literals from history paths that do not close, and since the same history
paths are closed in each, the same literals are not resolved away. Also
the composition of mgus in T 1 and that in T 2 are unique up to variable
renaming since, given a node N , the same literals are unified at N and
-(N ), up to variable renaming. 2
Corollary 11. Given a binary resolution tree T with an internal node
C and its child E, Operation 8 generates a new binary resolution tree
and cl(E) up to variable renaming.
Proof. Observe that Operation 8 produces a tree which closes history
paths similarly. 2
A rotation changes the order of two resolutions in the tree. Rotations
are invertible; after a rotation, no history path through D closes at
C, so another rotation at (E; C) can be done, which generates the
original tree again. We say that two binary resolution trees are rotation
equivalent if one can be generated from the other by a sequence of
rotations. For instance, the first binary resolution tree in Figure 4 is
produced by rotating the edge (N 4 ; N 5 ) in Figure 1. The second tree

Figure

4. From Figure 1 rotate
in

Figure

4 is then produced by rotating the edge (M 4 ; N 5 ). Thus both
trees are rotation equivalent to Figure 1. Rotation equivalent is an
equivalence relation. It is not surprising that rotation equivalent binary
resolution trees must close history paths similarly, but the converse is
true as well.
Theorem 12. Two binary resolution trees T 1 and T 2 are rotation
equivalent if and only if they close history paths similarly.
Proof. Since one rotation of T 1 creates a binary resolution tree that
closes history paths similarly to it, so too does the sequence of rotations
creating T 2 .
The converse is proved by induction on the number of internal nodes.
Suppose T 1 and T 2 close history paths similarly. Then they must have
the same number n of internal nodes since they have the same number
of leaves. If no rotation is possible and the theorem
holds. Let N be a node in T 1 with parents L 1 and L 2 that are leaves.
Then in T 2 , -(N) has proper ancestors -(L 1 ) and -(L 2 ) which also are
leaves, and -(N) closes only history paths with tails -(L 1 ) and -(L 2 ).
We create T 0
2 by rotating edges so that -(L 1 ) and -(L 2 ) are parents of
-(N ), if this is not already the case. Let C be either parent of -(N) and
let A and B be the parents of C. If -(L 1 ) and -(L 2 ) are both ancestors
of C then neither is an ancestor of the other parent of -(N ). But -(N)
must close a history path from that other parent, contradiction. Thus
the edge (C; -(N)) can be rotated, since not both A and B contain a
history path closing at -(N ). This rotation reduces the total number
of non-leaf ancestors of -(N ). After a finite number of such rotations,
both parents of -(N) are leaves. Call this tree T 0
.
Let T
1 be T 1 with leaves L 1 and L 2 deleted, and let T
2 be T 0
2 with
leaves
close history paths
similarly since T 1 and T 0
close history paths similarly. By induction T  and T
are rotation equivalent. The sequence of rotations to convert
1 to T
will also convert T 1 to T 0
2 which is rotation equivalent to T 2 .We focus on the set of rotation equivalent trees that do not contain
an irregular binary resolution tree. Any tree in this set is said to be
minimal, since surgery cannot be applied to make it smaller.
Definition 13. A binary resolution tree T is minimal if no sequence
of rotations of edges generates a tree T 0 that is irregular.
Theorem 14. If a binary resolution tree T on S is non-minimal, there
exists a minimal binary resolution tree T 0 on S which is smaller than
T and the result of T 0 subsumes the result of T .
Proof. If T is not minimal, apply Operation 8 and Operation 5 so that a
regular tree is produced. If this tree is minimal then let T 0 be this tree.
Otherwise repeat from the beginning until T 0 is defined. This process
must terminate because the tree is getting smaller at each application
of Operation 5. Also the old result is subsumed by the new result at
each step. 2
Thus a smallest binary resolution tree is minimal. Goerdt has shown
[4] that a smallest regular binary resolution directed acyclic graph
(DAG) may be exponentially larger than an irregular binary resolution
DAG. Thus by considering only regular or minimal binary resolution
DAGs, a theorem prover may not find the smallest proof, and hence
may require more inferences to prove a given theorem. However in most
cases we believe that the space of minimal binary resolution DAGs is
much smaller than the space of all binary resolution DAGs and that
considerable time can be saved by restricting to minimal ones. (Similar
arguments can be made for and against most restrictions of resolution,
including set-of-support and hyperresolution.)
4. Checking Minimality
Determining whether a given binary resolution tree is minimal seems
to be labourious, since the straightforward application of the definition,
as is done in the proof of Theorem 14, checks every possible sequence
of rotations, and there can be exponentially many. In this section we
give an efficient algorithm for determining visibility - which nodes can
be rotated below which - so deciding minimality is efficient.
Definition 15 (Visible). In a given binary resolution tree with internal
nodes N and M , we say that M is visible from N , and that N
can see M , if there exists a sequence of rotations such that M is a
descendant of N . Otherwise M is invisible from N .
Thus a node can see the nodes that can be rotated below it. Although
this is a property defined in terms of rotations, it is possible to inspect a
static tree, without doing any rotations, to determine visibility. Because
of this, visibility can be computed in linear time. That static property,
holds, is defined after one more concept, precedes.
Definition directly precedes a history
path Q if P and Q have no nodes in common, and P closes at
some node in Q. We write P OE Q. Moreover we say P precedes Q, and
there is a sequence of history paths (P
directly precedes P i+1 for
A history path P precedes a node N if N closes some history path Q
and P OE   Q.
The precedes relation is the reflexive and transitive closure of directly
precedes. In particular a history path precedes itself, even though
it does not directly precede itself. Also note that precedes defines a
partial order on the set of history paths.
In most cases a rotation does not change the precedes relation on
history paths.
Lemma 17. Let the history path P precede the history path Q in
the binary resolution tree T and let P be the images of
and T respectively after a rotation of the edge (C; E) as in Definition 8.
Further suppose that the head of Q is not C. Then P 0 precedes Q 0 in
Proof. Let and TD be the subtrees of T rooted at A; B and
C respectively. If close(P ) and head(Q) are in the same one of these
subtrees, the rotation has no effect on whether P ! Q. We know that
head(Q) is a proper descendant of close(P ). Also head(Q) is not C so
head(Q) must be a descendant of E.
Now if tail(P ) and tail(Q) are in the same subtree,
then the rotation does not affect P OE   Q because the node of Q which
P precedes is also in that subtree. Let P 1 be the path closing at that
node. Thus P OE   P 1 OE Q. We have these cases, which are illustrated
in

Figure

5.
1. tail(Q) is in . If tail(P ) is in closes at C. After the
rotation,
1 , which is one node longer, closes at C in T 0 . P 0 OE
Otherwise tail(P ) is in TD . Let P 0 denote the path through B
closing at C. After the rotation P 0 OE   P 0
2. tail(Q) is in TB . If tail(P ) is in after the rotation P 0 OE
Otherwise if tail(P ) is in TD then after the rotation, P 0 OE   P 0
3. tail(Q) is in TD . Since no path through A closes at E, we know
tail(P ) is not in . Thus tail(P ) is in TB . After the rotation,
directly hold an
internal node M of a binary resolution tree if M is the first node that
occurs on both P 1 and Q 1 , (i.e. the parent of M does not occur on
both.) We say that P and Q hold M if there exist history paths P 1 and
directly hold M . A
node N holds M if P and Q hold M and both P and Q close at N .
Also, if P and Q hold M and are in a set of history paths, we say
that the set holds M .
The following theorem relates invisible, a property that depends on
all rotation equivalent trees, to held, a property that can be checked
by examining just the one tree of interest.

Figure

5. Cases of Lemma 17
Theorem 19. The nearest common descendant of M and N holds M
if and only if M is invisible from N .
Proof. ()) We show that if the nearest common descendant of M and
holds M , then after a rotation, the (possibly new) nearest common
descendant of M and N holds M . Thus M can never be a descendant of
N for if it were then the nearest common descendant would be M , and
a node cannot hold itself. Cases in the proof of the forward direction
are in

Figure

6.
Let F be the nearest common descendant of N and M , and let the
rotated edge be (C; E), and let nodes A; B and D adjacent to it be as
defined in Operation 8. Let P and Q hold M and close at F , while P 1
and directly hold M , and P 1 OE   P and Q 1 OE   Q. Consider the case
where F 6= E, so that after the rotation F is still the nearest common
descendant of M and N . By Lemma 17, P 0
Suppose M 6= E. Then after the rotation, M is still the first common
node on P 0
1 , so F still holds M . Now suppose that
Without loss of generality assume that P 1 contains C and Q 1 contains
D. (Case 1a) If P 1 contains B then after the rotation, P 0
hold E, so F holds M . (Case 1b) If P 1 contains A then consider the
path and closing at C. After the rotation, P 0
that P 0
hold E, so again F holds M .
Now suppose that Consider the case (Case 2) where M is
an ancestor of C so that N is either E or an ancestor of D. Since no
history path can contain A and C and close at E, M 6= C. For the
same reason, P and Q contain B and close at E. (Case 2a) If M is an
ancestor of A then the paths that directly precede P and Q close at C
and hold M . Thus C holds M . After the rotation the nearest common
descendant of M and N is C, and C still holds M . (Case 2b) Otherwise
if M is an ancestor of B then after the rotation the nearest common
descendant of M and N is E and E still holds M .
Finally consider the case (Case is an ancestor of D.
(Case 3a) If N is either E or an ancestor of B then after the rotation,
holds M and is still the nearest common descendant of N and
M . (Case 3b) If N is C or is an ancestor of A, then consider path
R with head at B which closes at C. After the rotation, the nearest
common descendant of M and N is C, while R 0 directly precedes both
after the rotation.
Conversely, suppose that M is not held by the nearest common
descendant F of M and N . If then M is a descendant of N
and therefore visible from M . If F is the child of M then the edge
can be rotated, making M a descendant of F and therefore of
N . Assume that there exists a path (N
A
A
A
A
Case
3a
Case
R'
R
Q'
Q'
A
A
Q'
Case
A
A
A
Case
1a
Case
R
R'
A
A
Case
2a

Figure

6. Cases of Theorem 19
so that N i\Gamma1 and K i\Gamma1 are the parents of N i , for
use induction on n. Without loss of generality N 0 is
chosen so that al(N 2 ) occurs in cl(N 0 ); thus a history path through N 0
closes at N 2 . If there exists i in ng such that no history path
through N i\Gamma2 closes at N i then the edge (N can be rotated,
shown as Rotation 1 in Figure 7. We say that N i has been rotated
to the side of the path(M; F ), and so M is now closer to F and the
theorem holds by induction.
Thus for each i in path through N i\Gamma2 closes
at N i . If there is no j in ng such that N j is held by F then
in particular N n\Gamma1 is not held so the edge (N can be rotated.
But then the distance from M to the nearest common descendant of
M and N is and the theorem holds by induction. Choose
the smallest j in ng so that N j is held by F . Note that j 6= 1 by
assumption. If there is a history path through K j \Gamma2 closing at N j then
this path and the path through N j \Gamma2 closing at N j directly hold
Thus held by F , contradicting the definition of j. Therefore no
such path through K j \Gamma2 exists and the edge can be rotated as
illustrated in Rotation 2 of Figure 7. If is closer to F
after the rotation; we say that N j has been rotated off the top of the
path(M; F ), and the theorem holds by induction. Otherwise consider
the history paths through N j \Gamma2 closing at N j . If one of these includes
another includes K j \Gamma3 then N j \Gamma2 is held by F after the
rotation. This means that N j \Gamma2 was held by F before the rotation,
contradicting the definition of j. Thus the edge N j \Gamma2 N j can be rotated.
The result of this rotation is that N j will be rotated either to the side of
the path(M; F ), or closer to M . By a second induction on the distance
from M to N j , N i will eventually be rotated off the top or to the
side. Thus the distance from M to F will decrease by the constructed
sequence of rotations. 2
The sequence of rotations, constructed in the proof of Theorem 19
rotates the visible node M to below the node N that can see it. Thus
if the tree were regular but non-minimal, the rotations would make
it irregular, exposing the non-minimal parts. Using surgery, we can
remove all non-minimal sections of a tree, and reconstruct a minimal
tree from the pieces left behind.
Unfortunately the number of rotations required to expose a non-
minimality may be quadratic in the length of the path in the tree.
Since the trees are not balanced, this is quadratic in the size of the tree
in the worst case. Later we give a linear time operation, called splaying,
to bring a visible node to a descendant. In the next section, however,
we detect and avoid non-minimal trees.
Rotation 1: to the side
F
F
Rotation 2: off the top
F
F

Figure

7. Rotations in the proof of Theorem 19
5. Minimality as a restriction
Now we turn our attention to a theorem prover that keeps only minimal
binary resolution trees. Since every non-minimal tree is subsumed by
some minimal tree, this strategy uses the minimal criteria to reduce
redundancy, lessening reliance on subsumption.
Definition 20. Let T be a binary resolution tree. Then atoms(T
fal(N)jN is a node of Tgis called the set of atoms of T . A subbrt of T
is a binary resolution tree whose node set consists of a node of T called
the root of the subbrt, and all of its ancestors. For a subbrt T 0 of T ,
is a node of T 0 and visible from the root of Tg is
called the set of visible atoms of T 0 .
Theorem 21. Let binary resolution tree T consist of a root node R
and two subbrt's T 1 and T 2 . T is minimal if and only if
1. T 1 and T 2 are minimal,
2. no atom in cl(R) is in atoms(T ),
3.
4.
Proof. Assume that T is minimal. If T 1 or T 2 were not minimal, then
there would be a sequence of edge-rotations which would make the
subbrt irregular. The same sequence performed on T would make T
irregular as well. Hence the first condition is true. If the second condition
were false, then T would be irregular immediately. Assume that the
third condition is false. Then there are two nodes,
whose atom labels are the same, and M is visible from R. Hence M
can be rotated below R, without rotating any edges in T 2 . Now M
is the descendant of N , making T irregular. The fourth condition is
symmetric to the third.
Conversely, assume that T is not minimal. Then there is a sequence
of rotations that create an irregular tree T 0 . Node N has a descendant
M 1 in T 0 such that al(N) occurs in cl(M 1 ). Since the rotations do not
change cl(R), if al(N) occurs in the result of T 0 , it occurs in cl(R) in T
and then T violates the second condition. Thus al(N) does not occur in
cl(R) so there is a descendant M of N in T such that
If M and N are in the same T i then T i violates the first condition.
Assume M and N are in different T i . Since M has been rotated below
is visible from N in T , and by Theorem 19 M is not held by the
nearest common descendant R of M and N . Thus M is visible from R.
Therefore al(M) is in vis(T i ) while al(N) is in atoms(T 3\Gammai ). 2
Any theorem prover based on binary resolution that keeps only minimal
trees has already satisfied the first condition in Theorem 21 for
any newly constructed tree, since only minimal trees are used in the
construction. It is easy to check that the new result does not contain
an atom in atoms(T ). What is left is to find is an easy way to calculate
those atoms in each subbrt T i which are visible from the root of T .
The idea in Procedure 22 is that a node is visible in a subbrt T i if
and only if it is not held by the root of T , by Theorem 19. So for each
node N we need to calculate the history paths going through it that
precede the root. If some of these paths go through one parent of N ,
and some go through the other, then N is held by the root; otherwise
N is visible from the root.
In each procedure call visible(N; PN ), the first argument N is a
node in the tree and we want to know whether or not it is held by the
root. Initially it is one of the parents of the root, but as we traverse
upward it becomes instantiated with each ancestor of this parent. The
invariant we maintain is that the second argument, PN , is the set of
paths that include N and precede the root. The paths through the
parent of the root that precede the root are simply those whose heads
are that parent, so the invariant is easy to establish in the first place.
Suppose N has parents A and B. To calculate the paths PB through B
that precede the root, we start with PN , and remove those paths that
go through A if there are any. If there are none then PB is PN . If there
is some path in PN through A, we need to add to PB any path with
head at B, since these paths precede that path through A, and thus
precede the root.
Since we know that the paths in PN must go through at least one
of the parents of N , we assume that B is that parent.
Procedure 22 (Visible). Given a node N in a binary resolution tree
and a set PN of history paths that precede the root of the tree, vis(N; PN )
returns the atoms labels of nodes at and above N visible from the root.
Procedure vis(N; PN )
If (N is a leaf) return OE
Let A and B be the parents of N and partition PN into PA and
PB , which are the sets of paths which go through A and through B,
respectively. Assume without loss of generality that B is chosen so
that PB is nonempty.
Let CA and CB be the history paths with head at A and B respectively.
if PA is nonempty then
//N is held
return
else
and N is not held, so it is visible
return
endif
The third and fourth conditions in Theorem 21 require us calculate
which is done by calling vis(R i is the root of the
binary resolution tree, R 1 and R 2 are the parents of R, and R i is the
set of paths with heads at R i .
Procedure 22 runs in a number of set union calculations which is
proportional to the number of nodes in the tree. With hashing, these
operations can in principle be performed in time proportional to the
size of the clauses. Hence vis is a linear time algorithm, which is as fast
as one could expect.
We have implemented a prototype theorem prover for propositional
logic. It resembles OTTER[9], but it retains only minimal binary resolution
trees (so that the recursive calls in the first condition of Theorem
21 are not needed), whereas the proofs built by OTTER correspond
to non-minimal trees in some cases. Our implementation actually builds
binary resolution DAGs, instead of trees, to save space. This is important
because bottom-up theorem provers are limited by space as well
as by time. Note that Procedure 22 traverses the entire implicit tree,
so it may visit a single stored node more than once. Thus its runtime
is not guaranteed to be linear in the size of the DAG. The prototype
includes another restriction, discussed in [6], which ensures that from
each set of rotation equivalent binary resolution trees, exactly one is
found.
6. Restoring Minimality
There are two basic ways to restore minimality. If a binary resolution
tree is irregular then the surgery operation will remove the non-
minimality. For a binary resolution tree that is regular but non-minimal,
some rotations need to be done to make it irregular - we call this
exposing the non-minimality. This irregularity involves two internal
nodes M and N that resolve upon the same atom a and neither is a
descendant of the other but (at least) one is visible from the other.
Suppose M is visible from N . When M is rotated below N , one parent
of M becomes a descendant of N . Since a occurs in that parent's clause
label, surgery from N can be done.
In the proof of the converse of Theorem 19 we gave a quadratic
length sequence of operations for moving visible nodes down to become
descendants. Operation 24, called splaying, performs this with a linear
number of rotations. It is related to the splay operation [12] for binary
trees in that it brings a node closer to the root. In our case, to splay a
binary resolution tree at M , we divide the descendants of M into two
sets: the observers O for those that can see M , and the non-observers O
for those that cannot. Then we rearrange the tree so that all the nodes
in O are ancestors of M , leaving just those nodes in O as descendants
of M . Thus M is brought as close to the root as possible. This is done
in a way that guarantees N is an ancestor of M , so that surgery can
be accomplished.
The first step in the splay is to determine which descendants of M
can see M , as computed by Procedure 23.
Procedure 23 (Obs). Given a node M in a binary resolution tree T
such that P 1 is the complete set of history paths containing both M
and one parent of M and P 2 is the paths containing M and the other
returns the set of proper descendants of M that
can see M , called the observers of M .
Procedure obs(M;
if M is the root of T then
return OE
else
Let D be the child of M and K be the other parent of D.
Let C 1 be the paths from P 1 that close at D and C 2 be the paths
from P 2 that close at D.
Let K be the paths that contain both K and D.
holds M
return
else if C
return fDg [ obs(D; K[
else
endif
endif
The invariant of Procedure 23 is every pair of paths one from P 1
and the other from P 2 holds M . Then if we ever find a descendant D
that closes a member of each, then D holds M and so cannot see M ;
otherwise D can see M . To maintain the invariant as we go from M to
D, if D closes paths in P i we need to add to P i all of the paths K that
come from K through D, because each one of these paths precedes a
path that contains M . If D does not close any path through P i , then
we have nothing to add to P i . In either case we remove from P i any
path that closes at D since these do not contain D.
Operation 24 uses the same code as is in Procedure 23 (but not the
procedure itself) to decide how the resolutions below M should be re-ordered
in the new tree. The parents S 1 and S 2 of M are considered
stubs to be built upon. The observers of M are put into one of two
queues Q 1 and Q 2 , and the non-observers are put into Q 3 . Along with
each node put in the queue, we also put the subtree rooted at the other
parent of the node, so that the resolution can be reconstructed later.
Those put into Q 1 become descendants of S 1 , while those in Q 2 become
descendants of S 2 . After this is done, the two subtrees are joined by
a resolution corresponding to M . Finally all of the nodes in Q 3 are
made descendants of M . Each of the resolutions is done so that all
of the history paths closed at a node in the given tree are closed at
the same node in the constructed tree. Thus the resulting tree closes
history paths similarly with the given tree.
Operation 24 (Splay). A splay at an internal node M in a binary
resolution tree T produces a new binary resolution tree T 0 such that
all descendants of M cannot see M in T 0 .
Operation
Initially are three empty queues.
Initially P 1 is the set of history paths through M and one parent S 1
of M , while P 2 is the set of history paths through M and the other
parent S 2 of M .
call splay1(M;
return processQ(
Procedure splay1(M;
if M is the root of T then
return
else
Let D be the child of M and let K be the other parent of D.
Let C 1 be the paths from P 1 that close at D and C 2 be the paths
from P 2 that close at D.
Let K be the paths that contain both K and D.
holds M
enqueue D and the subtree rooted at K into Q 3
else if C
enqueue D and the subtree rooted at K into
else
enqueue D and the subtree rooted at K into
endif
endif
The resolutions must be done so that the resulting binary resolution
tree closes history paths similary with T .
Procedure processQ(T ; Q)
If Q is empty then
return T
else
dequeue T 1 and N from Q leaving Q 1
construct T 2 by resolving T and T 1 making them parents of N
return
endif
For example, the binary resolution tree in Figure 8 shows the result
of performing a splay from the node M = N 5 on the second tree in

Figure

4. The subtrees S 1 and S 2 are the leaves labelled a- b -:c, and
h, respectively. The descendants of N 5 are M 4
which N 3 and N 4 are put into Q 1 , while M 4 is put into Q 2 . These end
up above N 5 in the resulting tree. The only non-observer, N 6 , is put
into Q 3 and ends up below N 5 .
Lemma 25. If T is a binary resolution tree and M is an internal node
of T , then the result T 0 of splay(T ; M) is a binary resolution tree. T 0 is
defined on the same set of clauses as T ; is the same size as T and has
the same result as T .
Proof. T 0 closes history paths similarly with T . 2
Operation 26 (Splay Surgery). Let T be a binary resolution tree T
containing a pair M;N of nodes such that al(M) = al(N) and N can
see M
Procedure
if M is a descendant of N in T then
return surgery(N)
else
return
endif
To see that a splay actually brings M below N , so that surgery is
possible, suppose that the nearest common descendant of M and N is
not M . Thus either N is that nearest common descendant or N and M
are on different branches. N is either D or an ancestor of K at some
point in splay1. We also know N can see M so it must be put into one
of the queues Q 1 or Q 2 , and cannot be part of Q 3 . Thus when the tree
a-d
a:b-d-e
d:b-c-e
e:b-c-f-g
-a-b-e
c-d
e-f-g
a-b-c
a: b-b-f -b
g: a- b-b-f
-g
c:a- b-b-f-g
h:-a
-a-h

Figure

8. Result of Operation 24 at N5 on Figure 4
is rebuilt, N is put in while we are processing either Q 1 or Q 2 . M is
put in after Q 1 and Q 2 are empty, so it is below N .
Returning to our example, after the splay, the node N 5 in Figure 8 is
below N 1 and surgery from N 1 can be done. The result of this surgery
is the same as that shown in Figure 2. Notice that in this example we
also have the option to splay at N 1 and do surgery from N 5 , since N 1
is also visible from N 5 . The resulting tree would be different: it would
not require the leaf :a - h nor :h, and its result would have been f .
Thus surgery does not always produce a more general result, but always
produces a smaller tree.
Splay surgery does a linear number of set operations since it does
at most n resolution steps where n is the length of a branch, and then
performs surgery, which itself does a number of resolutions limited by
the length of a branch. Thus it is effectively a linear time algorithm.
7. Support
Often a node that is visible from another can be rotated so that it is not
a descendant, but sometimes no sequence of rotations can bring a node
from below another. In this case we say the node supports the other,
since it is always beneath it. Support, like visibity, is a property that
depends on the entire set of rotation equivalent trees. In this section
we characterize support in terms of history paths, so it can be checked
by examining a static tree.
Definition 27 (Support). A node N in a binary resolution tree T
supports a node M if after every sequence of rotations, M is an ancestor
of N .
Definition 28 (Tightly holds). Two history paths
tightly hold a node N if there exist two sequences
of history paths such that for
directly holds M ,
and for no pair does the head of P i equal
the head of Q j . A node N tightly holds M if both P and Q close at N .
Tightly holds is the special case of holds, where no two paths in the
sequences have the same head except the bottom two. This turns out
to be the exact condition for one node being below another after every
sequence of rotations.
Theorem 29. A node N tightly holds M if and only if N supports
M .
Proof. ()) We show that if N tightly holds M before a rotation, it
also does afterwards, and thus must be a descendant of M . Consider a
rotation of edge (C; E) where nodes A; B; C; D and E are defined as in
Definition 8. We will indicate the image of any history path P after the
rotation by P 0 . At first assume that fC; Eg is disjoint from fM;Ng. By
. Before the rotation all heads of
paths in the sequences were distinct so they must be distinct after the
rotation, unless there is a new path in either of the sequences. There is
one case where such a new path occurs. (In Lemma 17, the new path
introduced when tail(Q) is in .) In that
case the head of new history path was not on path(M; N ). Thus the
heads of paths of the new sequences (P 0
are
distinct except head(P 0
Also M is still directly held
by
1 so N tightly holds M .
Now suppose fC; Eg is not disjoint from fM;Ng. (Case 1) If
then without loss of generality let P 1 contain A and Q 1 contain B.
We know that P 1 does not close at E since the rotation is possible.
Suppose first that Q 1 does not close at E, as in Figure 9.1a. Then
after the rotation C is directly held by P 0
1 and so is tightly
held by N . Now suppose Q 1 closes at E, as in Figure 9.1b. Note that
there must exist Q 2 since otherwise E holds N and and so Q 1 and P 1
both close at and then the rotation is not possible. After the
A
C=N
A
Qm
Pn
Q'n
Case
2b A
A
Case
3a
A
A
Case
A
A
Case
1a A
A
Case
1b A
C=N
A
Pm
Qn
Q'n
Case
2a

Figure

9. Cases showing tightly hold is invariant under rotations
Case 1
Case 2
A
A
A

Figure

10. Cases showing support implies tightly holds
rotation, C is directly held by P 0
2 , and so is still tightly held
by N . (Case close at N , and
either they both include A, as in Figure 9.2a, or they both include B
as in

Figure

9.2b. In either case, after the rotation, N tightly holds
M by the sequences (P 0
(Case
then assume without loss of generality that C is in P 1 and
D is in Q 1 . If A is also in P 1 as in Figure 9.3a, then there is a path
with head at B and P 0 OE P 1 . After the rotation, the sequences
ensure that N tightly holds M . If B
is in P 1 , as in Figure 9.3b, then after the rotation the paths (P 0
and (Q 0
tightly holds M . (Case Finally if
then either Pm and Q n both contain C or both contain D. If they both
contain C then note that they both contain the same parent B of C,
else the rotation is not possible. Then after the rotation, B is a parent
of E and although P 0
are one node shorter than before, N
tightly holds M by the paths (P 0
both contain D, then after the rotation there is no change to the
history paths; thus N tightly holds M .
We induct on the length of path(M;
is the parent of N i+1 for
1. If is a parent of N and then a rotation of the edge (M; N) is
possible unless there exist history paths that tightly hold M and close
at N . Suppose k ? 2. First assume (case 1) that N k does not support
. Then by the first half of this theorem N k does not tightly hold
and so the edge (N can be rotated. Since M must remain
an ancestor of N; M must be an ancestor of B in the rotation, as shown
in

Figure

10.1. Thus the path from M to N is shorter, and by induction
tightly holds M in the new binary resolution tree. By rotating the
edge back again, one sees that N tightly holds M in T , because tightly
holds is invariant under rotation. As this case is covered, from now on
we assume that N k supports N k\Gamma1 .
Next assume (case 2) that N k does not support N k\Gamma2 . Then N
does not support N k\Gamma2 , for otherwise N k supports N k\Gamma1 supports N k\Gamma2
and, by transitivity of support, there is a contradiction. By the first half
of this theorem, we may rotate the edge (N k\Gamma2 ; N k\Gamma1 ). Then we may
rotate the edge (N k\Gamma2 ; N k ), since N k does not tightly hold N k\Gamma2 . In
this second rotation, N k must remain a descendant of N k\Gamma1 so it must
be as shown in the third binary resolution tree in Figure 10.2. Thus
path(M; N) is shorter, so by induction N tightly holds M . Because
tightly holds is invariant under rotation, N tightly holds M in T also.
From now on we assume that N k supports N k\Gamma2 . By similar arguments
we assume that N k supports each of N
(Case If the edge (M; N 2 ) can be rotated in T then path(M; N)
is shorter in the resulting tree T 0 and by induction N tightly holds M
in T 0 . Since tightly holds is invariant under rotation, N tightly holds
in T .
Finally if the edge (M; N 2 ) cannot be rotated then there are paths
from each parent of M closing at N 2 in T . We know by induction that
tightly holds N 2 and assume that (P are
the paths that make this so. Assume without loss of generality that
includes M . Then Q 1 must include the other parent of N 2 since N 2
is the first node that P 1 and Q 1 have in common. No matter which
parent of M is in P 1 , there is a path closing at N 2 containing M and
's other parent. Call this path Q 0 . Since the head of Q 0 is M , it is
distinct from the heads of the other P i and Q j in the sequences. Thus
tightly holds M by the paths (Q
In this final theorem, we relate the notions of visibility and support.
Theorem 30. In a binary resolution tree, a node M is invisible from
a node N iff there is a support S of M on path(M; N ).
Proof. ()) Let D be the nearest common descendant of M and N .
Consider the path sequences by which D
holds M . Consider the least i and j such that head(P i
and let S be the node where P i and Q j both close. These i and j must
exist since the head(Pm the heads of paths before
are distinct, S tightly holds M and thus S supports M .
be the
sequences of paths by which S tightly holds M . Since S is on path(M; N ),
and S is a descendant of M , S must be an ancestor of the nearest
common descendant D of M and N . Thus there are paths R
such that S is on R 1 and R k closes
at D. Thus D holds M via the sequences
8. Relation to Clause Trees
The results in this paper were developed after we understood clause
trees [5], and then primarily as a means to implement clause trees.
We eventually found that most of our ideas from clause trees could be
expressed in binary resolution trees. Binary resolution trees are simpler
in some ways since they are easier to implement, but more often clause
trees are easier to use when exploring new ideas. In particular, visibility
and support can be read almost directly from a clause tree, whereas the
properties held and tightly held are somewhat harder to see in binary
resolution trees. We use whichever is appropriate. In this section we
relate the two.
Refering once again to the example, the rotation equivalent binary
resolution trees in Figures 1, 4 and 8 all correspond to the first clause
tree in Figure 11. The binary resolution tree in Figure 2 corresponds
the second clause tree in Figure 11. A reader familiar with clause trees
will note that the second clause tree in Figure 11 can be constructed
directly from the first by adding a merge path between the a atom
nodes and then doing surgery to remove the subtree beyond the tail of
the new merge path.
Some of the correspondences between binary resolution trees and
clause trees are obvious. The leaves of a binary resolution tree are the
clause nodes of the clause tree; both are labeled by an instance of an
input clause. The internal nodes of a binary resolution tree are the
atom nodes of a clause tree. A history path in a binary resolution tree,
which we associate with a literal, corresponds to a labeled edge in a
clause tree, also associated with a literal. If the history path is not
closed, it corresponds to an edge incident with an atom node leaf; a
closed history path corresponds to an edge incident with a closed atom
node.
_
a
c d a
e
f
_
_
a
c d a
_
_
_

Figure

11. Clause trees corresponding to Figures 1, 4, and 8 and to Figure 2
Minimal clause trees correspond to minimal binary resolution trees
whose result does not contain two identical atoms. The minimal condition
on clause trees does not allow any legal unchosen merge path
or legal tautology path, including leaf to leaf paths. A minimal binary
resolution tree may correspond to a clause tree with a legal unchosen
leaf to leaf merge path, or legal leaf to leaf tautology path. This is
because the regularity condition in binary resolution trees requires one
of the nodes to be internal.
Finally, visibility (resp. support) between internal nodes in a binary
resolution tree correspond to visibility (resp. support) between closed
atom nodes in a clause tree. Table I shows a number of the corresponding
notions.

Table

I. Corresponding notions
Clause Trees Binary resolution trees
clause node leaf node
internal atom node internal node
edge history path
open leaf atom node literal in result
cl(T
internal-to-internal surgery splay surgery
internal-to-leaf surgery surgery
merge path two history paths closing together
equivalence classes of reversal equivalent equivalence classes of rotation equivalent
minimal (up to leaf-to-leaf) minimal
visible internal atom nodes visible
support on internal atom nodes support
path reversal no structural change
no structural change rotation (change in derivation)
merge set all history paths closing at a given node
9. Related, Past and Future Work
Regularity is one of the most important restrictions and some form
of it is used in many theorem proving methods related to resolution,
including tableau [10], and all variants of model elimination[8].
Permuting inference steps has been investigated by Kleene [7], in the
context of Gentzen's sequence calculus, both classical and intuitionistic.
Kleene's permutations sometimes increase the size of the proof. It is
interesting to note he defines the ancestor relation between instances
of formulas in each inference, allowing him to state which instances of
formulas in the deduction belong to a given instance in the end sequent.
This is analagous to our notion of history paths.
In the context of binary resolution derivations, de Nivelle [3], has
two types of nodes: resolution nodes have two parents and factoring
nodes have one. He defines four types of edge rotations, depending on
the type of nodes incident with the edge. We disallow the rotation
where a factorization node is parent to a resolution node, because in
this case the size of the derivation must be increased. His application
is to construct resolution games which are then used to show various
completeness results for restrictions of resolution based on ordering
literals. Both de Nivelle's paper and ours show that basic properties
of resolution may be exposed by considering the set of trees equivalent
modulo permutations.
The main contribution of this paper is to present the minimal restriction
of resolution, originally developed in terms of clause trees, using
the well known proof format of binary resolution derivations. The original
motivation for doing so was to implement bottom up algorithms
for constructing minimal clause trees. A direct implementation, based
on the structural definition of clause trees was done, but each new tree
needed its own storage space and the visibility algorithm was cumber-
some. To remedy both of these problems we used the notion of binary
resolution derivations and implemented each new clause tree as a single
storage cell with two parent clause trees. (For propositional logic this
solves the space problem, since each tree can be used as part of other
trees.) The surprising result is that visibility can be expressed easi-
ly, and requires a linear algorithm using this data structure. We then
determined to explain as much of our clause tree work as we could
using binary resolution trees, to make them more accessible to readers
familiar with resolution. This task turned out to be difficult until we
discovered that edge rotations, history paths and the "precedes" relation
between history paths are the fundamental concepts needed. Then
we related visibility and support to history paths using the "holds" and
"tightly holds" relations. In retrospect the authors still believe that it
is easier to work conceptually with clause trees. We have anecdotal evi-
dence, from a graduate course in automated reasoning given two times
by the second author, once with clause trees only and once with binary
resolution trees followed by clause trees, that the intuitions for support
and visibility are quite understandable using either data structure.
We are presently developing the full theorem provers described here,
one to use minimality as a restriction and another to use splay and
surgery to improve proofs after they are constructed by resolution.
Redundancy elimination by subsumption is always an important consideration
for theorem provers. Unfortunately the minimal restriction
is not complete with full subsumption. For instance one can not refute
with a minimal binary resolution tree if one resolves p
against :p-q, generating q, and then uses back subsumption to remove
before the resolution between :p - q and :p - :q is done. The
latter resolution is part of the only minimal binary resolution refutation
of these clauses. The former resolution step leads to a binary resolution
tree in which p is resolved at two different nodes on one branch.
However, we have discovered [6] that one can retain completeness by
giving up some of the power of minimality, without giving up any of
the power of subsumption.
The space of minimal binary resolution trees is interesting for the
following reasons: (1) it is refutationally complete, (2) it extends the
well known regularity restriction of resolution (3) it contains the smallest
binary resolution tree, (4) non-minimal (sub)trees can be identified
in time linear in the size of the tree, and (5) non-minimal trees can be
reduced to minimal ones efficiently.



--R

An algorithm for the organizaton of information.
Symbolic Logic and Mechanical Theorem Proving.
Resolution games and non-liftable resolution orderings
Regular resolution versus unrestricted resolution.
Clause trees: a tool for understanding and implementing resolution in automated reasoning.
Bottom up procedures to construct each minimal clause tree once.
On the permutability of inferences.
Mechanical theorem proving by model elimination.
Otter 3.0 users guide.

A machine-oriented logic based on the resolution principle

Extending the regular restriction of resolution to non-linear subdeductions
On the complexity of derivation in propositional calculus.
--TR

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'finite domain'
 'zero storage biometric authentication' 'graph coloring']
marked:['regularity', 'theorem proving', 'resolution', 'binary tree', 'clause tree']
--T
A Full Formalization of SLD-Resolution in the Calculus of Inductive Constructions.
--A
This paper presents a full formalization of the semantics of definite programs, in the calculus of inductive constructions. First, we describe a formalization of the proof of first-order terms unification: this proof is obtained from a similar proof dealing with quasi-terms, thus showing how to relate an inductive set with a subset defined by a predicate. Then, SLD-resolution is explicitely defined: the renaming process used in SLD-derivations is made explicit, thus introducing complications, usually overlooked, during the proofs of classical results. Last, switching and lifting lemmas and soundness and completeness theorems are formalized. For this, we present two lemmas, usually omitted, which are needed. This development also contains a formalization of basic results on operators and their fixpoints in a general setting. All the proofs of the results, presented here, have been checked with the proof assistant Coq.
--B
Introduction
This paper is concerned with programming languages semantics. Its aim is
to formalize SLD-resolution in the calculus of inductive constructions (Coq).
First, terms and substitutions are dened, and the unication theorem, constantly
used in the denition of the semantics of denite programs, is proved
from a similar theorem for quasi-terms [26]. Then, we present the formal
denitions of syntactic objects used in logic programming (atoms, queries,
clauses, etc) and we introduce their interpretation schemes. Last, declarative
and operational semantics of denite programs are introduced, and the
proofs of fundamental results of the theory of logic programming are formalized
(lifting and switching lemmas, soundness and completeness theorems).
The reader is assumed to be familiar with the terminology and the basic
results on the semantics of logic programs, as introduced in [2, 19].
Of course, there exists an extensive literature on logic programming [3,
2, 17, 19, 28] where all these theorems are proved, and one can wonder
about the usefulness of such a formalization. For this, recall that formalizing
a proof rst consists in giving formal denitions of the objects used,
then explicitely stating a priori the hypotheses needed, and last inferring the
conclusion. Therefore, since our proofs have been encoded using the Coq
proof assistant, all objects and properties are explicitely dened as well as
all hypotheses needed for these proofs. Each step of a proof also explicitely
results from denitions or applications of induction schemes, lemmas, or the-
orems. This makes a dierence from the initial proofs, which can be found
in many books on logic programming. Hence, formalizing proofs brings us
at a level of detail that is often left to the reader, and we will see that if we
take into account some details, often considered as minor in more informal
presentations, proofs are getting a bit complicated. Furthermore, we will
see that some assumptions, usually overlooked, are required in some of the
theorems proved. The main diculty of this development is concerned with
the denition of the variable renaming process used in an SLD-derivation,
which is made explicit, thus introducing some complications, usually omit-
ted. For instance, we will see that the proof of the completeness theorem
is more delicate than it seems and requires, from a formal point of view,
several technical lemmas. Few papers are devoted to these details; however,
in [27], J. C. Shepherdson points out inaccuracy in some of the basic results
in the theory of logic programming and presents a discussion about the role
of standardizing apart in SLD-resolution. Let us give the most famous ex-
ample, coming from this paper, and showing how a classical result can be
incorrectly stated: in a famous book [19], J. W. Lloyd gives the fundamental
theorem of completeness of SLD-resolution in the following form:
Let P be a denite program and G a
denite goal. For every correct answer  for P [fGg, there exists
a computed answer  for P [ fGg and a substitution
such that
.
In 1994, J. C. Shepherdson wrote about this theorem: This is not true [27]
and gave a very simple counterexample. Of course, it is true that we can nd
two substitutions  and
such that
G; however, although  and
act only on the variables occurring in G, this does not imply that
can
be restricted to yield
. Another typical example is the lifting lemma
generally stated as follows:
be a program, N a goal and  a substitu-
tion. Suppose that there exists an SLD-refutation of P [ fNg
with the sequence of mgu's  . Then, there exists an SLD-
refutation of P [ fNg with the sequence of mgu's  0
n such
that  0
0 is more general than
No assumptions on the substitution  are supposed here. However, in order
to formalize the corresponding proof, which builds the derivation starting
from N , we have to assume some hypotheses. Let x be a variable occurring
in the domain of  and in N that does not occur in N . Whereas x can be
used in the renamed clauses during the derivation starting from N , it cannot
be used during the derivation starting from N . Hence, in order to exactly use
the same sequences of variant clauses (which is always suggested) in the two
derivations, we have to suppose that no variable used for the renaming of the
input clauses in the derivation starting from N occurs in N . Furthermore,
for similar reasons, in order to prove  0
we have to suppose
that no variable used for the renaming of the input clauses in the derivation
starting from N occurs in the domain of . A similar problem happens
during the proof of the switching lemma asserting that if during an SLD-
derivation, two atoms L 1 and L 2 are successively selected, then they can also
be selected in the reverse order and the derived states are the same up to
renaming of variables.
| {z }
| {z }
Here again, if we want to exactly use the same variants of C 1 and C 2 in
the two derivations, some assumptions are required. By hypotheses (coming
from the initial derivation), we have
However, if we
want to select L 2 before L 1 with C 2 , C 2 must satisfy
But nothing in the switching lemma ensures such a property, and, depending
on the standardization apart used for SLD-derivations, it is possible that a
variable occurring in R 0 but not occurring in R 1 , occurs in C 2 . To avoid
this problem, C 1 and C 2 must satisfy
holds if we use the standardizing apart coming from [2]). Hence, formalizing
a proof brings us to a greatest level of detail. Most of problems are due to
variables occurring in the input clauses (variants of clauses used in the resolution
steps) coinciding with other variables in the derivation. Surprisingly,
another problem comes from the necessity of an explicitation a priori of all
the assumptions needed to obtain a proof. Curiously, even if this requirement
seems natural, it is a common practice in logic programming, to suppose,
during the proofs (on paper) of most of basic results, some assumptions
on the variables occurring in the clauses used, without any justication. Of
course, there exists an independence result asserting that the existence of a
derivation does not depend on the choice of variables in the input clauses
(even if this choice is very important during the proofs), and these results
are still correct. However, these properties on variables have to be formally
proved from this independence result, and, furthermore, they can only express
properties on the objects occurring in the theorem (for instance, we
cannot suppose during a proof that a clause does not share variables with a
goal built in the proof if this goal does not appear in the theorem).
Standard semantics ( la Herbrand) of denite programs deals with well-known
objects: substitutions and renaming. However, several non-equivalent
denitions for these objects can be found in the literature, thus introducing
some confusion (for example, the meaning of renaming is often assumed
simpler than its formal denition implies). In [15], H. P. Ko and M. E. Nadel
point out that dierent authors give subtley dierent denitions of substitu-
tions. They cite six non-equivalent denitions and point out the implication
for the is an instance relation, thus giving dierent versions of the lifting
lemma. Therefore, substitutions are not as simple as they might rst ap-
pear, and this motivates some authors, as R. S. Kemp and G. A. Ringwood
in [14], to take the instance relation as primitive rather than taking substi-
tutions. There is also a confusion in the literature concerning the denition
of most general uniers (mgu in short). In [18], J. L. Lassez, M. J. Maher,
and K. Marriot compare four non-equivalent denitions of mgu. Of course,
logic programming is concerned with these problems and is also subject to
such confusions. For an SLD-derivation
the following denitions for the renaming process required in an SLD-deriv-
ation can be found:
and the confusion arises because all these conditions in fact dene dierent
hypotheses. There are at least two reasons for these conditions. First, the
standardization apart is needed in order to be able to unify the selected
atom with the head of the variant of the clause used. The last reason is
to obtain the most general possible answer (substitution). Therefore, this
formalization settles down the problem of variable renaming, which has long
been a cause of errors in the eld of logic programming.
Description of Coq
We use here version 6.2 of the proof assistant Coq [4], which allows the interactive
development of formal proofs. Coq is based on a logical framework
known as the calculus of inductive constructions [5], which is an extension of
a typed -calculus supporting dependent types and polymorphic types. The
basic idea underlying this logical framework, based on the Curry-Howard
isomorphism [9, 17], is that a proof of a proposition can be seen as a functional
object. For instance, a proof of a proposition of the form A ) B is a
function mapping every proof of A to a proof of B. The type of this function
is isomorphic to the proved proposition, so types and proposition are
identied, as are objects and proofs. Furthermore, this framework allows the
denition of inductive and co-inductive types (which are specied by their
constructors). A classical example of an inductive type is the denition of
natural numbers, which is dened in the system Coq by
Inductive
Three induction principles are (automatically) generated by this denition:
nat_id :(P:nat->s)(P O)->((n:nat)(P n)->(P (S n)))->(n:nat)(P n)
(id; s) is either (ind,Prop) or (rec,Set) or (rect,Type). A classical
example of a co-inductive type is the type of innite sequences, formed with
elements of type A, or streams for shorter. In Coq, it can be introduced by
CoInductive Set Stream [A:Set]:=cons:A->(Stream A)->(Stream A).
Note that there is no induction principle for co-inductive types. Coq also
allows the denition of mutually recursive types. We shall use here two
distinct sorts: Prop is the type of logical propositions; Set is the type of data
type specications. The type of Prop and Set is Type. Constructing a proof
within Coq is an interactive process: given a goal, the user species which
deduction rule should be applied, and Coq does all the computations. The
theorem prover solves successive subgoals with tactics (i.e., functions that
build a proof of a given goal from proofs of more elementary subgoals).
Goal
apply a tactic
Subgoal n
3 Terms and Unication
The rst step of this development consists naturally in formalizing uni-
cation of terms. The unication algorithm deals with two sets: terms and
substitutions. Let us rst present their formal denitions. Given a functional
signature  (i.e., a countable set with an arity function ar) and a countable
set X of variable symbols, terms (T  [X]) can be dened by induction as
follows:
if f is a functional symbol in  such that l is a list
of terms of length n, then tf(f; l) is a term.
At the same time, we introduce the dependent type L n; [X] of lists of terms
of length n (T  [X] and L n; [X] are mutually recursive).
Substitutions (S T ) are dened as applications from X to T [X]. Recall
now that given two terms t 1 and t 2 , if for a substitution s, we have
called a unier of t 1 and t 2 (unif_t). s is called a minimal
unier (min_unif_t), if for every unier s 0 of t 1 and t 2 , we have s  s 0 (i.e.,
there exists a substitution s 0 such that s 0 which is denoted by the
predicate less_subst_t). A unier s of t 1 and t 2 is called a most general
unier if it is minimal, idempotent (i.e. 8x 2 X which is
denoted by the predicate idempotent_t) and relevant to t 1 and t 2 (i.e., every
variable occurring in the domain of s also occurs in the list formed by t 1 and
and every variable occurring in the range of s also occurs in the
list formed by t 1 and t 2 (under_lt)). These denitions are extended to lists
of terms.
Instead of formalizing unication on terms by following the standard
approach (based on the unication algorithm), and since a similar (formal)
proof, dealing with quasi-terms, has been obtained by J. Rouyer [26], we have
transposed the unication property from quasi-terms to terms. Originally,
quasi-terms were introduced by Z. Manna and R. J. Waldinger in order
to illustrate their technique for program synthesis: in [21], an unication
algorithm has been derived by proving that its specication can be satised.
This proof has been veried within the Cambridge LCF theorem prover [10],
by L. C. Paulson [23]. A similar proof has been obtained by J. Rouyer [26]
for quasi-terms dened as follows:
are used to obtain variable quasi-terms, constant quasi-
terms and functional quasi-terms, ConsArg is used to build non-empty lists
of quasi-terms (terms and lists of terms are merged in a single data type).
For instance, the term f(x; a; g(x)) is obtained as follows
More precisely, J. Rouyer has proved that if there exists an unier for q 1
and q 2 , then if these two quasi-terms are well formed, this unier, which is
a quasi-substitution, is indeed a substitution. Clearly, T [X] is a proper
subset of Q [X] (ConsArg(ConsArg(x; x); x) is neither a term nor a list of
terms), and, in order to transpose the unication property, we have to dene
a mapping f from well-formed quasi-terms (i.e., quasi-terms representing
terms) to terms. However, Coq does not incorporate the notion of partiality:
any function is total. The current solution we have taken in this development
consists in considering simultaneously a quasi-term and the proof that it is
well formed. Hence, a predicate P on Q [X] characterizing well-formed
quasi-terms is dened. Depending on the type of P , two approaches are
possible. The rst one corresponds to a programming language
the type of P is dened recursively on the structure
of the quasi-term q considered. When q is not well formed, we use the
applied to Q [X] and to a proof of the
proposition False obtained from q and the proof that it is well-formed. The
second approach corresponds to a proofs language approach: the type of
dened recursively on the structure of the proof
that the quasi-term considered is well formed.
In addition to the interest of the unication theorem, we are motivated by
developing an approach based on the reusability of formal proofs. Therefore,
this development shows how to dene partial functions. More precisely, we
outline how from
two inductively dened sets,
a predicate P on E 1 , such that there exists a bijection between E 2 and
we can dene the following functions:
The two characteristics of this problem are as follows:
1 is dened by a predicate, that does not allow any (direct) denition
of functions or predicates on E P
1 . Therefore, every expression of the
stands for 8e
2. We just know the bijection between
such that
Furthermore, we would like to have
The function f 21 is easy to obtain, seeing that every e 2 in E 2 is in relation
with an element in E 1 . Furthermore, for every e 2 in
The denition of f 12 is rather critical because of the denition, by a pred-
icate, of the domain of f 12 . The technique used here consists in dening
inductively for every e 1 in E 1 , the collection P[e 1 of the
proofs of P logically equivalent, only P[e 1
has a computational content). Of course, for every e 1 in E P
1 , there must
exist a proof of P In this way, we are in position to dene,
recursively on the structure of such a proof, the function
Y
such that 8e 1 Hence, according to these def-
initions, proofs can be viewed as objects (data structures) of a mathematical
collection (inhabiting the Set type) with an explicit algorithmic content, thus
allowing to proceed by induction over these proofs or to dene recursive functions
on the structure of these proofs. Since E 2 is isomorphic to a subset
of our aim is to map result about E 1 to a result about E 2 by proving
only few conservativity lemmas. Let U 1 and U 2 be two formal properties,
respectively on E 1 and E 2 (quasi-unication and unication), such that
and assume that the property U 1 holds on E 1 . Given the two functions f 12
and f 21 , we can prove the property U 2 on E 2 , by the following reasoning. Let
e 2 be an element in by hypothesis, we have U 1 (f 21 (e 2 )), and there exists
a proof p for f 21
And the claim now follows, since
. In this manner, we have proved the unication theorem
without really working on unication, but by proving only few conservativity
lemmas (for a full presentation of this proof, see [13]).
Theorem 3.1 (Unication) Given two terms t 1 and t 2 , either t 1 and t 2
are not uniable, or they are and there exists an idempotent minimal unier
relevant to these terms.
4 Objects Used in Logic Programming
We present in this section the formal denitions of the objects used in logic
programming, and we introduce, for each of them, an interpretation schema.
First, let  be a relational signature. The collection At; [X] of atoms
is inductively dened as follows: if p is a predicate symbol in  such that
l is a list of terms of length n, then pl(p; l) is an atom. Then,
queries, denite clauses, denite programs, and Horn clauses are dened from
atoms by
(arity ar):fun -> nat
Mutual inductive
ar f)) -> Term
with list_term : nat -> Set :=
cons: (n:nat)Term -> (list_term n)->(list_term (S n)).
subst := var -> Term.
Inductive Unification_t[t1,t2:Term]:Set:=
(cons (S O) t1 (cons O t2 nil)))
(cons (S O) t1 (cons O t2 nil)))
->(Unification_t t1 t2).
Inductive quasiterm : Set :=
V:var->quasiterm | C:fun->quasiterm |
Root:fun->quasiterm->quasiterm |
ConsArg: quasiterm->quasiterm->quasiterm.

Table

1: Terms, substitutions, and unication.
In order to be able to give an interpretation of these objects, it is necessary
to attach some meaning to each of the symbols occurring in them. Therefore,
let us dene the interpretations of the signature [. For this, let ' n [A] be
the dependent polymorphic type of lists of elements of type A of length n,
and bool the set of booleans (with its usual operators,
order to dierentiate them from the operators :, ^, _ on Prop). We dene
two collections of mappings: F n [A] is formed by mappings from ' n [A] to A,
and P n [A] is formed by mappings from ' n [A] to bool. According to these
denitions, interpretations, over a domain D, are dened, in a classical way,
by
I
Y
F ar(f) [D] I
Y
Hence, given an interpretation I, over D, of  (resp. ), a n-ary function f
predicate p) is interpreted by a mapping f I
bool). We are now in position to give the following interpretation
At; [X] Inductive atom : Set :=
pl:(p:predic)(list_term (arity_p p))->atom.
Inductive request : Set :=
true_req:request|
cons_req:atom->request->request.
C; [X] Definition clause:Set:=(atom*request).
Inductive program : Set :=
nil_pgm:program|
cons_pgm:clause->program->program.
Inductive horn : Set :=
hp:program->horn|
hr:request->horn->horn.

Table

2: Syntactic objects.
schemes. First, we dene A X as the collection of valuations over A (mappings
from X to A). Then, terms (and lists of terms) are interpreted as follows:
let I be an interpretation of , over a domain D, a term t (resp. a list of
terms l of length n) is interpreted by an application
dened by
f I (l I (v)) if
l I
where nil_' and cons_' are the constructors of ' n [D] and cons is a constructor
of L n; [X]. Henceforth, we will nd it more convenient to write I
to denote an interpretation of  [  I  [D]  I  [D]). An object
clause, .) is interpreted by an application
dened by
I (l I (v))
r I
false if
(: b a I
true if
a I
I (v)) if
ri I (v) = a I (v) _ b r I (v)
true if
c I (v)
I (v) if
r I (v)
Inductive
[D:Set][n:nat](LIST D n)->D.
I  [D] Definition f_i : Set -> Set :=
[D:Set](f:fun)(f_n D (arity ar f)).
A X Definition valuation:Set->Set:=[A:Set](var->A).
Fixpoint Interp_t [D:Set;I:(f_i D);t:Term]
:(valuation D)->D:=[v:(valuation D)] Case t of
[f:fun][l:(list_term (arity ar f))]
((I f) (apply_l D I (arity ar f) l v)) end
with apply_l [D:Set;I:(f_i D);n:nat;l:(list_term n)]
(valuation D) -> (LIST D n) :=
[v:(valuation D)]<[n:nat](LIST D n)>Case l of
(NIL D)
[n0:nat][t0:Term][l0:(list_term n0)]
(CONS D n0 (Interp_t D I t0 v) (apply_l D I n0 l0 v))
end.
bool Inductive bool:Set:= true:bool | false:bool.
[D:Set][n:nat](LIST D n) -> bool.
I  [D] Definition p_i :Set -> Set :=
[D:Set](p:predic)(p_n D (arity_p p)).

Table

3: Interpretations of  [ .
An interpretation I, over a domain D, is said to be a model of an object
(atom, query, .), if for every valuation v, over D, we have
In this case, we write (depending on the interpretation schema used
for a query r, we write either I r or j= I ~ r). Note that, whereas given a
valuation v 2 D X , the proposition true is decidable, since D can be
an innite set, the proposition I o is not necessarily decidable. This will
introduce many complications in the following. To terminate, we dene the
relation of logical consequence (j=) between a Horn clause and a query as
follows:
General interpretation schema of
Definition Interp_O :(D:Set)(f_i D)->(p_i D)->O->
(valuation D)->bool:=[D:Set][If:(f_i D)][Ip:(p_i D)][o
[v:(valuation D)]  Case o of I end.
[p0:predic][l0:(list_term (arity_p p0))]
request
false
(orb (neg (Interp_p D If Ip l0 v)) (Interp_r D If Ip r0 v))
request
true
(andb (Interp_p D If Ip l0 v)) (Interp_rn D If Ip r0 v))
clause
[a:atom][r:request]
(orb (Interp_r D If Ip r v) (Interp_p D If Ip a v))
true
(andb (Interp_c D If Ip c0 v) (Interp_P D If Ip p0 v))
[p:program](Interp_P D If Ip p v)
(andb (Interp_r D If Ip r0 v) (Interp_h D If Ip h0 v))
I
Definition O_valid:(D:Set)(f_i D)->(p_i D)->O->Prop:=
((v:(valuation D))(Interp_O D If Ip l v)=true).
request -> Prop :=
(h_valid D If Ip h)->(reqn_valid D If Ip r) ).

Table

4: Interpretations and models.
5 SLD-Resolution
The logical approach of denite programs can be split in two parts: the
declarative semantics determines what can be computed; the operational semantics
(given by SLD-resolution) describes how it is computed. Execution
of denite programs is based on a combination of two mechanisms: replacement
and unication. This form of computing is a specic form of theorem
proving, called SLD-resolution (for Selection Linear Denite).
5.1 Resolution and Transitions
The resolution rule is an inference rule that denes a deductive relation,
C be the denite clause C + C in the program P ,
is an atom, called the head of C, and C a query, called the
body of C, R a query and r a renaming substitution such that no variable
occurring in r(C) also occurs in R. We can write the resolution rule in the
following abbreviated manner:
is a mgu of R =n and r(C
where R =n stands for the (n+ 1)-th atom of R (at_n_req), and where R[n
r(C )] is the query obtained by replacing in R, the (n 1)-th atom by
the query r(C ) ((change_n_r n (rsubst_req r (body_c C)) R)). In each
resolution step, two choices are made: the choice of the selected atom and
the choice of the input clause whose head unies with the selected atom.
Therefore, the resolution rule can be viewed as a rule that moves from a state
to another, dened as follows. First, we introduce SR as the collection of
mappings from X to X (the classical properties on substitutions are extended
to SR ) and a predicate P R on SR , characterizing the renaming substitutions
r such that
occurring in the domain of r also occurs in the range of r
(r is idempotent).
A resolution state is a pair :R, where  is a substitution and R a query.
We give now the formal denitions of the renaming conditions that have to
be satised during a transition  i :R i
. First, the initial resolution
state  i :R i must satisfy  i R Then, the renaming substitution r has
to rename all the variables occurring in C and only these variables; this is
denoted by PRC (r; C). Last, we dene two predicates satised when no variable
occurring in the range of r also occurs in the domain of  i (P RS
in the query R i (P RR We are now in position to dene the predicate
P , on the collection of transitions, by
where MGU(; a 1 ; a 2 ) (at_mgu) means that  is a most general unier of a 1
and a 2 .
5.2 SLD-Resolution and Derivations
A nite valid SLD-derivation is a nite sequence of composable transitions
satisfying the predicate
also
Hence, derivations can be dened inductively,
from , as lists. Most of proofs of properties of SLD-resolution are obtained
by induction over a derivation by using a left induction schema. However,
during the soundness proof, we shall need the right one. Hence, we dene
two collections of derivations as follows (D l and D r are isomorphic):
D l ::= d l
c (D l ;
| {z }
left induction
| {z }
right induction
Two functions are dened, in order to relate these two sets. As we did for the
transitions, we now dene a predicate, on D l , characterizing valid derivations
(sequence of composable transitions satisfying the predicate P and an
additional renaming condition). For this, we introduce the following deni-
tions. Two transitions e 1
f and e 2
f are said to be composable
. A function #, on D l ,
that computes the list of variables occurring in the input clauses of a deriva-
tion, is dened. The predicate P D , on D l , can now be recursively dened
by
t (t) and P (t), then P D (d)
Resolution states Definition state:Set:=(subst*request).
(Fst and Snd are projections on a product type.)
SR Definition rename:=var->var.
Definition good_rename:rename->Prop:=[sr:rename]
(((y:var)(x=y)->(rdom sr x)->(rdom sr y)->
(sr x)=(sr y))^((rdom sr x)->(rrange sr x))).
Definition rename_c:rename->clause->Prop:=
PRR
Definition rename_out_req:rename->request->Prop:=
PRS
Definition rename_out_dom:rename->subst->Prop:=
((x:var)(rdom r x)->(s (r x))=(tv (r x))).
Inductive trans:Set:=trans_cons:state->program->nat->
clause->rename->subst->atom->state->trans.
Inductive rsl [ei:state;p:program;n:nat;
c:clause;r:rename;s:subst;a:atom;ef:state]:Prop:=rsl_init:
(le (S n) (Length_r (Snd ei)))
(rsubst_at r (head_c c)))
(*H4*) ->(good_rename r)
(([x:var](Subst_t s ((Fst ei) x))), (subst_req s
->(rsl ei p n c r s a ef).
Definition RSL:trans->Prop:=[t:trans] Case t of
[a:atom][ef:state](rsl ei p n c r s a ef) end.

Table

5: Transitions.
D l
Inductive deriv:Set :=
deriv_cons : deriv -> trans -> deriv.
Inductive
deriv_f_cons : trans -> deriv_f -> deriv_f.
Inductive couple_trans_ok [t1:trans;t2:trans]:Prop:=
couple_trans_ok_init:(p_trans t1)=(p_trans t2)->
(state_end_t t1)=(state_init_t t2)->(RSL t1)->(RSL t2)
->(couple_trans_ok t1 t2).
Fixpoint list_var_c_d [d:deriv]:listv:=
Case d of
(var_cl (rsubst_cl (sr_trans t0) (c_trans t0)))
[d1:deriv][t1:trans](Appv (list_var_c_d d1)
(var_cl (rsubst_cl (sr_trans t1) (c_trans t1)))) end.
Fixpoint Deriv_ok [d:deriv]:Prop:= Case d of
((couple_trans_ok (end_d d2) t2)^(Deriv_ok d2)^
->(rrange (sr_trans t2) x))) end.

Table

Derivations.
where t d stands for the last transition of d 0 , R stands for the initial
query of d, and r stands for the renaming substitution used during the
transition t 0 .
These denitions allow a full formalization of classical properties of SLD-
resolution, presented in the next section.
6 Basic Results
SLD-resolution denes a relation satisfying several important properties.
The logical ones are soundness and completeness; two others are particular
to this form of computation: lifting and switching lemmas.
6.1 Two Classical Lemmas
6.1.1 Switching Lemma
At each resolution step of a derivation, an atom and a clause must be selected.
The following well-known lemma ensures that the non-determinism in the
choice of atom does not matter: this is called don't care non-determinism.
Lemma 6.1 (Switching) If during a valid derivation, two atoms A and B
are successively selected, then they can also be selected in the reverse order,
and the derived states are the same up to renaming of variables.
This lemma is proved by decomposing the two transitions:
s id :R 0
| {z }
d
s id stands for the identity substitution tv). Furthermore, note that we suppose
the rst selected atom is before the second in the query R, that's why
we have the assumption This proof (presented in details
in [12]) is classical and requires constantly the denition of the variable renaming
process used during a valid derivation. Note that we explicitely
build, during this proof, the substitution relating the two derived states, thus
allowing us to assert they are the same up to renaming of variables.
6.1.2 Lifting Lemma
The lifting lemma is another classical lemma and is used during the completeness
proof. As we said in the introduction, we have to assume some
additional conditions on the renaming used, in order to prove it.
Lemma 6.2 (Lifting) Let d : s id :R
be a valid derivation. If for
a nite list of variables l, we have dom()  l, then if
there exists a valid derivation s id :R
, such that    and for a
query R f , we have R
The proof is obtained by induction over the derivation (using a left induction
schema). Here again, the two derived states are explicitely related (for
a full presentation, see [12]).
6.2 Two Fundamental Properties
Answers and solutions dene the operational and declarative semantics of
denite programs. Given a denite program P , the solutions to a query
(Deriv_ok d) -> (Fst (state_init_d d))=tv
-> (Snd (state_init_d d))=(subst_req eta r) ->
->(rrange (sr_trans t) x)))
(p_trans t1)=(p_trans
(Fst
(Ex [rf:request]((Snd (state_end_d d0))=
(subst_req (Fst (state_end_d d0)) rf)
^(Snd (state_end_d d))=(subst_req (Fst (state_end_d d))
(subst_req eta rf))))
([x:var](Subst_t (Fst (state_end_d d)) (eta x)))))).
(Deriv_ok (deriv_cons (deriv_cons d t1) t2)) ->
(le (plus (n_trans t1) (Length_r (body_c (c_trans t1))))
(n_trans t2))->
(Fst (state_init_d d))=tv ->
((Deriv_ok (deriv_cons (deriv_cons d
(n_trans t3)=(S (minus (n_trans t2)
(Length_r (body_c (c_trans
(n_trans t4)=(n_trans
((Snd (state_end_d (deriv_cons (deriv_cons d t1) t2)))=
(subst_req r (Snd (state_end_d
(deriv_cons (deriv_cons d
(Snd (state_end_d (deriv_cons (deriv_cons d t3) t4)))))
-> (Ex [v:var] (r x)=(tv v)))))))).

Table

7: Lifting and switching lemmas.
(Fst (state_init_d d))=tv->(Snd (state_end_d d))=true_req
(subst_req (answer d) (Snd (state_init_t (head_d d))))).

Table

8: Soundness of SLD-resolution.
R are the substitutions  such that h p (P )
a declarative description of the desired output from denite program and
query. Conversely, we have the operational counterpart of solutions: answers.
If s id :R
is a valid derivation, then  is an answer to R.
6.2.1 Soundness of SLD-Resolution
The soundness theorem expresses that every answer to a query is also a
solution to this query.
Theorem 6.1 (Soundness) If, from a denite program P and a query R,
we get the valid derivation s id :R
The corresponding proof requires the notion of free transitions that are
transitions satisfying the resolution rule, except that the uniers used are
not necessarily most general uniers. Hence, the condition  on the
initial step (condition (*H5*) of the predicate rsl) does not necessarily hold.
This proof is obtained by using a right induction schema.
6.2.2 Completeness of SLD-Resolution
Let us now turn to the formalization of the completeness theorem of SLD-
resolution: if there exists a solution to a query, then there exists an answer
to this query (in this form, the completeness theorem is weaker than the
usual completeness theorem and the soundness theorem proved; we don't
relate here the solution with the answer). As usual, we need some new
denitions and lemmas rst: let us summarize them.
First, we have to dene Herbrand interpretations with respect to our
denitions. For this, we dene inductively the collection, T [;], of ground
terms (Ground_Term) that are terms not containing variables, obtained using
only function symbols occurring in  (this set is also called the Herbrand
universe of ). At the same time, the collection, L n; [;], of lists of ground
terms is dened. Similarly, we dene the collection, At; [;], of ground
atoms (Ground_Atom), which can be formed by using relation symbols from
with ground terms from the Herbrand universe of  as arguments (this
set is also called the Herbrand base of  [ ). We also dene S ; , as the
collection of ground substitutions (Ground_subst), which are mappings from
X to T [;] (which exactly corresponds to T [;] X ). Furthermore, to avoid
some uninteresting complications, we assume, from now, on that the signature
used, contains a constant (i.e., a 0-ary function symbol); hence the
above-dened sets are nonempty (in [8], J. H. Gallier discusses in detail the
need for at least one constant symbol in  in the context of the Herbrand
theorem).
We can now introduce the denition of the Herbrand interpretation of
, written H. For every f 2 , f H is a mapping from ' ar(f) [T  [;]] to T [;]
dened by:
where tfg is the unique constructor of ground terms and where l 0 has type
L ar(f) [;] and is obtained from l. Using this denition, we dene a Herbrand
interpretation of  [  as any interpretation over ground terms, based on
H. Since, for Herbrand interpretations, the assignment to constants and
functions is xed, it seems possible to identify a Herbrand interpretation I
of  [  with a subset of the Herbrand base, dened by a predicate (I),
characterizing the ground atoms a, such that I a
However, note that specications of I and (I) correspond to two dierent
kinds of specications: whereas for every Herbrand interpretation I and
every ground atom a, the proposition I a is decidable, for an arbitrary
predicate p, on At; [;], the proposition p(a) is not. Therefore, later, we'll
use in theorems, which need to get from a predicate on the Herbrand base,
a Herbrand interpretation, this proposition as an assumption (we will use
this assumption only as a mapping). Fixpoint characterization of the least
Herbrand model and completeness of SLD-resolution will be obtained only at
the cost of this strong assumption, implicitly suggested by the identication:
Type.
For every denite program P , we dene, by a predicate on At; [;], the
least Herbrand model of P as the intersection of all Herbrand models of
According to this denition, and from [28], we prove that M
)g. We wish now to obtain a deeper characterization
of this set using xpoint concepts. To study Herbrand models of
denite programs, R. Kowalski and M. van Emden associate with a denite
program P , an operator T P , called immediate consequence operator, which
provides the link between the declarative and operational semantics of de-
nite programs. Using the identication of a Herbrand interpretation with a
predicate on the Herbrand base, this operator is usually dened as follows.

Table

9: Declarative semantics.
Let P be a denite program, p a predicate on At; [;], and a a ground atom.
If for a clause c occurring in P and for a ground substitution  such that
then T P (p)(a). Since it is possible that some variables occur in the atoms a 0 ,
we cannot establish the decidability of T P (p), even if we know the decidability
of p: given an (not necessarily ground) atom and a Herbrand interpretation,
we cannot prove the decidability of the proposition this interpretation is a
model of this atom, needed to evaluate atoms not necessarily ground in
the body of the clause aected by T P .
Since Herbrand models of a program P are exactly the xpoints of T P , we
formalize some results on operators and their xpoints in a general setting.
In fact, T P is easily seen to be upward continuous, and it follows that its
least xpoint is T "!
P and is equal to M P .
us turn to the denition of the operational counterpart of the
least Herbrand model: the success set of a denite program, dened by a
predicate S P on the Herbrand base, characterizing the ground atoms a such
that there exists an SLD-refutation starting from a. From the two technical
lemmas presented in the end of this section, we prove that M P is equal to
S P and, under the assumption that every predicate on the Herbrand base is
decidable, we are now in position to prove the completeness theorem in the
following
Theorem 6.2 (Completeness) Let P be a denite program, R a non-empty
query, and  a solution for R. Then, there exists the following valid
derivation s id :R
operators
A:Set Definition PA:=A->Prop. Definition OP:=PA->PA.
Inductive ORD:Set:= Fixpoint ORD_FIRST [n:nat]:ORD:=
Case n of
So:ORD->ORD | Oo
Lo:(nat->ORD)->ORD. [p:nat](So (ORD_FIRST p)) end.
ordinal powers
Fixpoint
[a:A]False
[l:(nat -> ORD)]([a:A](Ex [n:nat]((POF f (l n)) a))) end.
Theorem

Table

10: Operators, ordinals, and xpoints.
One delicate step in the proof of the completeness theorem of SLD-resolution
is concerned with the combination of derivations. This proof is made by
induction over the length of the initial query, and the following argument is
always used:
has a refutation
Because each B i is ground, these refutations
can be combined into a refutation of P [
However, the combination of derivations is rather delicate, and the way it is
made is never given. Therefore, during the formalization of this proof, we
need a (technical?) lemma, whose proof is not so immediate and shows (in
a constructive way) how the nal refutation can be obtained from the initial
ones. Of course, in order to ensure the good properties on the variables
occurring in the nal refutation, we have to be able to rename the initial
refutations.
First, we have the following variant lemma, which is seldom stated and
whose proof is not immediate. It is proved, in a dierent form, in [2] and
stated in [20].
Lemma 6.3 (Variant lemma) Let l be a list of variables and d 1 the valid
derivation s id :R
There exists a valid derivation d 2 :s id :R
Inductive Tp
Inductive Success [p:program;a:Ground_Atom]:Prop :=
(p_deriv d)=p->(Deriv_ok d)->
(Fst (state_init_d d))=tv->(Snd (state_init_d d))=
(cons_req (Ground_to_atom a) true_req)->
(Snd (state_end_d d))=true_req-> (Success p a).
(r=true_req)-> (semantic_csq (hp p) (subst_req s r))->
(Fst (state_init_d
(Snd (state_end_d

Table

11: Completeness of SLD-resolution.
, such that no variable occurring in #(d 2 ) also occurs in l. Furthermore,
for a substitution s r such that dom(s r )  var(R 2 ), we have R
In other words, the existence of a derivation does not depend on the choice
of variables in the input clauses: it suces that the good properties hold.
Formalizing this lemma consists in building from a derivation d 1 starting
from R and from a nite set of variables Z, a derivation d 2 starting from R
such that no variable occurring in the input clauses of d 2 also occurs in Z.
R
| {z }
Z
| {z }
d2
As a (not so immediate) consequence, we obtain the following lemma.
Lemma 6.4 (Combination of derivations) Let P be a denite program,
R be a nonempty query, and  be a ground substitution. If every atom
occurring in R is in the success set of P , then, there exists a valid derivation
s id :R
Proof. We proceed by induction over R. For
the claim is obvious. For induction hypothesis,
if for every atom a 0 occurring in r, we have S P (a 0 ), then, there exists a
valid derivation s id :r
Moreover, by hypothesis, we have
By the induction hypothesis, we can build the valid derivation
I :r ; . Furthermore, by (1), we have S P (a) and, by denition, there exists
a valid derivation d
is a ground substitution,
we can prove, by induction over d 1 , that d 3 : s id :c r (a; r)
I :a is also
a valid derivation. Moreover, from d 2 and by Lemma 6.3, there exists a
valid derivation d 4 :  I :a
now, by induction over d 4 , we prove that d 5 :  I :a
valid derivation. Hence, by combining d 3 and d 5 , which is suitable for the
renaming conditions, one obtains the valid derivation s id :c r (a; r)
which settles the claim.  J
As we said, the formal completeness proved is weaker than the soundness
theorem proved: we don't relate the solution and the answer. This is
due to the following usual step in the proof:
be the variables of N . Enrich the
language of P by adding new constants a
be
the substitution fx 1 =a there exists an SLD-
refutation of P [ f
Ng (. By textually replacing in this refutation
a i by x i , for
This operation cannot be formalized by using our denitions (which do not
allow to add new constants in the functional signature or to textually replace
constants by variables and variables by constants). A possible solution
could consists in following the S-semantics approach developed in [7] which
allows variables in the Herbrand base.
7 Related Work
As we said, unication on quasi-terms has been formalized by L. C. Paulson
[23] using LCF and by J. Rouyer [26] using Coq. Concerning the deni-
tion of partial functions, C. Dubois and V. Vigui Donzeau-Gouge [6] have
recently proposed a method to generate automatically the predicate P characterizing
the domain of a partial function f from equations dening f in
a ML-style. Formal completeness proofs have been envisaged by several au-
thors. In [24], H. Persson presents a formalization, using the proof assistant
ALF, of a constructive completeness proof for intuitionistic predicate logic
w.r.t. models based on formal topology. In [11], J. Harrison discusses about
a formalization in HOL of basic rst-order model theory including Compact-
ness, Lowenheim-Skolem, and Uniformity theorems. In [16], J. L. Krivine
describes a formalization in second order-logic of an intuitionistic proof of
the completeness theorem of classical logic. Finally, note that in the area of
formal methods applied in logic programming, and starting from an operational
semantics for Prolog, the soundness and completeness proofs, for
each renement step toward the Warren Abstract Machine (WAM), have
been elaborated by C. Pusch [25] with the theorem prover Isabelle.
8 Conclusion
In order to prove formal properties about programs, it seems natural to
give a formal specication of their operational semantics. In this paper,
SLD-resolution has been formalized, and fundamental properties of this form
of computation have been proved (more than 600 technical lemmas have
been formalized during this development). The proofs we have machine-checked
are based on those in J. W. Lloyd's book [19] and then follow the
traditional we have chosen to take as the referential notion of
truth a semantical notion (an alternative could consist in taking another
referential notion like natural deduction or sequent calculus).
First, note that, instead of proving the unication theorem by following
the unication algorithm, as J. Rouyer did in [26], we have used his result on
quasi-terms to obtain our proof. The technique proposed is based on dening
a bijection between terms and quasi-terms satisfying a given predicate and
proving the preservation of the unication property. In this way, we have
proved the unication theorem without really dealing with unication theory,
thus avoiding a heavy proof requiring to handle sophisticated theories. This
construction is interesting because it represents an alternative to the classical
approach and shows how to take advantage of a previous formal proof, even
if the desired proof does not exactly deal with the same objects. Whereas
the unication theorem proved in [26] requires about 140 lemmas, only 75
lemmas have been proved in our development. This shows the interest that
a proofs library be provided.
In more informal presentations, variable renaming problems are often
ignored; however, these problems cannot be ignored here. As we said, the
results presented here have been proved in the calculus of inductive construc-
tions, thus making a dierence from the proofs one can nd in many books on
logic programming. The main dierence is concerned with the renaming process
used in an SLD-derivation: it is made explicit. A typical example is the
proof of the completeness theorem that requires a variant lemma (assert-
ing that the existence of an SLD-derivation does not depend on the choice of
variables in the input clauses used) and a lemma on combination of derivations
(which explicitely allows this operation), whose proofs are rather long.
Therefore, formalizing SLD-resolution introduces many complications. Even
if combining some SLD-derivations seems to be easy, this operation requires
several renamings in order to satisfy the standardization apart assumed
in the denition of a derivation. Usually, these lemmas are implicitly used
without any proof or any justication. Not surprisingly, a full formalization
of these proofs shows how minor details could be crucial. An interesting
discussion about these ne points, at the foundations of logic programming,
can be found in [15, 27]. Of course, the main problems come from the explicitation
of all the substitutions used during the unication and the variable
renaming processes: substitutions are a quite hard matter to deal with. In
[1], substitutions are said to be the minence grise of the -calculus, this can
also be said for SLD-resolution. Like SLD-resolution for clauses; -reduction
denes a relation on -terms, and it may be interesting to compare properties
of these relations. Whereas -reduction is conuent, SLD-resolution
just satises the switching property, which is weaker than conuence. Fur-
thermore, the variables renaming process used in an SLD-derivation can be
viewed as an implicit -conversion: the renaming is made explicit and allows
a full formalization of SLD-derivations, along the lines of the calculus
of explicit substitutions [1] for -reduction (the -calculus is a renement
of the -calculus, where substitutions are manipulated explicitely, and provides
a link between the classical -calculus and concrete implementations).
The renaming process required in an SLD derivation plays an essential role
in logic programming: it constitutes an important step in the denition of
operational semantics of denite programs from the declarative reading of
Horn clauses.

Acknowledgments

Many thanks to Ren Lalement for enlightening discussions
about this work, as well as the anonymous referees for some very
useful comments.



--R

Explicit substitutions.
Logic programming.
Contributions to the theory of logic programming.
Project Coq.
The calculus of constructions.
A step towards the mechanization of partial functions: domains as inductive predicates.
Declarative modeling of the operational behavior of logic languages.
Logic for computer science
Proofs and Types
Edinburgh LCF
Formalizing basic
Formalization of SLD-resolution in the calculus of inductives constructions

Reynolds and Heyting models of logic programs.
In Koichi Furukawa
Une preuve formelle et intuitionniste du thorme de compltude de la logique classique.
Computation as Logic.

Foundations of Logic Programming.
Partial evaluation in logic program- ming
Deductive synthesis of the uni
Logic Programming and Prolog.
Verifying the uni
Constructive completeness of Intuitionistic Predicate Logic: A formalization in Type theory.

Dveloppement de l'algorithme d'uni
The role of standardising apart in logic programming.
The semantics of predicate logic as a programming language.
--TR

extracted:['file assignment' 'feedback controls' 'feature weights'
 'fault-tolerant software systems' 'fault-tolerant routing algorithm'
 'fault-tolerant routing' 'fault-tolerant algorithms' 'fault-tolerance'
 'flow analysis' 'constraint logic programming']
marked:['formal proofs', 'SLD-resolution', 'calculus of inductive constructions', 'semantics of logic programs', 'standardization apart']
--T
Some Lambda Calculus and Type Theory Formalized.
--A
We survey a substantial body of knowledge about lambda calculus and Pure Type Systems, formally developed in a constructive type theory using the LEGO proof system. On lambda calculus, we work up to an abstract, simplified proof of standardization for beta reduction that does not mention redex positions or residuals. Then we outline the meta theory of Pure Type Systems, leading to the strengthening lemma. One novelty is our use of named variables for the formalization. Along the way we point out what we feel has been learned about general issues of formalizing mathematics, emphasizing the search for formal definitions that are convenient for formal proof and convincingly represent the intended informal concepts.
--B
Introduction
"This paper is about our hobby." That is the first sentence of [MP93],
the first report on our formal development of lambda calculus and
type theory, written in autumn 1992. We have continued to pursue
this hobby on and off ever since, and have developed a substantial
body of formal knowledge, including Church-Rosser and standardization
theorems for beta reduction, and the basic theory of
Pure Type Systems ( PTS ) leading to the strengthening theorem and
type checking algorithms for PTS . Some of this work is reported
in [MP93, vBJMP94, Pol94b, Pol95]. In the present paper we survey
this work, including some new proofs, and point out what we feel
has been learned about the general issues of formalizing mathematics.
On the technical side, we describe an abstract, and simplified, proof of
standardization for beta reduction, not previously published, that does
not mention redex positions or residuals. On the general issues, we emphasize
the search for formal definitions that are convenient for formal
proof and convincingly represent the intended informal concepts.
The LEGO Proof Development System [LP92] was used to check
the work in an implementation of the Extended Calculus of Constructions
with inductive types [Luo94]. LEGO is a refinement style
proof checker, publicly available by ftp and WWW, with a User's Manual
[LP92] and a large collection of examples. Section 1.3 contains
Submitted to Journal of Automated Reasoning
y A version of this paper appears as technical report ECS-LFCS-97-359, University
of Edinburgh.
z Laboratory for Foundations of Computer Science, University of Edinburgh
x Basic Research in Computer Science, University of Aarhus. Centre of the Danish
National Research Foundation.
information on accessing the formal development described in this pa-
per. Other interesting examples formalized in LEGO include program
specification and data refinement [Luo91], strong normalization of System
F [Alt93], synthetic domain theory [Reu95, Reu96], and operational
semantics for imperative programs [Sch97].
1.1 Why?
PTS have a beautiful meta-theory, developed informally in [Bar92,
Ber90, GN91, vBJ93, Geu93]. These papers are unusually clear and
mathematical, and there is little doubt about the correctness of their re-
sults, so why write a machine-checked development? The informal presentations
leave many decisions unspecified and many facts unproved.
They are far from the detail of representation needed to write a computer
program for typechecking PTS , and the lemmas needed to prove
correctness of such a program. At the start, our long-term goal was
to fill these gaps in order to increase confidence in proofchecking programs
(such as LEGO) based on type theory. That goal is largely met
in [Pol95]. Also, while the basic informal theory of PTS is well under-
stood, the difficulties of formalization suggested reformulations which
clarify the presentation.
Another goal of the project is to develop a realistic example of formal
mathematics. In mathematics and computer science we do not
prove one big theorem and then throw away all the work leading up
to that theorem; we want to build a body of formal knowledge that
can continually be extended. This suggests some design criteria for
formalization. Representations and definitions must be suitable for the
whole development, not specialized for a single theorem. The theory
should be structured, like computer programs, by abstraction, providing
"isolation of components" so that several parts of the theory can
be worked on simultaneously, perhaps by several workers, and so that
the inevitable wrong decisions in underlying representations can later
be fixed without affecting too seriously a large theory that depends on
them. The body of knowledge we want to formalize is itself still grow-
ing, e.g. [vBJMP94] reports advances on typechecking for PTS done
later than our original formalization, that became part of our formal
development. The work on typechecking benefited from the basic formalization
of PTS , since proofs about several related systems could be
easily adapted from proofs already done for PTS . Further, new subjects
were included; e.g. the standardization theorem, not used in the
type theory, was formalized by the first author. On the other hand, we
do not claim that type theory is a realistic example for all formal math-
ematics: it is especially suitable for formalization because the objects
are inductively constructed, their properties are proved by induction
over structure, and there is little equality reasoning.
Perhaps the most compelling reason for our continuing fascination
with this work is the lure of completely concrete, yet simple, proofs
of results whose conventional presentation seems to require some notions
that are "messy" to formalize, e.g. the standardization theorem
discussed in section 3.4. We see such proofs as beautiful, both by their
simplicity and their concreteness. There is a tendency in formalization
to throw simplicity to the winds in frustration to get the proof to work
at all; but once it is checked, it can be beautified relatively easily, as
improved definitions and arguments are mechanically checked, easily
pointing out new glitches and suggesting how to fix them. Also, a formal
development is easy to come back to a year later, as all the details
you would not otherwise have written down are explicit, and don't have
to be rediscovered.
1.2 Related Work
There are many formalizations of the Church-Rosser theorem [Sha85,
Hue94, Nip96, Pfe92]; the only formalization of a standardization theorem
we know of is [Coq96a], for lazy combinator expressions. Formalizations
of type theory include [DB93, Bar96]; both of these address
limited aspects of very special type theories (essentially the Calculus of
Constructions), although [Bar96] is very interesting work in which the
program extraction mechanism of Coq is used to extract an executable
typechecker from a proof of decidability of typechecking. In contrast
to all the cited work except [Bar96], our development hasn't been terminated
by reaching one specified theorem, but continues to grow in
various directions guided by our interests, and by other work we come
across that we feel needs checking. For example, both authors have
checked parts of type theory papers we were asked to referee.
A novelty in our presentation is the use of named variables. Most
of the formalizations of type theory or lambda calculus that we know
of use de Bruijn indices ("nameless variables") [Sha85, Alt93, Hue94,
Nip96, Bar96] or higher order abstract syntax [Pfe92] to avoid formalizing
the renaming of variables to prevent unintended capture during
substitution. While de Bruijn notation is concrete and suitable for
formalization, there are reasons to formalize the theory with named
variables. For one thing, implementations must use names at some
level, whether internally or only for parsing and printing; in either
case this use of names must be formally explained. More interesting
is the insight to be gained into the meaning of binding. Many
researchers agree that de Bruijn representation "really is" what we informally
mean by lambda terms, in the sense that there is no need
to quotient terms by alpha-conversion, i.e. intensional equality on de
Bruijn terms corresponds with what is informally meant by identity
of terms. Nonetheless, de Bruijn representation is a coding of the informal
notion of binding, and doesn't address at all the relationship
between free and bound variables, namely how free variables become
bound. In our formalization, syntactic terms using named variables
are themselves concrete: the names of bound variables actually occur
(parametrically) in meta-formulas containing them, just as the names
of free variables do. This is done using a formulation suggested by Coquand
[Coq91], based on syntactically distinguishing free from bound
variables 1 . Other work on formalization of binding and substitution
using names includes [Coq96b, GM96, Owe95, Sat83, Sto88], but these
do not work out any large examples using their binding notions. It
would be interesting to compare our development with some similar
example using the terms up-to alpha conversion of [GM96]. A presentation
of type theory based on treating terms with named variables
concretely is Martin-L-of's calculus of explicit substitutions [Tas93], but
this presentation is not closed under alpha-conversion, as our presentation
is (section 5.5.3), and we view this as a failure of concreteness
of Martin-L-of's system.
1.3 This paper and the formal development
The source files for the development described in this paper, along
with a README file explaining how to check them, is available on the
LEGO WWW homepage http://www.dcs.ed.ac.uk/home/lego/.
LEGO uses a module system (described in [JP93]) based on
Cardelli's mock modules [Car91]. Each source file is a module, and each
module has a header saying which modules it depends on. Thus the
directory of modules associated with this paper contains parallel, and
even incompatible, developments. If you type Load strengthening,
the file strengthening.l (which contains the proof of strengthening
for PTS ) will be loaded, preceeded by every module it depends on 2 .
This distinction is already present in Gentzen [Gen69, pages 71-2, 116-7, 141,
216-7] and Prawitz [Pra65]
2 The dependencies are determined from the module headers, not by examining
There are over 70 proof source files with extension .l
containing
over 1500 definitions and lemmas. This is a large amount of formal
knowledge, which we can only survey here. This paper uses informal
mathematical notation, but almost every definition and lemma that
we mention is given with its formal name in typewriter font (often
in parentheses). You can then use grep to find the file in which it is
defined and the files in which it is used. This is not particularly elegant,
but it's how we do it too. Keeping track of a large amount of formal
knowledge is a serious problem that we have not addressed very well.
1.3.1 About notation
As mentioned, this paper uses informal notation, which is arrived at
by manually translating from the formal LEGO notation into L A T E X.
Further, the translation is not purely syntactical; we chose to surpress
some technical details to have a readable presentation. Errors are quite
likely, arising from both our translation and your interpretation. This
paper may be an informative outline of the formal work, but if you want
to believe one of our results you must read its formal statement, and all
the formal definitions used in its statement; see [Pol97] for discussion
of believing a large formal development.
In [MP93] we used formal notation, verbatim text manually extracted
from LEGO source files; no translation errors occur, but there
is no reason to believe the verbatim text in the paper actually appears
in the files. Indeed, the the document and the files drifted apart
over time. In [Pol94b] we again used formal notation, mechanically
extracting marked sections of the source files, following the idea of
Knuth's WEB. We could rerun the extraction to update the document
to the formal source, but many readers complained the document was
as unreadable as the formal source. Presenting a formal development
is a serious problem. Perhaps mechanical extraction with mechanical
translation to informal notation is the right direction to pursue.
For better or worse, we have sanitised this presentation so that very
little purely formal detail shows through. For example, we mostly sur-
press the distinction between boolean values and propositional values.
However, we don't want to hide the fact that formalization requires
many details that don't appear in informal presentations.
the actual dependencies in the files.
3 The .l files are the ones we wrote; LEGO generates "compiled" files with a .o
extension. These are the fully annotated -terms generated by the LEGO tactics
called in the .l file.
A few basic notations The development uses LEGO's built-in library
of impredicative definitions for the usual logical connectives and
their properties; we use standard notation for these connectives. Quantifiers
are typed in ECC, but we reserve symbols to range over certain
types, and drop the type labels almost everywhere; e.g. p will be reserved
to range over parameters, PP , so we
Well known computer science notations are used, e.g. if
as if-then-else, list( ) for the type of lists over , @ (or sometimes
just concatenation) for list append. All funtions of ECC are total (as
opposed to functions in the object theory of lambda terms and PTS ),
so some operations take extra arguments for a "failure value", e.g.
(assoc a b l) returns b if a is not the first element of a pair occurring
in l .
Pure Languages
In this section we discuss a formalization of the language of PTS ,
including terms, occurrences and substitution. We derive a strong induction
principle for well-formed terms.
A Pure Language ( PL ) is a triple (PP; VV; SS) where
ffl PP is an infinite set of parameters, ranged over by p , q , r ; these
are the global, or free, variables.
ffl VV is an infinite set of variables, ranged over by v , x , y ; these
are the local, or bound, variables.
ffl SS is a set of sorts, ranged over by s , t u ; these are the constants

PP , VV and SS have decidable equality. That PP and VV are infinite is
captured by the assumption that for every list of parameters (variables)
there exists a parameter (variable) not occurring in the list; e.g.
We are not assuming mathematical principles, but working parametrically
in types PP , VV and SS having the stated properties. These
can be instantiated with particular types that provably do have these
properties, e.g. the natural numbers, or lists over some finite enumeration
type. By working parametrically we are preserving abstractness:
only the stated properties are used in our proofs.
4 In this formula, member(p; l) is decidable because PP has decidable equality.
2.1 Terms
The terms of a PL , Trm , ranged over by M , N , A , . , E , a , b , are
given by the grammar
atoms: variable, parameter, sort
binders: lambda, pi
application
To be precise, Trm is inductively generated by six constructors: every
term can be thought of as a well-founded tree whose leaves are variables,
parameters and sorts, and whose interior nodes are lambda and pi
(having three branches each) and application (having two branches).
We often define functions on Trm by structural (primitive) recursion
over this inductive definition. As usual, we intend [v:A]B and fv:AgB
to bind v in B but not in A . However the intended binding structure
is not determined by the definition of Trm , but is made explicit by the
definitions of substitution and occurrence below. Equality on terms is
defined by recursion over Trm ; it inherits decidability from PP , VV and
SS .
Remark 2.1 (Notation) Often when doing case analysis by term
structure, we want to say that the binders, lambda and pi, behave the
same way. We introduce a notation hv:Aia to allow combining these
cases. The actual formalization does not have such a notation, but this
would have saved much cutting and pasting in developing the proofs.
The length of a term is used as a measure for well-founded induction

Two properties of this measure are used applications: if A is a proper
subterm of B then (used in induction on the
length of terms), and every term has positive length (used in reasoning
about PTS by induction on the sum of the lengths of the terms in a
context).
2.2 Occurrences of Parameters and Sorts
The list of parameters occurring in a term is computed by primitive
recursion over term structure, and the boolean judgement whether or
not a given parameter occurs in a given term is decided by the member
function on this list of parameters.
params(p) , [p]
params(hv:Aia) , params(A) @ params(a)
Similarly are defined.
2.3 Substitution
For the machinery on terms, we need two kinds of substitution, for
parameters and for variables, both defined by primitive recursion over
term structure. Write [a=p]M (formally psub) for substitution of a
for a parameter, p , in M . This is entirely textual, not preventing
capture. Since parameters have no binding instances in terms, there is
no hiding of a parameter name by a binder.
[a=p]q , if(p=q; a; q)
[a=p]ff , ff ff 2 VV; SS
[a=p]hv:Bib , hv:[a=p]Bi[a=p]b
Substitution of a for a variable, v , in M , written [a=v]M (formally
vsub), does respect hiding of bound instances from substitution, but
does not prevent capture.
[a=v]x , if(v=x; a; x)
[a=v]ff , ff ff 2 PP; SS
[a=v]hx:Bib , hx:[a=v]Biif(v=x; b; [a=v]b)
will be used only in safe ways in the type theory and
the theory of reduction and conversion, so as to prevent unintended
capture of variables. Note that these operations are total functions, and
do not rename variables. Also, occurrences of a in [a= ] are shared,
regardless of whether they occur within different binding scopes, in
contrast to the situation with de Bruijn indices.
Some important lemmas can now be proved:
Vcl-atom Vclosed(ff) ff 2 PP [ SS
Vcl-bind

Table

1: Inductive definition of the relation Vclosed .
and we have a ready supply of terms of the shape [p=v]M , with
Many other properties of these operations are proved in the formal
development.
2.4 No Free Occurrences of Variables
Intuitively parameters are the free names in terms; variables are intended
to be the bound names, and we do not consider terms with free
variables to be well formed. We define inductively a predicate Vclosed
(variable-closed) over terms (table 1). This is analagous to the way a
typing relation specifies another kind of well-formedness. (It will turn
out that every PTS -typable term is Vclosed ). Thus Vclosed is used
as an induction principle over well formed terms. As this relation is
a simple case of ideas that recur many times in what follows, we will
discuss it at some length.
Of course all terms of form s and p are Vclosed (rule Vcl-atom),
and no terms of shape v are Vclosed (there is no rule to introduce
Vclosed(v) ), but how do we define Vclosed for binders? The approach
to "going under binders" is a central idea of our formal handling
of names: for hv:Aia to be Vclosed , we require Vclosed(A)
and Vclosed( [p=v]a) for some parameter p . That is, to go under a
binder, first fill the hole with parameter, p . But p doesn't appear in
the conclusion of rule Vcl-bind; which parameter are we to use? In
the definition of Vclosed we say that any parameter will do, but there
is another possible choice: that Vclosed( [p=v]B ) be derivable for all
p . This is not a formal question; it is one of the tasks of a reader of formal
mathematics to decide if the formalisation correctly captures her
informal understanding. But a formaliser can help readers by pointing
out alternatives, and formally proving some relationship between them.
This is especially interesting when alternative definitions lead to easier
proofs in some cases. We will see this below for Vclosed .
Remark 2.2 Vclosed is equivalent to having no free variables
(Vclosed vclosed, vclosed Vclosed). This observation may be of
informal interest ("the definition of Vclosed is reasonable"), but we
do not use it formally because Vclosed allows us to avoid all talk of
Vclosed Generation Lemmas Suppose you have a proof of
examining it you know it must
be constructed by Vcl-bind from proofs of Vclosed(A) and
no other rule for Vclosed has a conclusion
of shape Vclosed( hv:AiB ) . The very fact that a relation is inductively
defined means that its judgements can only be derived by using
its rules. This is often called case analysis, and more generally, the lemmas
that express such properties are called generation lemmas [Bar92],
or inversion principles [DFH 93]. Note that inversion principles are
determined by the shape of a definition, not by its extension. LEGO has
new and very useful tactics to automate the use of inversion [McB96],
but most of what we describe in this paper was done before the tactics
were available. We will frequently use inversion on inductive definitions
in the rest of this paper without further comment.
The generation lemmas from the definition of Vclosed are
Notice how the existential quantifier in the case for binders expresses
the failure of the subformula property in Vclosed .
2.4.1 A better induction principle for Vclosed .
Here are three "obvious" facts about Vclosed (alpha Vclosed lem,
Vclosed alpha).
They are all directly provable, but appear to need length induction
(which appeals to well-founded induction and then subsidiary case
analysis; e.g. the proof of claim aVclosed alpha below), for the usual
reason that statements about change of names are proved by length induction
rather than structural induction: e.g. [q=v]M is not generally
a subterm of ( M N ) , but it is shorter than ( M N ) . We will derive
a new induction principle which packages up such arguments once and
for all.
Consider an alternative definition, called aVclosed , differing
only in the rule for binders, in which the right premise requires
aVcl-bind
We will show that Vclosed and aVclosed derive the same judgements.
Induction over aVclosed is the principle which Melham and Gordon
rediscovered [GM96, Section 3.2].
It is worth saying that Vclosed is a type of finitely branching
well-founded trees; i.e. Vcl-atom are the leaves, and Vcl-bind and
Vcl-app are binary branching nodes. On the other hand, aVclosed
contains infinitely branching well-founded trees, where aVcl-bind creates
a branch for each parameter p . Notice also that for any term, A ,
there is at most one derivation of aVclosed(A) , while there may be
many derivations of Vclosed(A) , differing in the parameters used in
the left premises of instances of aVcl-bind.
Equivalence of Vclosed and aVclosed (aVclosed Vclosed,
Vclosed aVclosed)
Both directions follow easily by structural inductions once we have the
following claim (aVclosed alpha):
Proof. The claim is proved by induction on lngth(B) . This works
because every term appearing in a premise of a rule of aVclosed is
shorter than the term appearing in its conclusion; the typing relations
to be considered later do not have this property, and more subtle proofs
will be required (section 5.2.1).
By well-founded induction on lngth(B) , we have the goal
Now using term structural induction on A , we have cases for sort, vari-
able, parameter, binder and application (only case analysis is necessary
here; we don't use the structural induction hypotheses). Consider the
case for binder: we must show aVclosed( [q=v] hn:AiB ) , i.e.
under the assumptions
(i.e. aVclosed(hn:[p=v]Aiif(v=n; B; [p=v]B)))
By aVclosed inversion applied to assumption vclp we also know
By aVcl-bind, it suffices to show
Noticing that [p=v] doesn't change lngth, the first of these holds
by ih and h1. For the second, let r be an arbitrary parameter, and
consider cases. If then we are done by h2; i.e. [q=v]B doesn't
actually appear in the goal, and [p=v]B doesn't actually appear in h2.
Finally the interesting case: if v 6= n we use a straightfoward lemma
(alpha commutes alpha)
to rewrite the goal to aVclosed( [q=v] [r=n]B ) . By ih it suffices to show
aVclosed( [p=v] [r=n]B ) , which follows by h2 after again rewriting the
order of substituting p and r .
What have we gained? By defining aVclosed and showing it to
be extensionally equivalent to Vclosed , we can view the induction
principle of aVclosed as an induction principle for the extension of
Vclosed , and this is clearly stronger than the induction principle of
Vclosed . We insist on extension to point out that aVclosed-induction
may be used to prove statements about the judgement Vclosed , but
not about derivations of the judgement.
Notice that we could directly prove the analogue of claim
aVclosed alpha for Vclosed (the proof outlined above works), but
it is not just the stronger premises of aVclosed we are after (i.e. the
generation lemmas), it is the stronger induction hypotheses.
2.5 A Technical Digression: Renamings
are sequential operations; we have not used a notion
of simultaneous substitution, except in the following special case. A renaming
is a finite function from parameters to parameters. Renamings
are represented formally by their graphs as lists of ordered pairs.
rp , PP \Theta PP (renaming pair)
Renaming , list(rp)
ae and oe range over renamings. The action of a renaming (renTrm)
on parameters is by lookup in the representing list, and is extended
compositionally to all terms.
aep , (assoc p p ae)
aeff , ff (ff 2 VV; SS)
aehv:Aia , hv:aeAiaea
ae(M N) , ae(M) ae(N)
This is a "tricky" representation. First, if there is no pair (p; q) in ae ,
returns p , so the action of a renaming is always total,
with finite support. Also, while there is no assumption that renamings
are the graphs of functional relations, the action of a renaming is func-
tional, because assoc finds the first matching pair. Conversely, consing
a new pair to the front of a renaming will "shadow" any old pair with
the same first component. We do not formalize these observations.
Renamings commute with substitution in a natural way:
Renaming is iterated substitution. We can analyse the action of
a renaming in terms of substitution (renTrm is conjugated psub):
From this lemma it is easy to show that renaming respects any relation
that substitution of parameters respects (psub resp renTrm resp):
Similar results hold for n -ary relations R .
Injective and Surjective Renamings It is useful to have bijective
renamings (e.g. in Section 5.2.1). The definitions are standard:
It is surprisingly difficult to construct bijective renamings in general
because of the trickiness of the representation mentioned above. However
it's clear that any renaming that only swaps parameters is bijective
(swap sur, swap inj), and this is enough for our purposes:
3 Reduction and Conversion
In this section we outline the theory of reduction and conversion of
Pure Languages. The main results are the Church-Rosser and standardization
theorems.
As in the definition of Vclosed (Section 2.4), the interesting point
in defining reduction is how the relation goes under binders. To understand
how reduction works, consider informally one-step beta-reduction
of untyped lambda calculus. In our style the fi and - rules are:
The substitution [N=x]M on the RHS of fi does not prevent capture,
so some restriction is required. It is obvious that no capture can occur
if N is closed in the usual informal sense, but because we distinguish
between parameters and variables it is enough that N be Vclosed .
This is no actual restriction: we will only want to reason about Vclosed
terms anyway, as these are the "well-formed" terms.
To use fi under a binder, as allowed by - , we must preserve the
invariant that fi is only applied to Vclosed terms: we fill the "holes"
left by stripping off the binder with a fresh parameter. Here is an
instance of - where incorrect capture might occur (contracting the
underlined redex):
After removing the outer binder -x , replacing its bound instances by a
fresh parameter, q , and contracting the Vclosed redex thus obtained,
we must re-bind the hole now occupied by q . (Since q was fresh, all
instances of q mark holes that should be re-bound). According to - ,
we require a variable, y , and a term, N , such that [q=y]N is the
contractum of the Vclosed redex, -x:q in the example. Such a pair
is y , -x:y (the one we have used above), as is z , -x:z for any z 6= x .
However - does not derive the incorrect judgement
because
Thus incorrect capture is avoided.
3.1 Parallel Reduction
Rather than use ordinary fi -reduction, we take parallel reduction ('a
la Tait-Martin-L-of) as the basic reduction relation. Parallel reduction
is convenient for the Church-Rosser and standardization theorems, as
emphasised by Takahashi in her beautiful account [Tak95]. Our development
follows that of [Tak95], with some refinements.
3.1.1 One-step parallel reduction
This relation, ![
(par red1), is defined in Table 2. As in Vclosed
above, the dependence of the congruence rule for binders on the choice
of a parameter p is only apparent. However, something new arises here,
namely the side conditions p 62 . These are eigenvariable
pr1-atom ff ![
pr1-beta
A ![
Vclosed(U)
pr1-bind
A ![
pr1-app
A ![

Table

2: 1-Step Parallel Reduction
conditions 5 , which ensure that the parameter p correctly indicates the
position of the bound variables in the compound terms.
Only Vclosed terms participate in ![
(par red1 Vclosed)
and ![
is reflexive on Vclosed terms (par red1 refl)
A stronger induction principle for ![
The rules pr1-atom, pr1-
bind and pr1-app are the congruence rules for our language. As with
Vclosed (section 2.4.1), we introduce a strong congruence rule for
binders
~
pr1 -bind
A ~
and prove that ![
and ~
are extensionally equivalent, giveing us
stronger induction and inversion principles. Because of the eigenvariable
conditions in pr1-bind, a technique using renamings is required
5 Kleene [Kle52, x78, on the notion of "pure variable" proof] explains how to
treat such conditions; however, to do so he must explicitly consider operations on
derivations, hence dependent elimination, whereas our methods require only rule
induction, i.e. non-dependent elimination. The second author is grateful to N.
Shankar for this reference.
to show the equivalence. We omit the details, but a similar argument
is used in section 5.2.1.
The strong induction principle is used to show that ![
is closed
under substitution (par red1 psub):
Many-step parallel reduction !
(par redn), is the transitive
closure of ![
. It inherits properties par redn Vclosed and
par redn refl from the corresponding properties of ![
mentioned
above.
3.1.2 Alpha-Conversion
We define ff -conversion, ff
- , to be the least congruence, i.e. ff
- is exactly
without the rule pr1-beta, so ff
. This definition is
symmetric, by inspection. To show that it is transitive requires the
stronger induction principle for ff
- , which we prove in the same way as
above. Hence ff
- is an equivalence relation 6 . It is decidable for Vclosed
terms (decide alpha conv):
ff
with a straightfoward but messy proof, by double induction on
and aVclosed(B) .
Informally, alpha-conversion is used for changing the names of vari-
ables. We do not have hx:AiB ff
does
not prevent capture. However, we do have (true alpha conv pi):
Closure under ff -conversion One of Coquand's original motivations
for distinguishing between variables and parameters was to avoid
the need to reason about ff -conversion; many of the arguments below
(Church-Rosser, standardisation, subject reduction) achieve this goal.
Name-carrying syntax is regarded as an abbreviation for a quotient
modulo ff -conversion, so that when we formalise a relation R such
as parallel reduction above, we really intend R modulo the quotient
structure, i.e. ff
- . We say a relation R is:
6 This should be contrasted with Gallier's meticulous but long-winded treatment
in [Gal90].
closed under ff if ffffiR ' Rffiff ; strongly closed, if ffffiR ' R ;
full wrt ff if Rffiff ' ffffiR ; strongly full, if Rffiff ' R .
Remark 3.1 ![
is strongly closed under ff -conversion: the proof is
the same as that for transitivity of ff
- , with the additional case of a
redex handled by observing that an ff -variant of a redex is a redex.
However ![
is not full w.r.t. ff
-classes. For example
but no ff -variant of the LHS ![
-reduces to ( [y 1 :q]y 1
although this is an ff -variant of the RHS.
3.1.3 A Church-Rosser Theorem
Using the argument of Tait and Martin-L-of, as modernized in [Tak95],
we prove the first CR theorem (par redn DP):
by the usual strip lemma argument and the diamond property of ![
(comp dev par red1 DP).
To do so, we introduce an inductive characterisation of complete
development, \Gamma![
, (comp dev)
7 . It is given by the same rules as ![
except for the application rule:
cd-app
A \Gamma![
AB \Gamma![
A is not a lambda
The side condition in cd-app forces contraction of all redexes: we have
a deterministic sub-relation of ![ .
The theorem on finiteness of developments now becomes the combination
of:
ffl induction on the definition of \Gamma![
, which we may think of as a
partial correctness assertion;
ffl the existence (for Vclosed terms) of complete developments,
(comp dev exists), which we may think of as a termination argument

7 cf. the definition of \Gamma![
as a function by structural recursion on terms [Tak95]
This separation of concerns gives us an advantage over Takahashi's
informal proofs, in that we do not have to consider, in each proof about
\Gamma![
, a subsidiary induction (case-analysis) to resolve the redex/non-
redex distinction in the case of an application. This is handled once
and for all in the existence proof, while induction on the definition of
\Gamma![
already delineates the redex/non-redex distinction. The price we
pay is that we no longer work with an object-level function, but rather
a functional relation.
Of course, we have simplified matters by considering developments
of the entire set of redexes in a term: this is sufficient for our purposes,
but a more refined analysis (e.g. [Hue94]) would take us beyond our
simple datastructure of terms.
The diamond property (comp dev par red1 DP) follows easily from
the following "Takahashi" lemma (comp dev preCR):
whose proof is by induction on M \Gamma![ M 0 , with inversion of M![ N . As
usual, the interesting case, of a redex/redex, appeals to par red1 psub.
Remark 3.2 These proofs do not make any appeal to ff -conversion.
This is because both the ![
and \Gamma![
relations are strongly closed under
ff -conversion. Indeed, we may show the following two properties, which
strengthen comp dev exists, namely comp dev unique:
and comp dev exists unique:
3.2 Conversion
We define conversion, ' (conv), as the symmetric and transitive closure
of !
. It inherits properties conv Vclosed and conv refl from
those of !
mentioned above.
The second Church-Rosser theorem is now straightfoward to
prove for conversion (convCR):
3.3 Normal Forms
A term is beta normal (beta norm), if it has no beta redexes . This
may be defined with the same rules as aVclosed , except the rule for
application, which is
bn-app beta norm (A) beta norm (B)
beta norm ( AB )
A is not a lambda
All beta norm terms are Vclosed (beta norm Vclosed). A relation
of reduction to normal form is defined:
A # N , beta norm (N) - A !
(normal
is reflexive, so there is reduction from a normal form, but every
reduct of a normal form is a normal form (par-redn-bnorm-is-bnorm)
Any reduct of a normal form alpha-converts with that normal form
(par-redn-bnorm-is-alpha-conv)
Hence, by Church-Rosser, normal forms of a term are unique up to
alpha-conversion (nf-unique). Since ff
, the converse also holds
(nf-alpha-class)
Thus the class of normal forms of a ( Vclosed ) term is either empty or
exactly an alpha-conversion equivalence class.
Deciding conversion Conversion is decidable for normalizing terms.
The proof of this depends on Church-Rosser; since normal forms are
unique only up to alpha-conversion, it also depends on decidability of
alpha-conversion (Section 3.1.2).
3.4 The Standardization Theorem
Our work on type-checking requires us to go beyond theorems such
as Church-Rosser in the analysis of reduction. In particular, to talk
of syntax-directed systems, we must consider deterministic reduction
relations, of which weak-head reduction is the simplest. A typical
A !wh A 0

Table

3: One step of weak-head reduction
property required of such a relation is the following counterpart to the
quasi-normalisation theorem (wh standardisation lemma):
Takahashi showed how to approach such theorems with an analysis of
parallel reduction into head reduction followed by internal reduction, a
so-called semi-standardization lemma [Mit79]. We adapted her methods
to the case of weak-head reduction, and the corresponding modified
notion of internal reduction. In doing so we simplify them somewhat,
in particular by removing the need for the complex invariant M ? N .
Moreover, the arguments we employed can be replicated in the context
of head reduction and internal reduction in their classical senses.
Recently, we rounded off this line of development by proving a standardization
theorem for pure languages. The main novelty here is the
removal of any mention of residuals (so the reader may be unconvinced
that we have formalised the standardization theorem). The other thing
to observe is that all the desirable consequences of standardization,
which we required to analyse type-checking, such as the lemma above,
are already corollaries to the semi-standardization lemma.
There are three main ingredients to the theorem: weak-head reduc-
tion, internal parallel reduction, and standard reduction itself.
3.4.1 Weak-head reduction
One step of weak-head reduction (wh red1) is shown in Table 3. By
inversion, we see that there are no weak-head reducts of a lambda, so
we may assume without loss of generality that A is not a lambda in the
rule wh1-app. We have not built any Vclosed assumptions into the
definition, as this will always be used in a context in which all terms
are Vclosed .
The reader may validate such a definition by considering the weak-
head normal forms (Table 4), and various lemmas relating !wh and
wh-atom whnf(ff) ff 2 PP [ SS
wh-bind
A is not a lambda

Table

4: Weak-head normal forms
ip1-atom ff![
ip1-bind

Table

5: One step of internal parallel reduction
whnf (wh red1 determin, wh nf is nf1, alpha conv resp wh nf).
Many-step weak-head reduction, iwh , (wh redn) is defined as
the reflexive transitive closure of !wh . It is closed under renam-
ings, substitution (psub resp wh redn), and application on the right
3.4.2 Internal parallel reduction
The classical notion of head reduction leads to a notion of "internal"
redex, as any non-head redex. We adapt such a notion to weak-head re-
duction, which gives us the definition of internal parallel reduction, ![
(ipar red1) as shown in Table 5. We allow arbitrary parallel reduction
in each compound term, except in the rator position of applications,
where we restrict to internal reduction.
It is immediate by structural induction that internal parallel reductions
are parallel reductions. We also have the important abstract
property (ipar red1 refl wh nf) that ![
preserves and reflects weak-
head normal forms, and a fortiori , the shape (outermost constructor)
of a term. This reflection of weak-head normal forms, together with
the lemma below, is the key to the proof of the quasi-normalisation
result with which we opened this discussion.
Semi-standardization (par red1 wh redn ipar red1)
Proof. The proof proceeds by induction on M![
. The only tricky
case is that of the parallel fi step. By inductive hypothesis, we obtain
(introducing Skolem constants Aw ; Bw )
and we are required to show that there exists some Mw such that
Since
by psub resp wh redn and psub is vsub alpha, we may conclude the
result by stitching weak-head reduction sequences together, provided
we can establish the following claim, which is the easy base case of
Lemma 2.4 in [Tak95] (wh ipar red1 psub):
This is proved in the same way as we showed closure of parallel reduction
under substitution, by induction on M![
. A detail to observe
here is that we must explicitly assume that the reduction from N to
N 0 is parallel. Takahashi builds this into her ? invariant, whereas in
the use of wh ipar red1 psub, we obtain this assumption for free as
the premise associated with reduction in A .
We show the case of an application
and B![
. By induction hypothesis, there exists some PA such
that [N=p]A iwh PA![
. The proof of the claim, and hence
of the whole lemma, is completed by taking P= def PA ( [N=p]B ) ,
and appealing once more to psub resp wh redn, ipar red1 app and
par red1 psub.
To establish the full semi-standardization result for !
, we must
also show the commutation result (ipar wh redn commutes):
This is a corollary (by induction on Mw iwh M 0 ) of the following
lemma (ipar wh red1 commutes):
Proof. Induction on M![
of the ancillary hypothesis
All cases are trivial, except that of application
. We show
the case of a wh-redex, where we have:
and We use the fact that ipar red1 reflects the
weak-head normal form A to infer that A = [u:U ]A 000 .
Moreover, since A![
by inversion that U![
V and
8p: [p=u]A 000 ![
[p=v]A 00 . Choosing p 62 A 00 ; p 62 A 000 , and applying
pr1-sub, we obtain [B=u]A 000 ![
Now we appeal to
Lemma 3.4.2, finally to conclude that for some P , we have
as required.
Throughout we used induction on the definitions of the various reduction
relations to establish these lemmas. This is by contrast with
Takahashi's treatment, where induction is on the term structure (with
inversion of the relational hypotheses). This leads to slightly weaker
arguments, and consequently to the need for stronger inductive in-
variants. Such refinements to these arguments would be inconceivable
without machine support.
3.4.3 Standard Reduction
The property of being a standard reduction, \Gamma! s , is usually stated
(e.g. by Mitschke [Mit79]) in terms of a highly intensional geometric
definition on -terms. To formalise this definition directly, we would
have to enrich the datatype of terms in order to be able to speak of
std-atom ff\Gamma! s ff ff 2 PP [ SS
std-bind
std-app
std-wh

Table

Standard reduction, standard (adapted from Plotkin)
redex positions in a term. Such an approach has been taken in [Hue94];
we have chosen instead a presentation (table 6), adapted from Plotkin's
notion of standard sequence [Plo75]. The essence of this presentation
is to define standard reduction as the least congruence closed under
prefixing by weak-head reductions. We leave implicit the sequence
of redexes contracted, as this may be computed by recursion, and its
"left-to-right" character, and thus avoid mention of residuals or redex
positions.
Remark 3.3 We have defined standard reductions of arbitrary length
once and for all, and without any considerations of reduction to normal
note furthermore, however, that we may easily achieve this end
by adding the side condition A 0 6= - to the std-app rule, and whnf(B)
to the std-wh rule. Also, this definition is both strongly closed under
ff -conversion, and strongly full.
Our final aim is the following standardization theorem
(the standardisation lemma):
Induction on A !
suffices if we can show that our notion of standard
reduction absorbs single steps of parallel reduction:
By our Lemma 3.4.2, and std-wh, it suffices to prove the following
lemma, that standard reduction absorbs single steps of internal parallel
reduction (standard absorbs ipar red1):
Proof. The proof is by induction on B \Gamma! s C . This avoids reconsidering
the tricky application case of the commutation lemma above, and
we may exploit the fact
preserves and reflects the shape of
terms. The price we pay is the need for strong induction on B \Gamma! s C ,
as the ancillary hypothesis A![
then an expansion step. In each
non-atomic case, we make a subsidiary appeal to semi-standardization,
in order to be able to exploit the induction hypotheses. This gives a
rather mechanical and "abstract nonsense" flavour to the argument,
emphasising once more that the real complexity lies in the the proof of
Lemma 3.4.2.
We focus on the case of a binder, where we have as hypotheses:
By inversion of H, we conclude that
A
and 8p: [p=w]B c ![
[p=u]B . But now by semi-standardization,
A c iwh Aw![
. So by induction
hypothesis ihA, we have Aw \Gamma! s A 0 , and using std-wh to fold back the
reductions A c iwh Aw , we finally obtain A c \Gamma! s A 0 . In exactly
the same way, modulo the choice of a parameter
we obtain [p=w]B c \Gamma! s [p=v]B 0 . But now C \Gamma! s hv:A 0 iB 0 by construction

This concludes the proof that standard reduction absorbs internal
reduction, and hence parallel reduction, and so finally we may conclude
that every (parallel) fi -reduction sequence may be standardized.
4 Pure Type Systems
PTS is a class of type theories given by a set of derivation rules (table 7)
parameterized by a PL , (PP; VV; SS) , and by two relations
informally as ax(s 1 :s 2 )
Start
Weak
Lda
App

Table

7: The Typing Rules of PTS (formal name gts)
ffl rules, rl ' SS \Theta SS \Theta SS , written informally as rl(s
We usually intend ax and rl to be decidable, but this assumption is
not used in the basic theory of PTS . When we are interested in algorithms
for typechecking ([Pol94b, Pol95]), even stronger assumptions
about decidability are needed.
The typing judgement has the shape meaning that in
has type A . (The formal name of this relation
is gts, from the old name "Generalized Type Systems".) We call M
(or (\Gamma; M) ) the subject and A the predicate of judgement. Contexts,
ranged over by \Gamma , \Delta , bind parameters to types:
Contexts are formalized as lists over PP \Theta Trm . If \Gamma participates in
some derivable judgement,
4.1 A Generalization: Abstract Conversion
We have further generalized PTS by parameterizing the rules of table
7 on another relation, - (called abstract conversion), occurring
in the side condition of rule tConv. In conventional presentations of
PTS [Bar92], the actual relation of beta-conversion ( ' ) is used for - .
There are several reasons to be interested in parameterizing PTS
on its conversion relation. For one thing, the type theory ECC, implemented
in LEGO, is not actually a PTS because it uses a generalized
notion of conversion called cumulativity. ECC is of special interest to
us, so we formalize an extension of PTS which includes ECC. Our formal
development includes a typechecking algorithm for ECC [Pol94b].
Even for PTS , there is a notorious open problem, the Expansion Postponement
problem [vBJMP94, Pol94b], which asks if the conversion
relation in table 7 can be replaced by beta-reduction without changing
the typability of any terms. We know of one other work on PTS using
an abstract conversion relation: [BM].
The only properties of - necessary to prove the substitution lemma
(section 5.4) are reflexivity, transitivity, and invariance under substitution

cnv trans 8A;
To prove the subject reduction theorem (section 5.5), we finally need
that conversion is related to contraction of redexes, and has an "inter-
nal" Church-Rosser property:
Notice the contravariance in the last property. It is easy to prove that
' has these properties, so we can formally instantiate - by ' (or by
the cumulativity of ECC) and discharge all these assumptions; we are
working abstractly, not making assumptions.
' is an equivalence relation, but - is a partial order (which is why
we use an asymmetric symbol for - ). Other significant properties of
' that do not generally hold for - include:
Some differences between abstract-conversion- PTS and fi -conversion-
are detailed in [Pol94b]. This kind of analysis, of which properties
are actually used in some body of work, is greatly aided by formalization

4.2 Are the rules a formalization of PTS?
Leaving aside abstract conversion, the rules of table 7 differ from the
standard informal presentation [Bar92] in several ways. First, the handling
of parameters and variables in the Pi and Lda rules, is similar
to that in the rules of table 2. Other differences are restriction of
weakening to atomic subjects, and the Lda rule.
Binding and substitution The treatment of operating under
binders in table 7 is analogous to that in the reduction relations considered
above. (See discussion of the rule Lda below.) As the substitution
used in rule App may cause capture of variables, we must show that
N is Vclosed . In fact we have (gts Vclosed lem)
by structural induction. It also follows easily that a Valid context is
Vclosed in an obvious sense.
Atomic Weakening The standard presentation of weakening in
PTS allows any term as subject
(weakening)
where we restrict to atomic terms, ff (see rule Weak). Our rules
derive the same judgements (weakening is seen to be admissible in section
5.3), but allow fewer derivations (those derivations where weakening
is pushed to the leaves). This gives a cleaner meta theory, as
induction over derivations treats fewer cases. For example, given a
with an application as its subject, there
is no confusion whether it is derived by App or Weak. Thus, with
atomic weakening, any judgement may only be derived by tConv or
by exactly one of the remaining rules.
The Lambda Rule For the rule for typing a lambda in informal
presentations [Bar92, Geu93] is
The conventional understanding is that the bound variable, x , doesn't
really occur in the conclusion of rule - , as the notations "
refer to alpha-equivalence classes 8 . Thus, in concrete
notation, the subject and predicate of the lambda rule may bind different
variables, which we formalize in our rule Lda. One might, instead,
rule - as
This was our first attempt, and surprisingly, this system derives the
same judgements as the system of table 7 (lemmas rlts gts and
gts rlts). However, using Lda', the subject reduction theorem is
difficult to prove, and derivations are distorted by the need to use the
conversion rule for alpha-conversion. See [Pol94b] for more details.
5 PTS With Abstract Conversion
We survey the development leading to the subject reduction theo-
rem. The main difference between this section and the presentation
in [Bar92] is our use of the atomic weakening rules (section 4.2), and
our simpler proof of subject reduction (section 5.5).
5.1 Some basic facts
Here is a sample of the many small facts that had to be established,
usually by simple structural induction.
Parameter lemmas All parameter occurrences in a judgement are
bound in the context, and the binding instances in a Valid context are
distinct parameters:
(free params lem1)
8 However, in the left premise of rule - , all free instances of the actual symbol
are intended to refer to the context entry [x:A] . Thus the
conventional reading of this rule doesn't make sense as concrete notation.
Start Lemmas Every axiom is derivable in every valid context, and
the global bindings of a valid context are all derivable:
5.2 A Better induction principle for PTS
As for previous relations, we define an alternative typing relation, ' a
(apts), that identifies all those derivations of each ' judgement that
are inessentially different because of parameters occurring in the derivation
but not in its conclusion. ' a
differs from ' only in the right
premise of the Pi rule and the left premise of the Lda rule.
aLda
In these premises we avoid choosing a particular parameter by requiring
the premise to hold for all parameters for which there is no reason it
should not hold, that is, for all "sufficiently fresh" parameters. As
before, we will show that ' and ' a
derive the same judgements.
It is interesting to compare the side conditions of Pi with those of
aPi. In Pi we need the side condition p 62 B so that no unintended
occurrences of p (i.e. those not arising from occurrences of the variable
x ) are bound in the right premise; we do not need p because the
validity of \Gamma[p:A] is obvious from the right premise. In aPi, we cannot
require the right premise for all p , but only for those such that \Gamma[p:A]
remains valid, i.e. those p not occurring in \Gamma . However the condition
not required because of "genericity", that is, the right premise
of aPi must hold for the infinitely many parameters not occurring in
only finitely many of these instances can occur in B .
5.2.1 ' a
is equivalent to '
As with previous relations, this equivalence will give us a stronger induction
principle, and stronger generation (inversion) lemmas for ' .
(apts gts; gts apts)
Proof. Direction ) is straightfoward by structural induction on
To prove direction ( , first prove a lemma that bijective renamings
respect ' a
(bij ren resp apts)
by ' a
-structural induction 9 .
Now we proceed to prove
by structural
induction on a derivation of All cases are trivial except
for the rules Pi and Lda. Consider the case for Pi: we must prove
under the assumptions
l prem
prem
By rule aPi (using l ih) it suffices to show \Gamma[r:A] ' a
arbitrary parameter r 62 \Gamma . Thus, using the free parameter lemmas of
section 5.1, we know
A from l prem and noccG
prem
prem
Taking
derivable using bij-ren-resp-apts to rename r ih. (Recall from section
2.5 that swap(p; q) is always bijective.) Thus we are finished if we
can show
It is clear that the first equation holds from nopG, noccG, norA and
nopA. For the second equation, notice that if then we are done
trivially, so assume r 6= p , and hence r 62 [p=n]B (from r prem and
noccG). Now, using vsub renTrm commutes (section 2.5) we have
9 Actually injectivity of a renaming is enough for it to preserve ' a , but we cannot
prove this until after we know ' a = ' .
as required.
5.3 The Thinning Lemma and Weakening
The Thinning Lemma is important to our formulation because it shows
that full weakening (weakening) is admissible in our system, justifying
our use of atomic weakening in the definition of ' (section 4.2).
The subcontext relation is defined
We also say \Delta extends \Gamma . This is the definition used informally
in [Bar92, GN91, vBJ93]; a much more complicated definition is required
to express this property in a representation using de Bruijn
indices for global variables. Now we can state (thinning lemma):
A naive attempt to prove the thinning lemma by structural induction
encounters serious difficulties with parameter side conditions
(see [MP93, Pol94b] for discussion), but a proof is straightfoward
using ' a
-induction, justified by the previous section. The full weakening
rule is a corollary of thinning lemma.
5.4 Cut and type correctness
The substitution lemma
(substitution lemma)
is proved by induction on the derivation of \Gamma[p : From
this we get the commonly used case (cut rule) by instantiating \Delta to
the empty context.
Among the correctness criteria for type systems is that every type is
itself well formed. In PTS we have the theorem (type correctness):
The proof is by structural induction; the only non-trivial case is rule
App, which uses the substitution lemma and vsub-is-psub-alpha
(section 2.3).
Subject Reduction Theorem: Closure Under Reductio

An important property of type systems is that a term does not lose
types under reduction, thus types are a classification of terms preserved
by computation. In fact we will show entire ' -judgements are closed
under reduction. We now need all five properties of abstract conversion
(section 4.1).
5.5.1 Non-Overlapping Reduction
Our goal is to prove
usually called the subject reduction theorem. Our naive strategy is
to show that one step of reduction preserves typing, by induction on
. The critical case is rule App, when the application is
actually a redex that is contracted. In order to simplify that case,
we want to avoid overlapping redexes, as allowed in the fi -rule of ![
We want some reduction relation with no overlapping, whose transitive
closure is equal to !
Another difficulty is that in rules Pi and Lda, a subterm of the
subject of the conclusion (the type-label) appears in the context part
of a premise; thus in these cases of an induction argument, a reduction
in the subject of the conclusion may result in a reduction in the context
of a premise. This suggests that the induction hypothesis should be
strengthened to simultaneously treat reduction in the context and the
subject, leading to the goal
where ! is ordinary one-step fi -reduction. This approach is used
in [GN91, Bar92], and produces a large number of case distinctions,
based on which subterm of the subject contains the one redex which
is contracted; all of these subcases are inessential except to isolate
the one non-trivial case where the redex contracted is the application
constructed by rule App. This simultaneous treatment of one reduction
in either the context or the subject suggested to us that the proof would
be smoother using a reduction relation that is congruent simultaneously
in all branches, while forbidding overlapping of redexes. One step non-overlapping
reduction, no
(no red1), is defined by the same rules as ![
(table 2) except for the fi -rule, which is modified to prevent overlapping
redexes:
nor1-beta ( [u:U ]B ) A no
no
, so from the assumed property cnv conv (section 4.1),
we have
A no
We extend no
compositionally to contexts (red1Cxt), and to
pairs of a context and a term (red1Subj), writing \Gamma no
h\Gamma; Mi no
We also define no
(no redn), the transitive closure
of no
, and show no
(no par redn, par no redn).
5.5.2 The Main Lemma (subject reduction lem)
Proof. By structural induction on We show the interesting
case, from rule App. Given
l prem
prem
red
we must show . By induction hypotheses
By type correctness of gtsDM, for some s
By the pi-generation lemma, for some s 2 and p 62 B
By the cut rule on gtsDL and gtsDB (we also use vsub-is-psub-alpha
(section 2.3) here, and several more times in this case)
Now there are two subcases
!M 0 and L no
The goal is rule App and the induction hypotheses
we easily have Use rule tCnv and gts-
DBsub to expand L 0 in the predicate back to L as required (this uses
cnv-conv).
The goal is
by the lambda-generation lemma, for some w , B 0 and s 0
By (cnvCR-pi c') (this is the only time it is used in this proof)
By a generation lemma on gtsDpi'
By (tCnv cnvA gtsDL gtsDA')
By tCnv, cnvB and gtsDBsub, it suffices to show
which follows by cut on gtsDL' and gtsDb.
5.5.3 Closure Under Reduction
It is now easy to show the subject reduction theorem, gtsSR, and a
useful corollary, predicate reduction
Finally, extending !
compositionally to contexts 10 , ' is closed under
beta-reduction (gtsAllRed)
This relation is equivalent to transitively closing the compositional extension of
of no
! to contexts.
There is a trivial but useful lemma (predicate conv):
Unlike rule tCnv, we don't ask for evidence that s has a type, but the
side condition uses ' , not - . To prove such a lemma with - requires
technical restrictions; e.g. ECC with its type hierarchy chopped off a
finite level fails to have such a property because of the sort at the top
of the hierarchy.
Closure Under Alpha-Conversion From gtsAllRed and the fact
that ff
, it follows that ' judgements are preserved by ff
(gts alpha closed). Hence an implementation may typecheck a
judgement as stated by a user, rather than searching for an alpha-
equivalent judgement which is derivable. See [Pol94b, Pol94a].
5.6 Another Presentations of PTS
In several rules of ' the context \Gamma occurs more then once in the list
of premises; in order to build a complete derivation, \Gamma must be constructed
(by Start and Weak) in each branch in which it appears. It
is much more efficient to assume that we start with a valid context, and
only check that when rules extend the context (i.e. the right premise
of Pi and the left premise of Lda) they maintain validity. This is more
in keeping with implementations which are actually used, where we
work in a "current context" of mathematical assumptions. We present
such a system in table 8, and show it is equivalent to ' . The idea is
originally due to Martin-L-of [Mar71], and is used in [Hue89].
This system has two judgements, a type judgment of shape
(lvtyp), and a validity judgement of shape \Gamma ' lv
(lvcxt).
Note that they are not mutually inductive: validity depends on typing,
but not conversely.
We have proved that ' lv
characterizes ' (iff gts lvcxt lvtyp):
Direction ( of the proof is subtle. Formally, it uses an auxiliary
mutual inductive definition, and a well-founded induction requiring dependent
elimination; this is the only place in the entire development
that either mutual induction or dependent elimination are used. More
abstractly, direction ( claims termination of a function that replaces
all the proof annotations omitted by ' lv
. As this is a fast-growing
function, its termination is a strong result. The proof is described
in [Pol94b].
lvLda
lvCons

Table

8: The system of locally valid contexts (lvtyp, lvcxt)
6 PTS with fi -conversion
It is remarkable how little about - has been needed for the theory
described so far (section 4.1). In [Pol94b] we pursue the theory of
PTS with abstract conversion to a correct typechecking algorithm for
cumulative PTS , including Luo's system ECC. Here, we point out a
more standard theory of PTS with fi -conversion, i.e. we instantiate -
in the preceeding with the actual relation ' . (In LEGO the command
Cut executes the admissible rule substitution lemma of section 5.4.)
This theory, leading to the strengthening theorem and typechecking
algorithms for classes of PTS , is detailed in [vBJMP94].
6.1 Strengthening
Strengthening is a tricky result about PTS , first proved by Jut-
ting [vBJ93]:
\Gamma[q:C
(gts strengthening)
The development we formalize, in which strengthening is a corollary to
work on typechecking, is described in detail in [vBJMP94]. We were
particularly interested to prove strengthening because LEGO uses it in
the Discharge command.
6.2 Functional PTS
Functional PTS are well behaved and are, perhaps, the only ones that
are interesting in practice.
Functional ,
ae
In a functional PTS , ax and rl are the graphs of partial functions,
but we do not necessarily have procedures to compute these functions.
Uniqueness of Types The definition of functional PTS makes sense
for abstract-conversion PTS , and is useful in that setting, as it gives a
kind of uniqueness: when building a derivation of a typing judgement
guided by the syntax of its subject, it is deterministic which axiom
to use at each instance of Ax, and which rule to use at each instance
of Pi [Pol94b]. The idea behind the definition of functional is that
the uniqueness just mentioned propagates through whole derivations
to give a property that types are unique up to conversion:
conv unique types ,
For fi -conversion PTS we prove
Functional ) conv unique types (types unicity)
by structural induction on
However, this proof uses properties of ' , and cannot be modified
to prove any similar property of abstract-conversion PTS .
types unicity is too linear for - , which is only a partial order; the
correct generalization is a principal types lemma, saying that any type
is above some principal type, but we cannot hope that every two types
are comparable [Pol94b].
Subject Expansion Any fi - PTS with uniqueness of types also has
a subject expansion property (subject expansion):
conv unique types )
While subject reduction says that terms don't lose types under reduc-
tion, this lemma says terms don't gain types under reduction. In this
A is the principal premise, and is a well-formedness
premise. There are examples of two different ways subject
expansion can fail for non-functional PTS in [vBJMP94].



--R

A formalization of the strong normalization proof for System F in LEGO.
Lambda calculi with types.
Coq en Coq.
Type Dependence and Constructive Mathemat- ics
On the subject reduction property for algebraic type systems.
F-sub, the system.
An algorithm for testing conversion in type theory.
Combinator shared reduction and infinite objects in type theory.
An algorithm for type-checking dependent types
Towards checking proof checkers.

On Girard's
The Collected Papers of Gerhard Gentzen.
Logics and Type Systems.
Five axioms of alpha con- version
A modular proof of strong normalization for the calculus of constructions.
The constructive engine.
Residual theory in
Incremental changes in LEGO:
Introduction to Metamathematics.
LEGO proof development sys- tem: User's manual. Technical Report ECS-LFCS-92-211, LFCS, Computer Science Dept., University of Edinburgh, The King's Buildings
Program specification and data refinement in type theory.
Computation and Reasoning: A Type Theory for Computer Science.
Per Martin-L-of
Inverting inductively defined predicates in LEGO.
The standardisation theorem for
Pure Type Systems for- malized
More Church-Rosser proofs (in isabelle/hol)
Coding binding and substitution explicitly in Is- abelle
A proof of the Church-Rosser theorem and its representation in a logical framework

Closure under alpha-conversion
The Theory of LEGO: A Proof Checker for the Extended Calculus of Constructions.
A verified typechecker.
How to believe a machine-checked proof
Natural Deduction
Program Verification in Synthetic Domain The- ory
Synthetic domain theory in type theory: Another logic of computable functions.
Theory of symbolic expressions
Auxiliary variables and recursive procedures.
A mechanical proof of the Church-Rosser theorem
Substitution revisited.
Parallel reductions in
Formulation of Martin-L-of's theory of types with explicit substitutions. Master's thesis


--TR

--CTR
Michael Norrish, Mechanising Hankin and Barendregt using the Gordon-Melham axioms, Proceedings of the ACM SIGPLAN workshop on Mechanized reasoning about languages with variable binding, p.1-7, August 26, 2003, Uppsala, Sweden
Christian Urban , Michael Norrish, A formal treatment of the barendregt variable convention in rule inductions, Proceedings of the 3rd ACM SIGPLAN workshop on Mechanized reasoning about languages with variable binding, p.25-32, September 30-30, 2005, Tallinn, Estonia
Yasuhiko Minamide , Koji Okuma, Verifying CPS transformations in Isabelle/HOL, Proceedings of the ACM SIGPLAN workshop on Mechanized reasoning about languages with variable binding, p.1-8, August 26, 2003, Uppsala, Sweden
Michael Norrish, Mechanising -calculus using a classical first order theory of terms with permutations, Higher-Order and Symbolic Computation, v.19 n.2-3, p.169-195, September 2006
S. J. Ambler , R. L. Crole , Alberto Momigliano, A definitional approach to primitivexs recursion over higher order abstract syntax, Proceedings of the ACM SIGPLAN workshop on Mechanized reasoning about languages with variable binding, p.1-11, August 26, 2003, Uppsala, Sweden
Dimitri Hendriks, Proof Reflection in Coq, Journal of Automated Reasoning, v.29 n.3-4, p.277-307, 2002
Jonathan Ford , Ian A. Mason, Formal Foundations of Operational Semantics, Higher-Order and Symbolic Computation, v.16 n.3, p.161-202, September
Conor McBride , James McKinna, Functional pearl: i am not a number--i am a free variable, Proceedings of the 2004 ACM SIGPLAN workshop on Haskell, September 22-22, 2004, Snowbird, Utah, USA
James Cheney, Scrap your nameplate: (functional pearl), ACM SIGPLAN Notices, v.40 n.9, September 2005
Thierry Coquand , Randy Pollack , Makoto Takeyama, A Logical Framework with Dependently Typed Records, Fundamenta Informaticae, v.65 n.1-2, p.113-134, January 2005
Ren Vestergaard , James Brotherston, A formalised first-order confluence proof for the -calculus using one-sorted variable names, Information and Computation, v.183 n.2, p.212-244, 15 June
Andrew M. Pitts, Nominal logic, a first order theory of names and binding, Information and Computation, v.186 n.2, p.165-193, 01 November
Conor McBride , James McKinna, The view from the left, Journal of Functional Programming, v.14 n.1, p.69-111, January 2004
Riccardo Pucella, SIGACT news logic column 14, ACM SIGACT News, v.36 n.4, December 2005

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'finite automata'
 'zero storage biometric authentication' 'parallel programming']
marked:['type theory', 'lambda calculus', 'LEGO proof checker', 'Pure Type Systems', 'formal mathematics']
--T
New Worst-Case Upper Bounds for SAT.
--A
In 1980 Monien and Speckenmeyer proved that satisfiability of a propositional formula consisting of i>K clauses (of arbitrary length) can be checked in time of the order 2i>K / 3. Recently Kullmann and Luckhardt proved the worst-case upper bound 2i>L / 9, where i>L is the length of the input formula. The algorithms leading to these bounds are based on the i>splitting method, which goes back to the DavisPutnam procedure. i>Transformation rules (pure literal elimination, unit propagation, etc.) constitute a substantial part of this method. In this paper we present a new transformation rule and two algorithms using this rule. We prove that these algorithms have the worst-case upper bounds 20. 30897 i>K and 20. 10299 i>L, respectively.
--B
Introduction
.
SAT (the problem of satisability of a propositional formula in conjunctive normal
can be easily solved in time of the order 2 N , where N is the number of variables in the
input formula. In the early 1980s this trivial bound was reduced for formulas in 3-CNF by
Monien and Speckenmeyer [20] (see also [22]) and independently by Dantsin [1] (see also
[4] and [2]). After that, many upper bounds for SAT and its subproblems were obtained
[21, 17, 25, 13, 14, 26, 16, 8, 23, 24]. Most authors consider bounds w.r.t. three main
parameters: the length L of the input formula, the number K of its clauses and the number
N of the variables occurring in it. Before the conference proceedings version of this paper
[9], the best known worst-case upper bounds for SAT were:
p(L)2 L=9 [16],
p(L)2 K=3 [21] (see also [16]),
where p is a polynomial. Also, the upper bound p(L)2 L=4 for satisability problem for general
Boolean formulas (i.e., not necessarily in CNF) is known [28]. Recently Paturi, Pudlak, Saks
and Zane [24] proved that 3-SAT is checkable in the randomized time O(2 0:446N ), but it is still
Some of the work described in this paper was presented at the 9th ACM-SIAM Symposium on Discrete
Algorithms (SODA'98) [9].
y Steklov Institute of Mathematics at St.Petersburg, 27 Fontanka, 191011 St.Petersburg, Russia.
Email: hirsch@pdmi.ras.ru, URL: http://logic.pdmi.ras.ru/~hirsch/index.html. Supported in
part by grants from INTAS and RFBR.
unknown whether the trivial upper bound 2 N for the general SAT problem can be improved.
Kullmann and Luckhardt in [16] simplied the algorithm of Monien and Speckenmeyer from
[21] and presented also an improved algorithm of the same complexity p(L)2 K=3 which works
better when the ratio of the number of clauses to the number of variables is greater than
2:64557. In the conference proceedings version of this paper [9] we presented two algorithms
which improve the bounds p(L)2 to
p(L)2 0:30897K and p(L)2 0:10537L respectively. In this journal version we improve the second
algorithm and prove the worst-case upper bound p(L)2 0:10299L . Our new algorithm uses the
p(L)2 0:30897K -time algorithm as a subroutine; the proof of the corresponding upper bound is
much simpler than the proof in the conference proceedings version [9].
The most popular methods for solving SAT are the local search method and the splitting
method . Many experimental and average-case results show the signicant power of the local
search method (for references, see the survey [7]). However, most best known worst-case
upper bounds for SAT and its NP-complete subproblems are obtained using the splitting
method [21, 16, 8] (for worst-case upper bounds for the local search method see [10, 11,
12]). The splitting method is also useful in proving worst-case upper bounds for exact and
approximate MAXSAT solving [18, 3, 19].
The splitting method goes back to the Davis{Putnam procedure [6]. Let l be a literal
occurring in a formula F . Let F [l] be the formula obtained from F by assigning the value
T rue to the literal l, i.e. by removing all clauses containing l and deleting all occurrences of
l from the other clauses. In short, the main idea of the Davis{Putnam procedure is that F is
satisable i at least one of the formulas F [l] and F [ l ] is satisable. In a wider sense [5], a
splitting algorithm constructs a tree by reducing satisability of a formula F to satisability
of several formulas F [I 1 ]; F [I obtained from F by assignments I 1 ; I
spectively. Then, a splitting algorithm simplies each of the formulas F [I 1 ]; F [I
according to transformation rules which do not change their satisability. These transformations
take a polynomial time. Their role is to reduce some of the parameters L, K or
N of the formulas F [I 1 ]; F [I and to simplify these formulas by eliminating pure
literals (a literal is pure if its negation does not occur in the formula), 1-clauses (1-clause is
a clause consisting of one literal) and other \easy pieces".
Recurrent equations are often used in complexity analysis of algorithms. Kullmann and
Luckhardt in [16] described a very similar, but simpler technique which is very useful in the
estimation of the running time of splitting algorithms. One can consider an execution of
a splitting algorithm as a branching tree, i.e. a tree such that formulas labelling children
are simpler than formulas labelling parents (leaves are labelled with the simplest formulas).
With each node of this tree we associate a branching vector of non-negative numbers and
a polynomial constructed from this vector. One can estimate the number of leaves in the
tree using the largest of the positive roots of these polynomials. Precise denitions and
formulations are given in Sect. 3.
Transformation rules play an important role in splitting algorithms. Two simplest rules
were proposed in the original paper of Davis and Putnam [6]: we can eliminate pure literals
and 1-clauses. All transformation rules we use are described in Sect. 4. This paper introduces
a new transformation rule which goes up to the following simple observation: if each clause
of our formula contains at least one negative literal, then this formula is trivially satisable
by the assignment in which all variables have the value F alse. Let P (l; F ) be a property of
a literal and a formula, for example, \F contains exactly two occurrences of l and at least
three occurrences of l ". In addition, we suppose that for each variable v in the formula F
at most one of the literals v and v satises P . Given F , the literals that satisfy the property
P will be referred as P -literals.
The black and white literals principle. Let F be a formula in CNF. At least
one of the following two alternatives holds.
(1) There is a clause in F that contains a P -literal and does not contain the
negations of any other P -literals.
(2) Satisability of F is equivalent to satisability of the formula obtained
from F by removing all clauses containing the negations of P -literals.
A formal proof of the black and white literals principle is given in Lemma 4.3. Figure 1
illustrates this principle. White circles (-) denote P -literals, black circles () denote their
negations, circles with dots (-) denote other literals (i.e., literals which are neither P -literals
nor the negations of P -literals; note that the negation of such literal is again neither P -literal
nor the negation of a P -literal). Columns correspond to clauses of the formula. The gure
contains two formulas, the rst one satises the condition (1), the second one satises the
condition (2).
(1)
| {z }
(2)
@
@
@
@
@

Figure

1: Two alternatives of the black and white literals principle.
This principle is the key point of our algorithm corresponding to the upper bound p(L)2 0:30897K :
In this algorithm we use the presence of clauses that contain a P -literal and do not contain
the negations of other P -literals for a certain property P . The black and white literals principle
is a kind of insurance: it guarantees that if we cannot nd a required clause, then we
can replace F by a simpler formula, or F does not contain any P -literals at all. This principle
is one of re-formulations of the following simple property: for any partial assignment A
(i.e., a set of literals which does not contain simultaneously x and x for any variable x) for
a formula F in CNF, either there is a clause in F containing only literals from A, or setting
the values of all these literals to F alse does not change the satisability of F .
Another known re-formulations of this property are the generalized sign principle [16] and
the autarkness principle [20, 22, 17] (see also [16]). A comprehensive study of the autarkness
principle can be found in [15].
The use of the black and white literals principle leads to two new bounds for SAT presented
in this paper and also to several upper bounds for the satisability problem for
formulas in CNF{(1,1) [8] (a formula is in CNF{(1,1) if each literal occurs in it positively
at most once; the satisability problem for these formulas is NP-complete).
In Sect. 2 we give basic denitions. Section 3 contains the technique that allows us to
estimate the size of a branching tree. In Sect. 4 we explain transformation rules that we
use in our algorithms. In Sect. 5 and Sect. 6 we describe the algorithms having the upper
bounds p(L)2 0:30897K and p(L)2 0:10299L respectively, and the corresponding proofs.
2 Basic denitions.
Let V be a set of Boolean variables. The negation of a variable v is denoted by v. Given a
set U , we denote Ug. Literals are the members of the set Positive
literals are the members of the set V . Negative literals are their negations. If w denotes
a negative literal v, then w denotes the variable v. A clause is a nite set of literals that
does not contain simultaneously any variable together with its negation. The empty clause
is interpreted as F alse. A formula in CNF (CNF-formula) is a nite set of clauses. The
empty formula is interpreted as T rue. The length of a clause is its cardinality, the length of
a formula is the sum of the lengths of all its clauses. The length of a clause C is denoted by
jCj. A k-clause is a clause of the length k. A k + -clause is a clause of the length at least k.
A k -clause is a clause of the length at most k. If we say that a literal v occurs in a clause
or in a formula, we mean that this clause or this formula contains the literal v. However, if
we say that a variable v occurs in a clause or in a formula, we mean that this clause or this
formula contains the literal v, or it contains the literal v.
An assignment is a nite subset of W that does not contain any variable together with
its negation. Informally speaking, if an assignment I contains a literal w, it means that w
has the value T rue in I. To obtain F [I] from F and an assignment I
remove from F all clauses containing the literals w i ,
delete all occurrences of the literals w i from the other clauses.
For short, we write F [w
An assignment I is satisfying for a formula F if F [I] is the empty formula. A formula is
satisable if there exists a satisfying assignment for it. We say that two formulas F and G
are equi-satisable if both are satisable, or both are unsatisable.
Let w be a literal occurring in a formula F . This literal is a i-literal if F contains exactly
i occurrences of w. It is a (i;j)-literal if F contains exactly i occurrences of w and exactly j
occurrences of w. It is a (i;j + )-literal if F contains exactly i occurrences of w and at least j
occurrences of w. It is a (i;j )-literal if F contains exactly i occurrences of w and at most
occurrences of w. Similarly, we dene (i
We denote the number of occurrences of a literal w in a formula F by #w
the sum of the lengths of the clauses containing w by }w When the meaning of F is
clear from the context, we omit F . A literal w is a }i-literal if Similarly, we dene
3 Estimation of the size of a branching tree.
Kullmann and Luckhardt introduced in [16] a notion of a branching tree. It is intended for
estimating the time complexity of splitting algorithms, since a tree of formulas which such an
algorithm splits, is a branching tree. One can consider an execution of a splitting algorithm
as a tree whose nodes are labelled with CNF-formulas such that if some node is labelled with
a CNF-formula F , then its sons are labelled with (simplied) formulas F [I 1 ]; F [I 2 ];
for some assignments I
Suppose we have a tree whose nodes are labelled with formulas in CNF. To each formula
F we attach a non-negative integer The tree is a branching tree if,
for each node, the complexity of the formula labelling this node is strictly greater than the
complexity of each of the formulas labelling its sons. In this paper we use
is the number of variables in F ;
is the number of clauses in F ;
is the length of F .
We prove two upper bounds, w.r.t. K and w.r.t. L. However, in this section we do not x
any concrete measure of complexity.
Let us consider a node in our tree labelled with a formula F 0 . Suppose its sons are labelled
with formulas F 1 , F 2 , . , Fm . A branching vector of a node is an m-tuple
are positive numbers not exceeding The characteristic polynomial of
a branching vector ~ t is dened by
The characteristic polynomial h ~ t (x) is a monotone function of x on the interval (0; +1),
and h ~ t exactly one positive root. We denote
this root by ( ~ t ) and call it the branching number . We suppose leaves. We
omit one pair of parentheses and write
Example 3.1 For example,
0:69424::: is the golden ratio;
The branching number of a tree T is the largest of the branching numbers ( ~ t ) of its
nodes. We denote it by  max;T . The following lemma proved by Kullmann and Luckhardt
allows us to estimate the number of leaves in a branching tree using its branching number.
Lemma 3.1 (Kullmann, Luckhardt, [16]) Let T be a branching tree, let its root be labelled
with a formula F 0 . Then the number of leaves in T does not exceed ( max;T
This lemma already allows us to estimate the running time of a splitting algorithm
if we know the branching number of the splitting tree and the algorithm processes each
its leaf in a polynomial time. However, our algorithm corresponding to the upper bound
calls the algorithm corresponding to the upper bound p(L)(6; 7; 6; 7) K
as a subroutine and thus processes some of the leaves in an exponential time. To estimate
the overall running time, we use the following simple generalization of Lemma 3.1.
Lemma 3.2 [8] Let T be a branching tree, let its root be labelled with a formula F 0 . Let G l
denote the object labelling a leaf l of the tree T . Let
l is a leaf of T
Proof is the induction on the construction of the tree.
Base. The tree consisting of the unique node. In this case
Step. Consider the tree T presented in Fig. 2. Let ~ be the branching tuple
in its root.
l is a leaf of T
s
l is a leaf of T j
f((G l ))A
s
s
(by denition of the tuple ~ t)

Figure

2: A splitting tree
s
s
(by denition of h ~ t )
(max(;  max;T (by monotonicity of h ~ t )
ut
Now if we know the branching number of the tree corresponding to a splitting algorithm,
then we can estimate its running time. We explicitly require our algorithms to perform
splittings with branching numbers not greater than we wish (and we only need to prove that
there always exists a splitting satisfying this condition). For this purpose, the algorithm has
to compare branching numbers corresponding to dierent vectors. Of course, this can be
done just by the examination of a constant number of cases and the use of the monotonicity
of  . However, a more general statement holds.
Lemma 3.3 (Kullmann, Luckhardt, [16]) Let m, k be natural constants, x 1
positive rational numbers. The problem whether
than (y 1 ;y solvable in time polynomial of max(x 1 ;x
In the following, when estimating the running time of our algorithms, we use frequently
inequalities like (7; 15) < (5; 17) or (5; 17) < (6; 7; 6; 7) 1=3 without proofs. One can check
these inequalities by approximate calculation of the branching numbers. However, there are
several simple observations that may help to prove some of this inequalities easier.
Lemma 3.4 (Kullmann, Luckhardt, [16])
(1) Permutations of the components of a branching vector do not aect the corresponding
branching number.
(2) The branching number strictly decreases as one or more components of the branching
vector increase.
4 Transformation rules.
In this section F denotes a formula in CNF.
Lemmas 3.1 and 3.2 allows us to take into consideration only the dierences between
the complexity of an input formula and the complexities of the formulas obtained from it
by splitting. The higher these dierences are, the smaller the number of leaves is. Thus,
to obtain a good algorithm, we should reduce as much as possible the complexities of the
formulas obtained by splitting. Here we explain transformation rules that allow us to do it.
More precisely, we explain that in certain cases we can nd a simpler formula equi-satisable
with F . In the following we use these rules so that they never increase the parameter under
consideration (the number of clauses in a formula or the length of a formula).
Elimination of 1-clauses. If F contains a 1-clause fag, then the formulas F and F [a] are
equi-satisable since all assignments that contain a are unsatisfying.
Subsumption. If F contains two clauses C and D such that C  D, then F and F nfDg are
equi-satisable since each assignment that satises the clause C, satises also the clause D.
Resolution with subsumption. Suppose we are given a literal a and clauses C and D
such that a is the only literal satisfying both conditions a 2 C and a 2 D. In this case, the
clause (C [ D) n fa; ag is called the resolvent by the literal a of the clauses C and D. We
denote it by R(C; D).
Let now F contain such clauses C and D. It is clear that adding R(C; D) to the formula
does not change its satisability. However, it increases its size. To avoid this eect, we
use this rule only if R(C; D)  D. In this case we reduce the satisability problem for the
formula F to the satisability problem for D)g.
Elimination of a variable by resolution. Given a literal a, we construct the formula
DP a
adding to F all resolvents by a;
removing from F all clauses containing a or a.
Lemma 4.1 (Davis, Putnam, [6]) The formulas F and DP a are equi-satisable.
This transformation can increase the size of a formula or/and the number of clauses in
it, but we use this rule only if it does not increase the parameter under consideration (the
number of clauses in a formula or the length of a formula). Note that in particular, this
transformation does not increase any of the parameters of F if a is a pure literal, and thus
it eliminates pure literals.
Elimination of blocked clauses. The clause C is blocked for a literal a w.r.t. F if C
contains the literal a, and the literal a occurs only in the clauses of F that contain the
negation of at least one of the literals occurring in C n fag. (Note that F may or may not
contain C.) In other words, there are no resolvents by a of the clause C and any other
clause of the formula F . For a CNF-formula F and a literal a occurring in it, we dene the
assignment
ag j the clause fa; xg is blocked for a w.r.t. Fg:
When the meaning of F is clear from the context, we omit F and write I(a).
The notion of a blocked clause was introduced and investigated by Kullmann in [13, 14].
We use the following two facts about blocked clauses.
Lemma 4.2 (Kullmann, [13, 14])
(1) If a clause C is blocked for a literal a w.r.t. F , then F , F n fCg and F [ fCg are
equi-satisable.
(2) Given a literal a, the formula F is satisable i at least one of the formulas F [a]
and F [I(a)] is satisable.
Application of the black and white literals principle. Let P be a binary relation
between literals and formulas in CNF, such that for a variable v and a formula F , at most
one of P (v; F ) and P (v; F ) holds.
Lemma 4.3 Suppose that each clause of F that contains a literal w satisfying P (w; F )
contains also at least one literal b satisfying P (b; F ). Then F and F [fl j P (l; F )g] are equi-
Proof. This is a particular case of the autarkness principle [20, 22, 17, 16]. We denote
Suppose G is satisable. Consider a satisfying assignment I for the
formula G. It is clear that the assignment I [ . On the other hand,
each assignment satisfying F satises also G. ut
5 A bound w.r.t. the number of clauses.
In this section we present Algorithm 5.1 which checks satisability of formula F in the time
is a polynomial). This algorithm has two subroutines, Function
REDUCEK and Function SPLITK . Function REDUCEK simplies the input formula
using the transformation rules (see Sect. 4). Function SPLITK is intended for reducing the
satisability problem for the input formula to the satisability problem for several simpler
formulas. The execution of this algorithm can be viewed as follows: Function REDUCEK
simplies the input formula, then SPLITK splits it into several formulas, REDUCEK sim-
plies each of these, and so on.
In the following, we denote by REDUCEK the formula that Function REDUCEK
outputs on input F . Similarly we dene SPLITK
Function REDUCEK .
Input: A formula F in CNF.
Output: A (simplied) formula in CNF.
Method.
(KR1) Elimination of 1-clauses. If F contains a 1-clause
Repeat this step while F contains 1-clauses.
(KR2) Application of the black and white literals principle. If each clause C in F
that contains a (2;3 contains also a (3
F := F [fa j a is a (3
Elimination of a variable by resolution. Choose a literal a such that a or a
occurs in F and maximal. If there are several such
literals, then choose a literal with # a minimal. If   0, then F := DP a
Repeat this step while F satises this condition.
If F has been changed at steps (KR1){(KR3), then go to step (KR1), otherwise
return F .
ut
Function SPLITK .
Input: A formula F in CNF.
Output: If F is satisable, then T rue, otherwise F alse.
Method.
(KS1) The empty formula. If
containing the empty clause. If ; 2 F , then return F alse.
Splitting into two subproblems. For each literal a occurring in F , construct two
If there exists a literal a such that (K(F
then choose the formulas F 1 and F 2 corresponding to this literal. If SPLITK returns
T rue for at least one of these, then return T rue, otherwise return F alse.
Splitting into four subproblems. Choose a literal a occurring in F . For each
two literals b and c occurring in F [a] and F [a] respectively, construct four formulas
F 11 =REDUCEK
F 21 =REDUCEK
F 22 =REDUCEK
If there exist literals b and c such that
then choose the formulas F 11 , F 12 , F 21 and F 22 corresponding to these literals. If
returns T rue for at least one of these, then return T rue, otherwise return
F alse.
ut
Algorithm 5.1.
Input: A formula F in CNF.
Output: If F is satisable, then T rue, otherwise F alse.
Method.
Return SPLITK (REDUCEK
ut
In Sect. 4 we explained that none of the steps of REDUCEK changes the satisability of
the formula. The steps (KR1){(KR4) cannot be repeated more than K(F
since none of them increases the number of clauses or the number of variables, and during
each iteration (KR1){(KR4){(KR1) at least one of these quantities decreases. Each of these
steps takes a polynomial time. Thus, REDUCEK does not change the satisability of the
formula and returns the answer in a polynomial time.
We now construct a tree which re
ects the behaviour of Algorithm 5.1 (together with
Functions REDUCEK and SPLITK ). Its internal nodes are labelled with formulas that
Algorithm 5.1 splits at steps (KS3) and (KS4). Its leaves are labelled with formulas that
satisfy the conditions of the steps (KS1) and (KS2). Each internal node labelled with F has
two sons labelled with F 1 and F 2 or four sons labelled with F 11 , F 12 , F 21 and F 22 . Since
the conditions of the steps (KS3) and (KS4) guarantee that the corresponding branching
numbers do not exceed (6; 7; 6; 7), by Lemma 3.1 the number of leaves in the tree is at most
is the input formula. Thus, the Algorithm 5.1 returns the answer
in the time p(L(F ))(6; 7; 6; 7) K(F ) , where p is some polynomial.
It remains to prove that Algorithm 5.1 performs correctly, i.e., each formula in CNF
reduced by REDUCEK satises at least one of the conditions of the steps (KS1){(KS4). To
prove this statement, we need two simple lemmas concerning the output of REDUCEK .
Lemma 5.1 Let F 1 be the value of F at step (KR1) of Function REDUCEK at some point
of time (maybe after eliminating several 1-clauses), F 2 be the corresponding output formula
of Function REDUCEK , d and e be literals occurring in F 1 .
(1) If d is a (1 ;1
(2) If d and e are (1 ;1 and there are no clauses in F
that contain d and e simultaneously, then 2.
Proof. (1). If REDUCEK modies once more the formula at steps (KR1) or (KR2), then at
least one clause will be deleted from it. Otherwise, at least one clause will be eliminated at
step (KR3) since K(F 1 ) K(DP d
(2). W.l.o.g we suppose that F 1 contains at most one 1-clause, this clause (if any) contains
a 1-literal and after eliminating this clause the formula F 1 does not contain 1-clauses at
all. If F 1 contains exactly one 1-clause, then it will be eliminated at step (KR1), and one
more clause will be eliminated by (1) since at least one of the literals d and e remains a
Now let F 1 contain no 1-clauses. If REDUCEK modies this formula at step (KR2), then
at least one clause, i.e., at least two occurrences will be eliminated.
Now F has the value F 1 before the step (KR3). W.l.o.g. we suppose that
of the step (KR3). At step (KR3) a 1 -literal will be chosen. After the rst application of
DP , at least one of the literals d and e remains a 1 -literal and its negation or the literal
itself remains in the formula. Hence, by (1) at least one more clause will be eliminated. ut
Lemma 5.2 Let F be a formula in CNF. Then the formula REDUCEK does not contain
1-clauses, 1 -literals and (2;2)-literals.
Proof. Function REDUCEK eliminates 1-clauses at step (KR1). If d is a 1 -literal or a
(2;2)-literal, then K(DP d eliminates such literals at
step (KR3). ut
Theorem 5.1 Algorithm 5.1 performs correctly and stops in the time p(L)(6; 7; 6; 7) K <
p(L)2 0:30897K , where L is the length of the input formula, K is the number of clauses in it,
and p is a polynomial.
Proof. We have shown above that it su-ces to prove that each formula F in CNF reduced
by REDUCEK satises at least one of the conditions of the steps (KS1){(KS4). Suppose
F does not satisfy the conditions of the steps (KS1){(KS4). We now consider all possible
cases. We prove that each of this cases is impossible. All symbols have the same meaning
as they have at the corresponding steps of Function SPLITK . For
The key observation is that if the formula F contains few \frequent" variables, then after
the transformation of F into F [a]; F [a] for some literal a we can apply Lemma 5.1. We now
prove this more formally.
Case 1: The formula F contains a (3
All clauses of the formula F containing the literal a disappear in F [a], all its clauses containing
the literal a disappear in F [a]. Since a is a (3 4. Thus, in terms
of the step (KS3), i.e., the condition of the step (KS3) is
Case 2: The formula F contains a clause that contains a (3;3)-literal a and a (2;3
b.
All three clauses of the formula F containing a disappear in F [a], all three clauses containing
a disappear in F [a]. In addition, b becomes a (1 ;1 + )-literal in F [a]. Thus, in terms of the
step (KS3) we have 1  1 by Lemma 5.1. Hence,  i.e., the
condition of the step (KS3) is satised.
Case 3: The formula F contains a 2-clause consisting of 2-literals.
All clauses of the formula F containing a disappear in F [a], all its clauses containing a
disappear in F [a]. By Lemma 5.2, a is a (2;3 3. Similarly to
Case 2, in terms of the step (KS3) we have 1  1. In addition, C becomes a 1-clause in
F [a] and will be eliminated at step (KR1), i.e. 2  1. We now have
4, i.e., the condition of the step (KS3) is satised.
Case 4: The formula F contains a 3 consisting of 2-literals.
sg. All clauses of the formula F containing a i disappear in F [a i ], all
its clauses containing a i disappear in F [a i ]. By Lemma 5.2, a i is a (2;3
In addition, a 2 and a 3 become
in F [a 1 ]. If C is the only clause that contains a 2 and a 3 simultaneously, then by Lemma 5.1
we have K(F [a 1 ]) K(REDUCEK 2. Otherwise, a 3 becomes a (0;3
in F [a 2 ], i.e. K(F [a 2 ]) K(REDUCEK 2. Depending on which of these two
alternatives takes place, we choose a 1 or a 2 as a in terms of the step (KS3). We now have
i.e., the condition of the step (KS3) is satised.
Case 5: The conditions of Cases 1{4 are not satised.
Since the step (KR2) does not change F , it contains only 3-literals. Since F does not satisfy
the condition of the step (KS3), for each literal a,
3:
Let a be a literal occurring in F . We now prove that there exist literals b and c such that the
condition of the step (KS4) is satised. We consider three sub-cases (the rst two of them
are similar to Cases 2{4).
Case 5.1: There exists a clause in F [a], such that a 1 is a (3;3)-literal
and a 2 is a 2-literal.
We suppose in terms of the step (KS4). Similarly to Case 2, 11  4, 12  3.
Case 5.2: There exists a clause in F [a], such that a 1 is a (2;3)-literal
and a are 2-literals.
We suppose in terms of the step (KS4). Similarly to Cases 3{4, 11  3, 12  4,
or 11  4, 12  3. (Note that by (5.1) the literals a i and a j cannot occur together in two
clauses.)
Case 5.3: The conditions of Cases 5.1{5.2 are not satised.
Since by (5.1) the steps (KR1){(KR3) do not reduce the number of clauses in the formula
F [a] and the conditions of Cases 5.1{5.2 are not satised, this formula consists of clauses
that contain only (2;2)-literals and clauses that contain only (3;3)-literals. Since exactly
three clauses of the formula F disappear in F [a], all occurrences of any (3;3)-literal cannot
disappear in F [a]. Hence, it contains at least one (2;2)-literal b 1 . Let b 1 be a 2-literal. The
formula F does not contain 2-clauses by (5.1). Similarly, it cannot contain two clauses,
each containing the literals a and b 1 simultaneously. Thus, F [a] contains a 3
are 2-literals.
We choose the literal b 1 as b. Both clauses of the formula F [a] that contain b 1 disappear
in F [a; b 1 ]. In addition, these clauses contain only (2;2)-literals which become
literals. Thus, by Lemma 5.1
3:
On the other hand, two clauses of F [a] containing b 1 disappear in F [a; b 1 ], and two more
clauses disappear by Lemma 5.1 since b 2 and b 3 become )-literals. We note that by
(5.1) the formula F contains only one clause that contains b 2 and b 3 simultaneously. Thus,
We now have that in each of Cases 5.1{5.3 there exists a literal b in F [a] such that
7. A literal c can be chosen similarly. Hence, the
condition of the step (KS4) is satised. ut
6 A bound w.r.t. the length of a formula.
In this section we present Algorithm 6.1 which checks satisability of formula F in the
time p(L(F ))2 0:10299L(F ) (where p is a polynomial). As in the previous section, we dene
two subroutines, Function REDUCE L and Function SPLIT L . We use them similarly to
REDUCEK and SPLITK : Function REDUCE L simplies the input formula, then SPLIT L
splits it into several formulas, REDUCE L simplies each of these, and so on.
Function REDUCE L .
Input: A formula F in CNF.
Output: A (simplied) formula in CNF.
Method.
(LR1) Elimination of 1-clauses. If F contains a 1-clause
Repeat this step while F contains such a clause.
Subsumption. If F contains two clauses C and D such that C  D, then F :=
fDg. Repeat this step while F contains such clauses.
Elimination of blocked clauses. If F contains a blocked clause C, then F :=
fCg. Repeat this step while F contains blocked clauses.
Resolution with subsumption. If F contains two clauses C and D such that
R(C; D)D, then F := [fR(C;D)g. Repeat this step while F contains
such clauses.
Elimination of a variable by resolution. Choose a literal a such that a or a
occurs in F and
Repeat this step while F satises this condition.
If F has been changed at steps (LR1){(LR5), then go to step (LR1), otherwise
return F .
ut
Function SPLIT L .
Input: A formula F in CNF.
Output: If F is satisable, then T rue, otherwise F alse.
Method.
(LS1) The empty formula. If
containing the empty clause. If ; 2 F , then return F alse.
containing no 2-clauses. If F does not contain 2-clauses, apply Algorithm
5.1 to F and return its answer.
Splitting into two subproblems. For each literal a occurring in F , construct two
If there exists a literal a such that (L(F )
choose the formulas F 1 and F 2 corresponding to this literal. If SPLIT L returns T rue
for at least one of these, then return T rue, otherwise return F alse.
ut
Algorithm 6.1.
Input: A formula F in CNF.
Output: If F is satisable, then T rue, otherwise F alse.
Method.
Return SPLIT L (REDUCE L
ut
Similarly to REDUCEK , Function REDUCE L does not change the satisability of the
formula and returns the answer in a polynomial time.
We now construct a tree which re
ects the behaviour of Algorithm 6.1 (together with
Functions REDUCE L and SPLIT L ). Its leaves are labelled with formulas that satisfy the
conditions of the steps (LS1){(LS3). Algorithm 6.1 processes in a polynomial time formulas
satisfying the conditions of the steps (LS1) and (LS2), and passes formulas satisfying the
condition of the step (LS3) to Algorithm 5.1 which processes such a formula F in the time
is a polynomial (note that F
contains no 2 -clauses since 1-clauses are eliminated at step (LR1)).
Internal nodes of our tree are labelled with formulas that Algorithm 6.1 splits at step (LS4).
Each internal node labelled with F has two sons labelled with F 1 and F 2 . Since the condition
of the step (LS4) guarantees that the corresponding branching number does not exceed
3.2 the running time of Algorithm 6.1 is upper bounded
by q(L(G))(6; 7; 6; 7) L(G)=3 for input formula G, where q is a polynomial.
It remains to prove that Algorithm 6.1 performs correctly, i.e., each formula in CNF
reduced by REDUCE L satises at least one of the conditions of the steps (LS1){(LS4). We
need three simple lemmas concerning the output of REDUCE L to prove this statement.
Lemma 6.1 Let F be a formula in CNF, d be a literal occurring in it.
(1) If d is a (1;2 )-literal occurring in a 3 -clause, then L(DP d
(2) If d is a (1;1 )-literal occurring in a 2 -clause, then L(DP d 2:
(3) If d is a (1;1)-literal, then L(DP d 2:
Proof. By straightforward calculations. ut
Lemma 6.2 Let F 1 be the value of F at one of the steps (LR1){(LR5) of Function REDUCE L ,
F 2 be the corresponding output formula of Function REDUCE L , d be a literal occurring in
(1) If d is a (1;2 )-literal occurring in a 3 -clause, then
(2) If d is a (1;1 )-literal occurring in a 2 -clause, then 2:
Proof. (1). Note that any change of the formula at steps (LR1){(LR4) is a removal of some
clauses and/or a change of other clauses by their subsets, and thus results in decreasing the
length of the formula by at least one occurrence. If the formula does not change any more
before the step (LR5), then at least one occurrence will be removed by Lemma 6.1(1).
(2). Suppose that after F gets the value F 1 , Function REDUCE L modies the formula
before the step (LR4). This modifying cannot result in increasing the length of a clause or
in increasing the number of occurrences of a literal. Moreover, if the formula changes at
these steps, at least one occurrence is removed. If only one occurrence is removed, then at
least one of the literals d, d remains in the formula. If this is the occurrence of d, then by
Lemma 6.1(1) at least one more occurrence will be removed at step (LR5), and (2) holds.
Otherwise, d becomes a pure literal and will be removed at step (LR5), thus (2) holds again.
Suppose now that REDUCE L does not modify the formula before the step (LR5).
Then, (2) follows from Lemma 6.1(2). ut
We use the following simple properties of the formula reduced by Function REDUCE L
in the proof of 6.1 without explicit mentioning.
Lemma 6.3 Let G. Then
(1) F does not satisfy any of the conditions of the steps (LR1){(LR5);
(2) there are no 1-clauses in F ;
(3) there are no pure literals and (1;1)-literals in F ;
(4) there are no 2-clauses in F containing a 1-literal;
(5) for any literal d occurring in F , } d
for any 1-literal d occurring in F , } d +# d  7;
(7) for any literal d occurring in F , } d +# d  5.
Proof. (1) is trivial; (2){(7) since F does not satisfy the conditions of the step (LR1)and
see also Lemma 6.1. ut
Theorem 6.1 Algorithm 6.1 performs correctly and stops in the time p(L)(6; 7; 6; 7) L=3 <
p(L)2 0:10299L , where L is the length of the input formula and p is a polynomial.
Proof. We have shown above that it is su-cient to prove that each formula F in CNF reduced
by REDUCE L satises at least one of the conditions of the steps (LS1){(LS4). Suppose F
does not satisfy the conditions of the steps (LS1){(LS3). We now consider the two possible
cases: when F contains at least one 1-literal a, and when it contains no 1-literals. We prove
that in each of these cases F satises the condition of the step (LS4).
Informally, this is done as follows. In the rst case we show that the assignment I(a)
contains many literals. The second case is handled by careful examination of sub-cases: we
choose a 2-clause fc; dg in F and examine the following sub-cases:
there are many occurrences in the clauses containing the literal c (or d);
F contains many occurrences of the literal c (or d).
In these two sub-cases we show that many occurrences are eliminated during the transformation
of F into F [c]; F [c] (or F [d]; F [d]) and the subsequent elimination of 1-clauses
obtained from the clauses containing the literals c; c (or d; d). The third sub-case is:
none of the previous sub-cases holds.
In this sub-case one of c and d is a (2;2)-literal occuring only in 2- and 3-clauses which
allows us to use Lemma 6.1 after one occurrence of this literal is eliminated during the
splitting by another literal.
In the following we denote
F 2 and a literal a have the same meaning as they have at step (LS4) of Algorithm 6.1.
Case 1: The formula F contains a 1-literal a.
be the only clause containing a. Let
and } a +# a  7 by Lemma 6:3, we have # a  max(7 } a ;
be the number
of 3 -clauses among all clauses that contain a.
During the transformation of the formula F into F 2 , all occurrences of the literal a and
all clauses containing a will be eliminated. Thus,
+# a
5:
During the transformation of the formula F into F 1 , the clause D and all occurrences
of the literal a will be eliminated; all 2-clauses containing a will be eliminated too (at
step (LR1)). Since D is not blocked for any a i w.r.t. F , for each there exists
clause D i in F such that D i \ fa; a g. All these clauses will be eliminated
since all clauses fa; a i g are blocked for a w.r.t. F (i.e. fa; a clauses
containing a, all clauses D i and the clause D are distinct. Thus, we have
r+(7 r)+(# a
We now have i.e., the
condition of the step (LS4) is satised 1 .
Case 2: The formula F does not contain 1-literals.
Since F does not satisfy the condition of the step (LS3), it contains a 2-clause dg.
Let us note that
The formula F does not contain other clauses that contain
the literal d and the literal c, or the literal d and the
literal c, or the literal d and the literal c, simultaneously.
This proposition is true since F does not satisfy the conditions of the steps (LR2) and (LR4).
Also, in this case I(c) = fcg and I(d) = fdg by (6.2) and the denition of I(: : :).
Let us denote one of the literals c, d by a (we shall choose later which one). We denote
the remaining literal by b. Let  be the number of 3 -clauses among all clauses that contain
a (note that # a   1), let
be the number of 3 -clauses among all clauses that contain
a. We now count the occurrences that disappear during the transformation of the formula
F into F [I(a)] (= F [a]) and F [a] and subsequent elimination of 1-clauses resulted from the
2-clauses containing the literals a and a.
During the transformation of the formula F into F 1 , all occurrences of the literal a and
all clauses containing a will be eliminated; all 2-clauses that contain a will be eliminated too
(at step (LR1)), there are (# a
clauses.
1 Kullmann and Luckhardt [16] prove for a very similar algorithm that in this case
thus, (5; 17) in the condition of the step (LS4) can be replaced by (8; 12). However, this would not improve
our upper bound while would make the proof longer; thus, we present here this simpler proof.
The formula F [a] contains the 1-clause fbg obtained from the 2-clause D. It will be
eliminated at step (LR1). If F contains other 2-clauses containing a, the clauses obtained
from them will be eliminated at step (LR1) too. The number of such clauses is (# a 1 ).
During the transformation of the formula F into F 2 , all occurrences of the literal a and all
clauses containing a or b will be eliminated. By (6.2), D is the only clause containing b in
which the literals a; a can occur. Thus, even if we count only occurrences that disappear in
clauses containing a or a before the rst application of the step (LR2), we have
We consider several sub-cases.
Case 2.1: } c  6 and } d  6.
From (6.3) and (6.4) we have
Thus, i.e., the condition of the step (LS4) is satised.
Case 2.2: # c  3 or # d  3.
Let a 2 fc; dg be the literal for which # a  3 holds, let b be the remaining literal in fc; dg.
Thus, we have
a +# a  22:
Thus, i.e., the condition of the step (LS4) is satised.
Case 2.3: # or } d  5.
Let a 2 fc; dg be a literal such that } a be the remaining literal in
fc; dg. Then 4  } b  5 and } a  } b . In this case b is a 2-literal, it occurs in D and in one
other 2- or 3-clause. We denote this clause by C b .
The literal b becomes a (1;2)-literal in F [I(a)] (= F [a]). We now complete the proof
by showing that several literal occurrences will be eliminated in addition to the occurrences
counted by (6.3). There are three sub-cases.
Case 2.3.1: At least two occurrences are eliminated before the rst application of step (LR2)
in addition to the occurrences counted by (6.3).
In this case,
+(# a )9;
a +# a  20:
Case 2.3.2: Exactly one occurrence is eliminated before the rst application of step (LR2)
in addition to the occurrences counted by (6.3).
We remind that (6.3) counts only occurrences to the clauses containing one of the literals
a,a. By (6.2) the occurrence of the literal b to the clause C b or any occurrence of the literal
b cannot be eliminated during the transformation of the formula F into the formula F [a]. If
we counted any of these occurrences in the # a
term of (6.3), then it was an elimination
of a 1-clause fbg obtained from a 2-clause fa; bg. Since it is impossible to have two distinct
identical clauses fa; bg in the formula, the other two occurrences (another occurrence of the
literal b and the occurrence of the literal b in C b ) eliminated at this moment are not counted
by (6.3); this fact contradicts the assumption of Case 2.3.2.
Thus in this case b remains a (1;2)-literal before the rst application of the step (LR2). By
Lemma 6.2(1) at least one more occurrence will be eliminated after that and thus (6.5){(6.7)
hold.
Case 2.3.3: No occurrences are eliminated before the rst application of step (LR2) in
addition to the occurrences counted by (6.3).
Similarly to Case 2.3.2, b remains a (1;2)-literal before the rst application of the step (LR2)
during transformations of the formula F [a] by Function REDUCE L . Thus, if jC b
two more occurrences are eliminated by Lemma 6.2(2), and (6.5){(6.7) hold.
If, however, jC then we can apply only Lemma 6.2(1), but in this case } a  }
and thus
10;
Hence, in all three sub-cases 2.3.1{2.3.3, i.e., the condition
of the step (LS4) is satised. ut
7 Conclusion and further work
In this paper we have improved the existing upper bounds for SAT with respect to K (the
number of clauses) and L (the length of a formula). The key point in our algorithms and
proofs is the black and white literals principle, a new transformation rule which can be viewed
as a re-formulation of two previously known principles: the autarkness [20, 22, 17, 15] and
the generalized sign principle [16].
Our proofs (as well as the proofs of the previous upper bounds [21, 16]) are neither short
nor elegant. It would be a kind of breakthrough to nd a compact way to present proofs
of upper bounds for splitting algorithms. It is believable that such a way could lead to
even better bounds, since currently our possibilities to create new heuristics and prove the
corresponding upper bounds are limited by the length of a comprehensible proof. On the
other hand, it is a challenging problem to prove (more) tight lower bounds for (some class
splitting algorithms: currently, the exponential lower bounds for resolution proofs (see,
e.g., [27]) are far enough from the known upper bounds for splitting algorithms. (Most of
splitting algorithms can be viewed as resolution proofs and vice versa.)
Another direction for work is to nd randomized algorithms that give better upper bounds
for SAT (or to nd a way how to apply modern randomized algorithms that have already been
invented for 3-SAT in recent breakthrough papers [23, 24]). Also, it remains a challenging
problem to nd a less-than-2 N upper bound where N is the number of variables.



--R

Tautology proof systems based on the splitting method
Less than 2 n satis
Approximation algorithms for Max Sat: a better performance ratio at the cost of a longer running time
Exponential upper bounds for the satis
A machine program for theorem-proving
A computing procedure for quanti
Algorithms for Satis
Separating the signs in the satis
Two new upper bounds for SAT
Local Search Algorithms for SAT: Worst-Case Analysis
Hard formulas for SAT local search algorithms
SAT local search algorithms: worst-case study

New methods for 3-SAT decision and worst-case analysis
Investigations on autark assignments
Algorithms and their complexity
Obere Komplexit
Parametrizing above guaranteed values: MaxSat and Max-Cut
New upper bounds for MaxSat

Upper bounds for covering problems
Solving satis

An Improved Exponential-time Algorithm for k-SAT
Solving 3-satis ability in less than 1:579 n steps
Pure literal look ahead: An O(1:497 n
The Complexity of Propositional Proofs
A satis
--TR

--CTR
Rainer Schuler, An algorithm for the satisfiability problem of formulas in conjunctive normal form, Journal of Algorithms, v.54 n.1, p.40-44, January 2005
Bolette Ammitzbll Madsen, An algorithm for exact satisfiability analysed with the number of clauses as parameter, Information Processing Letters, v.97 n.1, p.28-30, January 2006
Evgeny Dantsin , Andreas Goerdt , Edward A. Hirsch , Ravi Kannan , Jon Kleinberg , Christos Papadimitriou , Prabhakar Raghavan , Uwe Schning, A deterministic (2 - 2/(k+ 1))n algorithm for k-SAT based on local search, Theoretical Computer Science, v.289 n.1, p.69-83, 23 October 2002
Ramamohan Paturi , Pavel Pudlk , Michael E. Saks , Francis Zane, An improved exponential-time algorithm for k-SAT, Journal of the ACM (JACM), v.52 n.3, p.337-364, May 2005
Tobias Riege , Jrg Rothe , Holger Spakowski , Masaki Yamamoto, An improved exact algorithm for the domatic number problem, Information Processing Letters, v.101 n.3, p.101-106, February, 2007
Ryan Williams, Algorithms for quantified Boolean formulas, Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms, p.299-307, January 06-08, 2002, San Francisco, California
Haiou Shen , Hantao Zhang, Improving exact algorithms for MAX-2-SAT, Annals of Mathematics and Artificial Intelligence, v.44 n.4, p.419-436, August    2005
Jens Gramm , Edward A. Hirsch , Rolf Niedermeier , Peter Rossmanith, Worst-case upper bounds for MAX-2-SAT with an application to MAX-CUT, Discrete Applied Mathematics, v.130 n.2, p.139-155, 15 August
Edward A. Hirsch, Worst-case study of local search for MAX-k-SAT, Discrete Applied Mathematics, v.130 n.2, p.173-184, 15 August
Laurent Simon , Daniel Le Berre , Edward A. Hirsch, The SAT2002 Competition, Annals of Mathematics and Artificial Intelligence, v.43 n.1-4, p.307-342, January 2005
Oliver Kullmann, Lean clause-sets: generalizations of minimally unsatisfiable clause-sets, Discrete Applied Mathematics, v.130 n.2, p.209-249, 15 August
Rolf Niedermeier , Peter Rossmanith, An efficient fixed-parameter algorithm for 3-hitting set, Journal of Discrete Algorithms, v.1 n.1, p.89-102, February

extracted:['feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-containment'
 'finite automata' 'zero storage biometric authentication' 'semantics']
marked:['SAT', 'worst-case upper bounds']
--T
Alternative Translation Techniques for Propositional and First-Order Modal Logics.
--A
We describe and analyze techniques, other than the standard relational/functional methods, for translating validity problems of modal logics into first-order languages. For propositional modal logics we summarize the &squ;-as-Pow method, a complete and automatic translation into a weak set theory, and then describe an alternative method, which we call i>algebraic, that achieves the same full generality of &squ;-as-Pow but is simpler and computationally more attractive. We also discuss the relationships between the two methods, showing that &squ;-as-Pow generalizes to the first-order case. For first-order modal logics, we describe two extensions, of different degrees of generality, of &squ;-as-Pow to logics of rigid designators and constant domains.
--B
Introduction
Translation techniques have oered valuable aid for the study and development
of theorem provers for non-classical logics, and their nave
use can be ascribed to tradition. The rst systematic study of the relational
translation was carried out within Correspondence Theory (van
Benthem, 1983). In later years further studies of the relational and similar
techniques for computational purposes were initiated ((Ohlbach,
1991; Ohlbach, 1993; Nonnengart, 1993); see also (Ohlbach et al., 2001)
for a comprehensive survey). The eld has revealed itself to be fruitful
and interesting.
The relational translation of a modal
the rst-order formula RT('; x) (it has also been called ST , for \stan-
dard translation", but we will reserve this designation for \set-theoretic
inductively dened by
c
2001 Kluwer Academic Publishers. Printed in the Netherlands.
Alberto Policriti, and Matteo Slanina
Here, each propositional variable p i is mapped one-to-one to a unary
predicate symbol P i . Let RT(') be
If
axiomatizes a frame-complete logic, then
Here  f denotes validity of ' in the logic axiomatized by
, and
denotes classical (in this case second-order) validity. The second-order
quantiers in RT(') can be removed, since they are universal and appear
only as prex to the consequent of an implication. However, the
second-order character of RT(
is, in general, not eliminable, since,
in this case, the second-order (universal) quantication appears in the
antecedent of an implication. This makes the technique inapplicable for
computational purposes, since the target language is highly undecidable
(non-arithmetic, cf. (van Benthem, 1983)). Whenever RT(
can be
proved equivalent to a rst-order formula, the logic axiomatized by
is
said to be rst-order denable and the modal validity problem can be
solved by a rst-order theorem prover. Determining when RT(
a rst-order equivalent is the main topic of correspondence theory. In
some cases it can even be done automatically (e.g. Sahlqvist formulae,
the SCAN procedure (Gabbay and Ohlbach, 1992)). Unfortunately,
not all modal logics have a rst-order (frame) correspondent, and, even
when they do, there is no complete algorithm to nd one. A lot of work
has been done to avoid these di-culties, resulting in some extensions
of the class of tractable logics and, especially, in improved performance
(Nonnengart, 1993; Ohlbach, 1998). In particular, (Ohlbach,
1998) proposed a way to incorporate axioms and rules of Hilbert calculi
into relational translations, thus creating a hybrid system able to deal
with the axioms that cannot directly be captured by the translation.
A dierent perspective on the problem is taken in the approach
proposed in (D'Agostino et al., 1995), where the 2-as-Pow translation
was rst introduced. The novelty of 2-as-Pow lies in the fact that,
instead of determining which modal theories can be mapped into a rst-
order language, a fragment of second-order logic is used as a framework
for the translation. It turns out that the fragment required is very
small and can be specied as a weak rst-order axiomatic set theory.
Under this theory, the resulting translation is sound and complete for all
frame-complete (not only with respect to a rst-order class of frames)
Alternative Translation Techniques for Propositional and First-Order Modal Logics 3
logics. More specically, given a frame-complete logic with axiom
and
a formula ', the 2-as-Pow translation (
is such that
where
is the weak axiomatic set theory that will be dened in Section
2. Several extensions of this method were discussed in subsequent
papers (Montanari and Policriti, 1997a; Montanari and Policriti, 1997b;
van Benthem et al., 1998), including a variation (van Benthem et al.,
1997) that allows for completeness for any logic (not necessarily frame-
complete) and another with strong decidability results (D'Agostino
et al., 1996).
In this paper we systematically present set-theoretic and algebraic
translation techniques that are less restrictive than RT and its vari-
ations, and whose range includes propositional modal logics as well
as an important class of rst-order modal logics. After a short review
of the original 2-as-Pow translation (Section 2), we introduce
a new, very natural, translation method, that we call algebraic. This
algebraic translation, suggested by the algebraic semantics for modal
logics, achieves completeness for all logics in a simpler way than 2-
as-Pow. We point out strong relationships between 2-as-Pow and the
algebraic method, showing a new way to obtain the former from an
algebraic point of view (Section 3). We show that, in a certain sense to
be made precise later, 2-as-Pow and the algebraic translation coincide
at the propositional level. However, no such strong relationship holds
when we try to extend these methods to handle rst-order modal log-
ics: 2-as-Pow can be so extended, but for the algebraic method there
seems to be a signicant stumbling block (unless we allow the target
language of the translation to be second-order, which is unacceptable).
The relational translation and its derivations can be extended, in their
restricted setting, to rst-order modal logics in a fairly straightforward
manner. In Section 4, we collect and organize some results (Montanari
et al., 2000a; Montanari et al., 2000b) on the extension of 2-as-Pow
to the important class of rst-order modal logics of rigid designators
and constant domain, which is signicant given the aforementioned
observations. Section 5 contains some lengthy proofs that would have
broken the continuity of the exposition if put in the main text. We
conclude with some discussion on open problems and possible future
work.
We have tried to use a commonly understood symbolism. Some less
common notations are the ones for logical
syntactic derivation in classical and modal logic;
4 Angelo Montanari, Alberto Policriti, and Matteo Slanina
'Ks syntactic derivation in the modal calculus K s , that is, K
plus the Instantiation Rule;
truth in a model, semantic consequence in classical logic;
truth in a world or model, frame semantic consequence
in modal logic.
2. 2-as-Pow Translation (Propositional Case)
The 2-as-Pow translation, at the propositional level, can be described
as an extension to the 2 operator of the set-theoretic interpretation
of the propositional connectives ^, _, and :. The approach brie
y
described here was rst presented in (D'Agostino et al., 1995) and
subsequently developed in (van Benthem et al., 1997) and (van Benthem
et al., 1998). The key idea is simply to replace the accessibility
relation R of the possible world semantics by the membership relation
2. Accordingly, a world y accessible from x becomes an element of
x, and taking a further step from y via R amounts to moving into y
to inspect one of its elements. This viewpoint allows us to identify a
frame F with its support w. Moreover, since we clearly want all worlds y
accessible from a given world x in a frame w to be themselves elements
of w, it is natural to require that w be a transitive set: w  P(w).
Valuations for propositional variables, i.e. sets of worlds, can now be
identied with worlds themselves. 2-as-Pow is designed in such a way
that the translation '  of a compound formula ' turns out to be the
set
of all worlds where ' is true.
For propositional connectives we simply follow the Boolean tradition
and say that
In order to complete the denition it is su-cient to calculate the Kripke
semantics of the 2 operator replacing R by 2:
which is to say: 2' holds true at x if and only if x is a subset of '  .
This allows us to complete our denition by putting
Alternative Translation Techniques for Propositional and First-Order Modal Logics 5
It is possible to show that the
collection
of the following set-theoretic
principles su-ces for proving the soundness and completeness
of the translation:
z
Dening Trans(w) as w  P(w) it can be shown that
When a modal logic is characterized by a nite collection of Hilbert
axioms, whose conjunction is
, these results generalize to
where
Axiom
(w) stands for 8~x (w
), where ~x is the list of all the variables
in
distinct from w.
Thus when  is frame-complete an exact correspondence between
modal and set-theoretic derivability is guaranteed
It turns out that the
theory
is almost minimal in providing a
formal counterpart to the idea underlying the 2-as-Pow translation.
Let us call MM (an acronym for \Minimal Modal") the theory whose
axioms result from those
of
by replacing x 2 P(y) $ x  y with the
following two theorems of
1 As a technical note, if we were to deal with a logic with an innite axiom set
, we could reformulate the result as follows (use skolemization on w/c and the
Deduction Theorem for rst-order logic to obtain the equivalence):
6 Angelo Montanari, Alberto Policriti, and Matteo Slanina
where u \ v is an abbreviation for u n (u n v). Although MM is even
weaker
than
it can still drive the 2-as-Pow translation. A further
possible simplication would be to eliminate the binary union symbol,
observing that, by exploiting complementation, it can be dened in
terms of set dierence.
Theorem 1 (Completeness of 2-as-Pow). If
'Ks ', then MM
Theorem 2 (Soundness of 2-as-Pow).
If
'.
3. Algebraic Translation (Propositional Case)
The same mental shift that proceeds from relational semantics to the
relational translation enables one to discover the algebraic translation
as a straightforward rewriting of algebraic semantics in syntactic terms.
The basic objects playing the role of Kripke frames are Boolean algebras
with one extra operator (Bull and Segerberg, 1984; Blackburn et al.,
2001).
Denition 3. Consider the rst-order language of Boolean algebras
(with meet u, join t, complementation 0 , bottom element 0, and top
element 1) and an additional operator ' . Dene the theory MA (for
Modal Algebra) to be the rst-order theory of Boolean algebras with
the two additional axioms:
Denition 4. Let us map each propositional symbol p to a distinct
individual variable x p of the target rst-order language and dene
the algebraic translation ' # of a modal formula ' by the following
structural induction:
Alternative Translation Techniques for Propositional and First-Order Modal Logics 7
Based on this denition and following the truth denition of algebraic
semantics, we put
It is straightforward to prove that this translation is sound and complete
for all (nitely axiomatizable) modal logics, since the algebraic
semantics is complete for all logics (in fact, it is equivalent to the general
frame semantics).
Theorem 5. Given a modal logic with axiom
and a formula ',
The algebraic translation converts a validity problem in any modal
logic to a validity problem of rst-order logic, thus permitting the use
of any rst-order theorem prover. As a validity checking method, this
is slower than the use of the relational or functional translation, when
the latter is applicable, but it has the advantage of being a completely
syntactic method and it can turn out to be extremely useful for experimenting
with new logics. 2 A further advantage of this technique
is that it also works, with no modication, for multi-modal logics:
just introduce one ' operator for each of the modalities involved and
augment the underlying theory with a pair of axioms, of the form of (1),
for each of them.
What we are left with after the algebraic translation is a deduction
problem in equational rst-order logic. Tools that deal with such prob-
lems, such as EQP, are available and can be used for modal theorem
proving based on the algebraic translation. Equational problems can be
solved particularly e-ciently when rewriting techniques can be applied.
In the case of modal logic, Foret (Foret, 1992) discovered con
uent
(modulo associativity and commutativity of the Boolean operators)
and terminating rewriting systems for the logics K, D, T, and S5. The
rewriting systems are oriented versions of the equational axioms of
modal algebras and can be seen as a specialized solver for the algebraic
translation of the formulae. For example, Foret's system for K adds the
oriented versions of (1) to Hsiang's system for propositional logic|
an oriented version of the equational theory of Boolean rings with
multiplicative identity (Hsiang, 1982; Hsiang, 1985):
We did some simple experimentation with a raw algebraic translation given to
EQP (McCune, 1997), and were able to prove 31 out of the 72 problems of (Gore
et al., 1997) in seconds (at most a few minutes in a few cases) on a Pentium class
computer.
8 Angelo Montanari, Alberto Policriti, and Matteo Slanina
3.1. Relations with 2-as-Pow.
If one looks at the original formulation of the 2-as-Pow translation,
there is one peculiar feature: the membership symbol 2 does not appear
explicitly. In fact, the translation seems to be expressed in an almost
equational language and looks very much like the algebraic translation.
Moreover, the axioms of MM are strikingly similar to those of modal
algebras.
We will now prove that these remarks have indeed a deep under-
pinning: there is an equational theory of sets, capturing the Boolean
properties
of
and MM , that proves the propositional 2-as-Pow translation
sound and complete. In other words, at the propositional level,
the algebraic and the 2-as-Pow translation are essentially equivalent.
However, as will be shown in the next section, there is no simple way
to lift this equivalence to the rst-order level.
One would be inclined to use, as an equational set theory, a theory
of Boolean algebras. However, we cannot hope to have a complementation
function, since there is no constant for a top element. Hence, we
add a relativized complementation which mimics the behavior of set
dierence. This turns out to be all we need and the following axioms
are su-cient.
Denition 6. We dene the
theory
by the
universal closure of the following axioms:
Intuitively, the rst ten axioms describe a theory of \Boolean algebras
without maximum element". It can be shown that the models of
these axioms are such that the cone of elements below any given element
is a Boolean algebra. This kind of structure is exactly a Boolean ring
in the sense of, e.g., (Jacobson, 1951), that is, an idempotent ring with
no multiplicative null element. This allows us to establish a two-way
correspondence between models
of
and models of MA. Using this
correspondence and the 2-as-Pow translation, which can be applied
directly to the language
of
, we can prove the following equivalence:
Alternative Translation Techniques for Propositional and First-Order Modal Logics 9
Theorem 7. Consider a nitely axiomatizable modal logic, with axiom
, and a modal
Proof. See Section 5.
Corollary 8. If
axiomatizes a frame-complete modal logic, then
Proof.
'Ks ' by soundness and completeness of 2-
as-Pow. By soundness and completeness of the algebraic translation
(Theorem 5),
To conclude apply Theorem 7.
4. 2-as-Pow Translation (First-Order Case)
The analogy between the 2-as-Pow and the algebraic translation does
not lift from the propositional to the rst-order level. An extension
of the algebraic method to rst-order modal logics would necessitate
employing a theory of complete Boolean algebras, which is second-
order, therefore leading again to problems of the same sort as those
mentioned in Section 1. However, we have successfully extended 2-
as-Pow to an important class of rst-order modal logics maintaining
the target language of the translation at the rst-order level. Hence,
2-as-Pow and the semantics it is based on can be seen as genuine generalizations
of the algebraic models; only at the lowest (propositional)
level do the two happen to be equivalent.
Here we describe two extensions of the 2-as-Pow translation method
to the class of rst-order modal logics with rigid designators and xed
domains, known as Q1 systems (see (Garson, 1984; Fitting, 1993; Fitting
and Mendelsohn, 1998)). Informally, the language of such logics is
a rst-order language together with a 2 modality. A model M consists
of a frame hW; Ri (possibly with restrictions dened by an underlying
propositional modal logic) plus a rst-order logic interpretation with
a single domain D, an interpretation FC for constants and one F F
for function symbols, and|the only part that varies with worlds|the
interpretation F P of predicate symbols:
There is a sound and complete calculus for any such logic, obtained
by adding to the propositional calculus for the underlying modal logic
Alberto Policriti, and Matteo Slanina
the axioms and rules of rst-order logic|the Universal Quantier Axiom
schema and the Universal Generalization Rule|and the Barcan
for every formula '(x).
Both translations will be targeted to a two-sorted rst-order logic.
This is done essentially for better understanding and ease of implemen-
tation: in principle everything could be done in a one-sorted language
(see (Slanina, 2001) for details; for a general discussion of the reducibility
of many-sorted to one-sorted logic see also (Davis, 1993)). If L is the
rst-order modal language, we dene L 0 to be a two-sorted language
where the sort term denotes L-terms and the sort set denotes objects
of the translation (possible worlds and sets thereof). The underlying
theory
is modied accordingly, with the third axiom now becoming
8x8y
We call this modied
theory
.
The rst translation (Montanari et al., 2000a) works for the fragment
of the Q1 language where modal operators cannot appear in the
scope of quantiers (below: \locally quantied formulae"). This case
appears often in practice and can be treated using the observation
that sentences can be decomposed into two disjoint parts: one rst-
order, but classical, and the other modal, but propositional. 3 More
precisely, a sentence of this fragment can be written as
are classical rst-order sentences and the
formula we obtain by replacing each ' i by a propositional variable p i ,
for short), is a propositional modal
formula. The simple, but crucial, observation at this point is that
derivability can be split accordingly, as stated in the following lemma.
Lemma 9. There is a nite set  of classical propositional formulae
(over the variables
'g for all  2 ,
and, for all ,
Proof (sketch). Clearly,
3 This is the case, for instance, of Manna and Pnueli's temporal frame-work
(Manna and Pnueli, 1995), where most specications can be conned to this
restricted form of quantication.
Alternative Translation Techniques for Propositional and First-Order Modal Logics 11
Let  be the set of propositional formulae on that are valid
under the substitution fp 1 7! ' g. Then, it is easy to see
that the right-hand side of (2) is equivalent to   (p
nite modulo logical equivalence.
Using the above results, the translation is dened as follows.
Denition 10. As an auxiliary notation, we introduce a transformation
that adds an extra variable (representing a world in a Kripke
frame) to the arguments of each predicate symbol ( ~ t are terms):
Now x a variable w of sort set. The translation (
of a locally
quantied respect to the logic
axiomatized by
, is expressed as follows:
where
Here Axiom 2
(w) and  are the same as in the propositional case, with
all bound variables of sort set.
The adequateness theorems are similar to the ones for the propositional
case:
Theorem 11 (Completeness).
Theorem 12
(Soundness).
Q1 F .
The proofs of these theorems can be found in (Montanari et al.,
2000a). We omit them here, since our next translation is stronger.
The second translation works for the full Q1 language and is developed
along a dierent line.
Alberto Policriti, and Matteo Slanina
Denition 13. The set-theoretic translation (
with respect to the logic with axiom
(a propositional modal
is the formula
Trans(w) and Axiom
(w) are the same as in the propositional case,
while  ' (w) encodes the semantics of quantiers. The notation is as
follows: variables occurring|free or not|in ' are denoted by ~x, which
stands for x are fresh function symbols f for some subformulae
of ' (to be specied below), with signature set term
set. Each term t , with subformula of ', represents the set of worlds
where is true, in the frame w and under variable assignment ~x.
Formally, let be a superset of the set of the variables
appearing in '. Then
are
the free variables in ', with
2.
' (w) is simply the conjunction, over all subformulae   of ', of the
formulae  (w), where  is dened as
(w) => > > > <
if is 8x i  and x
are the free variables in
, with
otherwise.
In analogy to the propositional case, we now have the following
theorems, whose proofs are given in Section 5.
Theorem 14 (Completeness).
Theorem 15
(Soundness).
Example. Let ' be the formula
The superscripts mark the main operator of each of the eight subformulae
of '. Let
be the axiom of seriality,
Alternative Translation Techniques for Propositional and First-Order Modal Logics 13
some freedom of notation (e.g., we use 9 , whose direct translation is
obtained patching the one for 8 in the obvious way), the translation is
the following:
Axiom
5. Proofs
5.1. Proof of Theorem 7.
We only give a sketch of the proof. The interested reader is referred
to (Slanina, 2001) for the details.
\Only if". The proof is by contradiction: assume
that
i.e., by completeness of rst-order
logic,
. This means there
exists a model A
of
E and an assignment  on A such that, putting
a  A P A (a) ;
Let B be the model for MA dened in the obvious way taking as
domain the lower cone of A with supremum a (see (Slanina, 2001) for
details). Each term t in the language
of
E such that any occurrence of
n has w as its left argument can be mapped to a term t 0 in the language
of MA, and the assignment  can be mapped into the assignment  0
on B dened by  0 in such a way that
and
14 Angelo Montanari, Alberto Policriti, and Matteo Slanina
It is then straightforward to prove that B 6 (
\If". The argument is analogous to the \only if" case, proceeding
by contradiction from a model A of MA such that A  ~ 8(
building a model B
of
E such that B 6 (
5.2. Proof of Theorem 14.
We need a preliminary lemma, whose omitted proof directly follows
from Denition 13.
Lemma 16. If there is no free occurrence of x i in ', then there is no
occurrence of x i in t ' (w; ~x).
The proof of the theorem is by induction on the derivation of '.
Propositional Axioms. The proof is exactly the same as the one for
the propositional 2-as-Pow translation (cf. (D'Agostino et al., 1995)).
Universal Quantier Axioms. Let us assume, without loss of gener-
ality, that we are instanciating the variable x 1 and that the other free
variables in
where the substitution fx 1 7! sg is free for x 1 in (x 1 ). Let  be
and
Assume  ' (w) (Trans(w) and Axiom
(w) are not needed in the
proof). We shall prove that w  t ' (w; ~x) by showing that, whenever
From Def. 13, we have that t
that is, taking
the negation out of the quantier,
with  ' (w) (the   (w) conjunct), this implies y 62 f  (w; x
Modus Ponens and Necessitation Rule. Both proofs are basically the
same as those for the propositional case. Refer, again, to (D'Agostino
et al., 1995).
Alternative Translation Techniques for Propositional and First-Order Modal Logics 15
Universal Generalization Rule. Assume the same notation as with the
Universal Quantier Axiom schema. The rule has then the form
does not occur free in '. Let  be the subformula 8x 1 . From
Def. 13, we get
Assume  '!8x 1 (w) (Trans(w) and Axiom
(w) are not needed in
the proof). We shall prove that w  t '!8x 1 (w; ~x) by showing that,
necessarily y 2 f  (w; x
Assume y 2 w and y 2 t ' (w; ~x). By induction hypothesis, w  (w n
contains no occurrence of x 1 ; hence, we can apply classical
Universal Generalization to conclude that
thus, from the conjunct   (w) of the assumption  '!8x 1 (w), that
Barcan Formulae. Let us call
and  the formulae 8x 1 2 respectively, and let
xm be the free variables in  (or, equivalently, in ). Then
Assume  ' (w) and y 2 w. Then
_ 8z: set (z 2 y
_ 8z: set (z 2 y
Alberto Policriti, and Matteo Slanina
The rst equivalence is just the unfolding of the denition of t ' (w; ~x) by
the axioms
of
2 . In the second we dropped the subformula y 2 w from
the conjunct because it is true by assumption, and then we rewrote the
rst disjunct using   (w) and the second using
the
In
the last, we used the denition of  in the left disjunct and   (w) in
the right one.
Now, to show that w  t ' (w; ~x), we take, as above, y 2 w and assume
that the second disjunct of the last line is false, which is equivalent
to
This obviously implies the rst disjunct, thus proving the claim.
5.3. Proof of Theorem 15.
The proof is an adaptation of the technique introduced in the soundness
proof of the propositional case in (D'Agostino et al., 1995). More pre-
cisely, we are going to follow the proof of Theorem 5 of that paper. The
rst part is almost the same and we will state some lemmas without
proof.
Let U be a universe of hypersets satisfying all the axioms of ZF FA
(ZF except the Foundation Axiom) and AFA (see (Aczel, 1988) for
details).
Lemma 17. Let  be an ordinal, V  be the set of all well-founded sets
of rank less than , and U n V  be the universe of all hypersets not
belonging to V  . Then any model for the language
of
2 with domain
set and interpretation function () 0 satisfying
is a model
of
.
Proof. See Lemma 6 of (D'Agostino et al., 1995).
Given a frame (W; R; D), we want to embed it into the universe
suitable . In (D'Agostino et al., 1995), the authors
prove the existence of a labeled decoration  such that the following
lemma holds.
Alternative Translation Techniques for Propositional and First-Order Modal Logics 17
Lemma 18. For each a; b 2 W ,
1. a
2. a  62 V +1 and a  n V aRbg.
Proof. See (ii) and (iii) in Lemma 7 of (D'Agostino et al., 1995).
Now consider an interpretation
an assignment  : f~xg ! D. Moreover, let M be the Q1 model
A model for the language
of
2 can be obtained as follows. First,
take the interpretation for set symbols as dened by the previous
lemmas and let D be the domain for the sort term. Then, interpret
term constant and function symbols as in FC and F F , respectively,
and, for each k-ary relational symbol Q, let f
be equal to
if this set is not empty, and to V +1 otherwise.
Finally, for each universally quantied formula
variables x j 1
be equal to
if this set is not empty, and to V +1 otherwise.
We call this model AM . Furthermore, let W  be equal to fa  j
a 2 Wg. W  62 V +1 because V +1 is transitive and, for each a 2 W ,
a  62 V +1 (Lemma 18).
Lemma 19. We have:
1. for each a 2 W , d
if and only if
a
2. AM   ' (W  ).
Proof. 1. Let The proof is by induction on '.
immediate from the denition of f
induction hypothesis) a
d) or a  2 0 t
d) ,
(for AM is a model
of
d)
d) , (by
denition of t ' ) a  2 0 t
d).
Alberto Policriti, and Matteo Slanina
d)
, (for a  2 0 W  ) a  2 0 W  and a  62 0 t
d) , (for AM
is a model
of
d) , (by denition of t ' )
a
d).
aRb , (by i.h.) for every b 2 W such that aRb, b  2 0 t
d)
d) , (by denition of
(by denition of t ' )
a
d).
immediate from the denitions of f
' and t ' .
2. Assume, without loss of generality, that
be the free variables in '. We are going to prove  ' (W  ). Let
a 2 W and d be arbitrary elements of D. Then a  2 0
f
(by denition of f
' , for every d 1 2 D, a; [x 1 =d (for the
rst part of this lemma) for every d 1 2 D, a
Corollary 20.
if and only if
Proof. Follows directly from Lemma 19 recalling that M Q1 L '
means that M Q1 L 8~x '.
Lemma 21. A modal propositional formula
, over the propositional
variables is valid in the frame (W; R) if and only if, for the
corresponding hyperset W  ,
holds in U n V +1 .
Proof. See Lemma 9 of (D'Agostino et al., 1995).
To conclude the proof of Theorem 15, let us suppose that
4 By Lemma 16, all the d's beyond the m-th do not count in the interpretation.
Alternative Translation Techniques for Propositional and First-Order Modal Logics 19
be a model based on a frame in which
is valid (a Q1-L model). From Lemma 21 it follows that Axiom 2
is true in AM , and from the second part of Lemma 19 the same follows
for  ' (W ). Furthermore, it is easy to prove that Trans(W  ) holds
as well. Since AM is a model
of
and, by hypothesis, the formula
8~x: term W   0 t
is true in AM , it follows from Corollary 20
that M Q1 L '.
6. Conclusions and Open Questions
All the results presented in this paper uniformly rewrite the deduction
problem for various families of modal logics into deduction problems
with some form of set-theoretic
avor. The main contributions are the
proposal of an equational theory capable of driving the translation at
the propositional level and the extension of the 2-as-Pow translation
technique to a signicant class of rst-order modal logics.
A systematic study of set-theoretic translation techniques for different
kinds of rst-order modal logics could be carried out starting
from the ideas presented here. In this paper, we deliberately conned
ourselves to one of the most signicant classes. Moreover, we did not
consider the extension of the algebraic method to the rst-order case:
although we did not see any particular obstacle in principle, its development
would almost certainly have called for the introduction of
a very ad-hoc (two-sorted, non-equational) extension of the theory of
Boolean rings, thus stripping the method of its elegance (which was the
main motivation for its use in the propositional case).
All of the results mentioned call for specialized techniques explicitly
designed for deduction in such theories
as
or, better yet, able
to exploit the syntactic features of the formulae obtained from the
translation. Preliminary work in this direction is presented in (Piazza
and Policriti, 2000), where a tableaux-based decision procedure for a
class of formulae in which the resulting formulae can be embedded is
presented. Other related results can be found in (Cantone and Zarba,
2000; Cantone et al., 2001).
A further step toward the applicability of the results presented could
be obtained via a higher level of integration of the set-theoretic translation
with more classical techniques. For example, it would be interesting
to study the possibility of separating the rst-order denable portions
of a logic (for which more e-cient techniques are often available) from
contexts in which the 2-as-Pow is the only available translation. In this
context, we mention as an open problem, the question of precisely char-
20 Angelo Montanari, Alberto Policriti, and Matteo Slanina
acterizing the expressive power of locally quantied rst-order modal
logics (cf. Lemma 9).
Finally, another research task, related to the quest for specialized
techniques designed to be coupled with the translations presented here,
is the search for reduction orders and rewriting systems to be used in
automated engines for testing derivability in the
theory
introduced
in Section 3.

Acknowledgments

We wish to thank prof. Martin Davis for a careful proof-reading of the
nal draft of this paper.



--R


Modal Logic and Classical Logic.


Modal Logic

Set Theory for Computing.
From Decision Procedures to Declarative Programming with Sets








Handbook of Philosophical Logic






Journal of Arti
Lectures in Abstract Algebra
Temporal Veri
'EQP 0.9c User's Guide'.






Journal of Logic and Computation 1(5)

Bulletin of the IGPL 1(1)





Technical Report 07/01
--TR

extracted:['feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-containment'
 'finite automata' 'zero storage biometric authentication' 'semantics']
marked:['first-order modal logics', 'computable set theories', 'translation methods']
--T
Checking Simple Properties of Transition Systems Defined by Thue Specifications.
--A
In (possibly infinite) deterministic labeled transition systems defined by Thue congruences, labels are considered as functions of states into states. This paper provides a method for computing domains of such functions for a large class of transition systems. The latter are related to model checking of transition systems defined by Thue congruences.
--B
Introduction
Countable transition systems can model any software or digital hardware system and appear
as one of the most fundamental structures in computer science. While the research on finite
transition systems has a long tradition (see e.g. [1] for further references), infinite transition
systems have only recently attracted attention of computer scientists. One of the basic
problems concerning infinite transition systems is their effective description. We argue that
Thue systems may be used for defining both finite and infinite transition systems and semi-
Thue systems may be used as mechanization tools for reasoning about underlying transition
systems.
When a transition system models a real-life system, the properties of the latter correspond
to the theory of the transition system within some suitable logic. The falsity of a sentence
on a given transition system amounts to the emptiness of a certain set, namely the meaning
of the sentence. Roughly speaking, this set is obtained as a combination of the elementary
relations that interpret atomic formulae. Such a combination is computable whenever so are
elementary relations and the operators of the logic are simple enough. In particular, this is
the case for the Hennessy-Milner logic [9] where the elementary relations are unary and the
operations are boolean ones. To establish the truth of a sentence, the central problem is then
to compute the elementary relations. This is the subject of the present paper.
Our work is related to the well established grounds of formal verification which are based
on the connections between logic and formal languages (see e.g. [19]). In that approach, the
truth of a sentence on a finite transition system is checked using automata-theoretic con-
structions, and is decidable. The transition systems considered here are more general and
possibly infinite. As a consequence, the relations interpreting atomic formulae are not always
computable. In this situation, the truth is no more decidable. We thus fall into a common
pattern of various areas of automated reasoning where deduction rules do not yield an algorithm
but rather a not always terminating procedure. We follow a similar approach to the
problem which is a first step towards formal verification of what we call Thue specifications,
and may be summarized as follows.
The semantics of a Thue specification is given as a labeled transition system, the states
of which form a language over an alphabet \Sigma and labels belong to an alphabet \Gamma . Each label
a in \Gamma corresponds to an event and may be seen as a partial function of states. As in a Hoare
triple [']P [/], where the domain (resp. range) of a partial function associated to a program
P is restricted by a precondition ' (resp. postcondition /), an event a 2 \Gamma may be restricted
to a partial function of X into U where X and U are some subsets of the set of states. This is
written a: X!U . Besides letters of \Gamma , the words over \Gamma may be seen as functions. This leads
to the word-functions of the form u: X!U with . The problem studied in this paper is
that of finding the maximal subset X 0 of X on which u is defined and u(X 0 ) ' U . However, the
only solutions of interest are those which may readily be subject to boolean operations, and
for which the emptiness problem is decidable. Within the Chomsky hierarchy, only rational
languages satisfy both requirements. This leads us to the problem of determining the domain
of u: X!U whenever it is rational, assuming that both X and U are also rational. To solve
this problem, we introduce a procedure that is given under the form of three deduction rules
that derive a system of left-linear (language) equations from a word-function u: X!U . If the
procedure terminates, the least solution of the resulting system of equations is the domain of
u: X!U .
In deduction rule-based automated reasoning, the most fundamental issues are soundness
and completeness [20]. While the soundness is an obvious requirement, the completeness may
be out of range in certain situations. Unlike the set of consequences of a finite set of axioms,
the theory of a single structure need not be recursively enumerable. For instance, since the
theory of the standard model of arithmetics is undecidable (see e.g. [17]), by the Turing-Post
Theorem, one concludes that this theory is not recursively enumerable. A complete method
for deciding the truth of a sentence cannot exist in such a case. This limitation is well known
in the area of inductive theorem proving (see e.g. survey [4]). Similarly, the problem studied in
this paper is not recursively enumerable, hence the procedure introduced here is not complete.
In particular, this procedure is not guaranteed to terminate on each instance of the problem
which has a rational solution. Nevertheless, we establish a reasonably weak sufficient condition
for the termination and we show that this condition is satisfied by several well-known classes
of semi-Thue systems for which the termination is expected.
This paper is organized as follows. In Section 1 basic notions from language theory and
string rewriting are recalled. Section 2 introduces Thue specifications and their models. The
main problem of the paper is stated in Section 3 and is related to the Hennessy-Milner logic.
In Section 4 a procedure that treats the problem is provided. Section 5 is devoted to the
correctness of the procedure. In Section 6 it is established that the problem is not recursively
enumerable and a reasonably weak condition for the termination of the procedure is provided.
The techniques developed in the paper are applied in Section 7 to an example of a simple
railway network. Brief conclusions close the paper.
Assuming a minimum theoretical background from rational languages [16] and string-rewrit-
ing [6], we recall in this section several notions from these topics and we set some notations
for the sequel. The family of rational languages over an alphabet \Sigma is written Rat(\Sigma   ) and
the family of the rational subsets of a language L ' \Sigma   is written Rat(L), viz
stands for the powerset of L.
Systems of equations We speak of rational expressions in the usual meaning but we also use
rational expressions with variables. The variables are written are assumed
to form an infinite, countable set. Subsequently we shall often replace variables in a given
rational expression R (resp. set of equations E ) with some rational expressions (possibly with
variables) by applying a substitution oe on it. The resulting expression (resp. set of equations)
is written Roe (resp. E oe). A substitution of a single variable X by an expression P may be
its application on an expression R (resp. set of equations E) is written
A (left) linear system of equations is a set of equations such that, for each variable, there
is at most one equation with this variable as lefthand side and each equation is of the form
where Y is a rational expression, each Y i is a rational expression denoting a nonempty set and
are variables. In such an equation, Y is called constant term. We say that the system
of equations E is balanced if, for each variable X of the system, there is exactly one equation
in E with X as lefthand side.
Among all solutions of E with respect to (w.r.t.) a given variable X we distinguish the least
solution (w.r.t. inclusion). It is well known that a balanced linear system of n equations has
a least solution w.r.t. any of its variables. One may compute such a solution using the Gauss
elimination method. For more details related to this topic the reader may consult e.g. [3].
Suffixes, prefixes, quotients and remainders Given u; v; w 2 \Sigma   such that
is called a prefix and v a suffix of w. When v 6= " (resp. u 6= "), u (resp. v) is a proper prefix
suffix ) of w. We note pref (u) (resp. suff (u)) the set of all the prefixes (resp. suffixes) of
a word u. These notations are extended to sets; for instance, given X ' \Sigma   , pref (X) stands
for
u2X pref (u). When words x and z, then y is called a factor of w.
When xz 6= ", y is an proper factor of w.
The (right) quotient (resp. remainder) of M ' \Sigma   by N ' \Sigma   , written M=N (resp. MiN)
is defined as follows
resp.
As we will see further, some steps of the procedure that is the main point of this paper
depend on computations of quotients and remainders between rational languages. Remember
that the family of rational languages is closed under both quotients and remainders [10].
String rewriting A semi-Thue system (sts) S over an alphabet \Sigma is a binary relation on
di of S, also noted g ! d, is called a rule; l(r) (resp. r(r)) stands for the
lefthand (resp. righthand) side of r, i.e. g (resp. d). The latter notation is extended on sets of
pairs (e.g. binary relations, functions etc.), especially l(S) denotes the domain and r(S) the
range of S. An sts S is
1. special if
2. monadic if r(S) ' A [ f"g and jlj ? jrj for each (l; r) 2 S,
3. left-basic if for all x;
2 r(S) and for all x;
reduction relation "!
" on \Sigma   is associated with S in the usual way, namely !
and the rewrite relation is the reflexive-transitive closure
of !
. A word is said to be reducible (resp. irreducible) w.r.t. S when it belongs
(resp. does not belong) to l(!
). The set of all irreducible words over \Sigma is written \Sigma   # S . It
is well known that this set is rational whenever l(S) is so, since \Sigma   #
. A
normal form of a word u w.r.t. S is an irreducible word v 2 \Sigma   # S such that u
v. 1 We say
that an sts S is confluent when for all words u; v; v 0 2 \Sigma   such that u
v and u
v 0 there
exists a word w 2 \Sigma   such that v
w and v 0
w. If in addition S is terminating, i.e. there
is no infinite chain
is said to be convergent. It is well known that the
convergence of an sts implies the existence and uniqueness of a normal form of each word.
Throughout this paper we only use convergent semi-Thue systems and the unique normal
form of a word u is written u# S . In notation !
usually omit the subscript S when
there is no ambiguity about the sts we refer to. Notice that in our notations "#" has a lower
precedence than the concatenation, thus uv# stands for the normal form of uv.
Notations for rewriting of rational expressions are similar to those for individual words.
For sake of simplicity, we identify every rational expression with the language it denotes. We
may write etc. for any rational expressions X, Y that, in
this context, stand for associated languages. Consequently, we speak of the normal form of a
rational expression X and we write X# in the reference to the set fu# j u 2 Xg or possibly
to some rational expression denoting this set.
It may be useful to restrict applications of some rules of an sts so that they can only
rewrite prefixes of words. For this aim we may assume that each alphabet \Sigma may be extended
by an additional symbol
the words to rewrite belong to $ \Sigma \Sigma   and S is a subset
of ($ \Sigma \Sigma   \Theta $ \Sigma \Sigma   ) [ (\Sigma   \Theta \Sigma   ). Note that if all rules are in $ \Sigma \Sigma   \Theta $ \Sigma \Sigma   then we are in
the particular case of regular canonical systems [5] (see also [8,15]). The latter are known as
generating rational languages.
Convergent Thue Specifications, Their Languages and Models
A convergent Thue specification cts is a triple h\Sigma is a state alphabet, \Gamma is
an event alphabet, R is a rational subset of $ \Sigma \Sigma   (of \Sigma   if S " ($ \Sigma \Sigma   \Theta $ \Sigma \Sigma   is a
finite convergent sts over \Sigma [ \Gamma and each word of R is irreducible, i.e. R 2 Rat($ \Sigma \Sigma   # S ). The
distinction between state alphabet and event alphabet has no importance from a theoretical
point of view but is useful in practical situations. When simply write h\Sigma ; R; Si.
Recall that the convergence is not decidable in general. On the contrary, for a rational
language R, it is decidable whether R '
The aim of Thue specifications is to provide means for specifying and verifying processes.
This article does not discuss that approach as a specification technique but focuses merely on
We use a restricted version of the notion of normal form that is common in term-rewriting. In the general
version a normal form of a word u w.r.t. S is an irreducible word v 2 \Sigma   #S such that u
v.
a basic problem related to formal verification. In this field, semi-Thue systems are needed.
Moreover since we are intersted in deterministic processes, we only consider confluent semi-
Thue systems.
The semantics of Thue specifications is given in terms of (a variant of) labeled transition
systems. Formally, a labeled transition system (lts) A is a tuple hQA
is a set of states, \Gamma is an alphabet, ' A 2 QA is an initial state and ffi A ' QA \Theta \Gamma \Theta QA a
transition relation. When (q; a; q 0 we say that q (resp. q 0 ) is the origin (resp. target)
of a transition labeled by a. A transition system is deterministic if for all q; q
each a implies that q
Each state q 2 QA recognizes some set of words of that is defined
as the smallest set satisfying the following conditions:
if
The language of A written L is the set of all words recognized by the states of A ,
namely
q2QA
Definition 2.1. An lts is an initial model of cts
and the following holds for all u; v 2 R and each a
if and only if ua#
The language L (cts) of a cts is the language of its initial model: L
cag
c
c
c
c
c
\Deltaff
A
A
A AU
a
c
c
c
c
c
c
c
c
\Deltaff
A
A
A AU
a
c
c
\Gamma\Psi
@
@
@ @R
a b
c
\Gamma\Psi
@
@
@ @R
a b
c c
c
c
c
c
c
\Deltaff
A
A
A AU
\Deltaff
A
A
A AU
ae
ae
ae
ae
ae
ae
ae
ae=
Z
Z Z
Z Z
Z Z Z~
c
a
a
c
c
a b
Fig. 2.1. A convergent Thue specification and its initial model
We emphasize that according to the above definition, the states QA of an initial model are
identified with the words of R. It may be observed that the confluence of S is sufficient for an
initial model to be deterministic. Consequently, convergent Thue specifications only model
deterministic processes. Henceforth, for an initial model A of a cts, ffi A is considered as a
function and we write ffi A (u; a) = v instead of (u; a; v) 2 ffi A . Another important observation
is that the language of a cts is always closed under nonempty prefixes because the language
of a transition system is so.
We speak of an initial model because one may define a satisfaction relation
labeled transition systems and Thue specifications. One may then consider the initial object
in the category of the models of a cts. This initial object may be alternatively characterized
as the initial model in the sense of the above definition. Obviously, the initial model is unique
up to isomorphism and always exists. An example of a convergent Thue specification together
with its initial model is provided in Fig. 2.1. In this example, the identification of the set of
states with R 1 is left as an exercise to the reader.
For sake of simplicity, from now on and up to Sect. 7, we neglect the distinction between
state alphabet and event alphabet. Both alphabets are assumed equal and a cts is written
3 Domains of Word-functions and the Hennessy-Milner Logic
In the previous section we explained the relationship between convergent Thue specifications
and deterministic labeled transition systems. To any convergent Thue specification h\Sigma ; R; Si,
one may associate its initial model, say A . Concerning A , a very basic problem consists
in characterizing all transitions with some given label a 2 \Sigma . More precisely, one may wish
to characterize all states that are origins of the transitions labeled by a. For this aim, it is
convenient to view a as a function of states 2 into states by writing a: R!R. The problem
consists therefore in characterizing the domain of this function. Instead of taking all the states
into account, one may consider a restriction of the above function written a: X!U , for some
subsets X and U of R. In the sequel, we restrict our attention to the case where both X and
U are rational subsets of R.
The main problem of this paper may be stated as follows.
Problem. Given a convergent Thue specification cts = h\Sigma ; R; Si and a function a: X!U
relative to cts and such that both X and U are rational subsets of R (X; U 2 Rat(R)), find
Dom(a: X!U), viz the domain of the function, whenever it is rational.
To better understand this problem, one has to reason about the initial model of the cts, say
A . According to Definition 2.1, given a state w 2 X we have
Hence, w 2 Dom(a: X!U) when wa# 2 U . Thus, Dom(a: X!U) is the largest subset X 0 of
X such that X 0 a# ' U .
In order to solve this problem, it is useful to consider a bit more general question that
appears as a technical trick without specific semantics related to transition systems. With
this aim in view, given u 2 \Sigma   and rational subsets X;U of R, the word-function u: X!U
is defined as follows: u(w) = wu# S , for every w 2 X such that wu# S 2 U . Thus
Remember that R is the set of states of A .
It is important to note that, without additional assumptions, a word-function needs not
coincide with the corresponding composition of functions associated to labels. When
unless 1g. Since the set
of the states of a cts may be strictly included in \Sigma # S , the assumption about \Sigma # S being the
source or a the target of a function is not compatible with the semantics in terms of transition
systems. For this reason, the word-function are considered here as a technical tool without a
special meaning.
To close this section we point out that the problem we are interested in is related to the
model checking, at least in the context of the Hennessy-Milner logic.
Checking Properties Expressed in the Hennessy-Milner Logic
The formulae of the Hennessy-Milner logic [9] are built up from the constants 1 and 0,
using the classical connectives (:; ; "; etc.) and a unary operator hai for each letter a of the
alphabet \Sigma .
Given a labeled transition system A , the meaning of a formula ', viz the set of the states
of A that satisfy ', written ['] A , is inductively defined as follows:
where the map [hai] A :  (QA defined by
When a transition system A is an initial model of cts Si, we can express
the meaning of a formula in terms of the domains of the word-functions. For this aim, the
equalities (3.1) remain valid (we have precisely [:']
becomes [hai] A
It is obvious from the above that the procedure developed in the sequel may be used
for checking the properties, expressed in the Hennessy-Milner logic, of systems defined by
convergent Thue specifications. This is relevant only for those cts's for which the procedure
succeeds in computing Dom(a: R!X) for all a 2 \Sigma and all X 2 Rat(R) (see Subsection 6.2).
Since in the case of success, Dom(a: R!X) is rational, the boolean operations stipulated in
may readily be performed.
4 Introducing the procedure
A method for solving the problem stated in the latter section is now developed. An instance of
the problem consists of a cts h\Sigma ; R; Si and a word-function u: X!U relative to it, such that
both X and U are rational subset of R. More precisely, X, U and R are rational expressions
that, for sake of simplicity, are identified with corresponding languages. In addition, u is
required to be irreducible. This is not restrictive since u: X!U and u#: X!U are equal due
to the convergence of S. Hence, if u is reducible, u has to be normalized before applying the
method.
The solution of an instance of the problem is the subset X 0 of X such that X
however that the method developed in the sequel can find X 0 only
rational. The method consists of a procedure that, provided it terminates, computes
a system of equations, the least solution of which is also the solution of the instance of the
problem. Basic ideas underlying the procedure are developed in subsequent paragraphs.
The task of finding Dom(u: X!U) becomes easier if X is partitioned into two parts Y and
Z such that Y u is irreducible and each word of Zu is reducible. Further, Z will be partitioned
into smaller parts, say Z . Indeed, such a "divide and conquer" strategy is based
on the following obvious equality
In order to characterize Z, assume that a rule g ! d of S is applicable to a word vu of Zu.
Since both v and u are irreducible, g overlaps v and u, i.e.
are such that x 6= " and w 6= ". This remark leads to the following
definition.
Definition 4.1. A word x 2 \Sigma   is a left-overlap completion of u 2 \Sigma   with respect to a
semi-Thue system S if there exist w; w 0 2 \Sigma   such that xw 2 l(S), ww
left-overlap completion x of u is minimal if no left-overlap completion of u is a proper suffix
of x.
In view of the above definition, each word of Z has a suffix that belongs to the set, say L, of
all left-overlap completions of u. The set Y , being the complement of Z in X, is characterized
by X i L. In fact, one may characterize Y as X i L min where L min stands for the set of all
minimal left-overlap completions of u.
In order to partition Z into smaller parts, observe that, given a left-overlap completion x
of u, the expression (X=x)xu denotes all the words of Xu that may be reduced by any rule
d such that w is a prefix of u. Consequently, one may wish to partition Z according to L
as
x2L (X=x)x. However, this is not a partition because (X=x 1 )x 1 ' (X=x 2 )x 2 whenever
x 2 is a suffix of x 1 . In fact, it is L min that provides a partition of Z, namely
(X=x)x.
In sum, the following partition of X is used as basis for the divide and conquer strategy:
Now, according to (4.1), the domain corresponding to each part has to be found. With respect
to the first member of the above partition, it is not too difficult to see that
Indeed,
since both U and (XiL min )u are irreducible.
With respect to the other members of the partition described in (4.2), Dom(u: (X=x)x!U)
has to be found for each x 2 L min . Observe first that
Consider, now, various ways of computing (X=x)xu#. Obviously, one may compute xu# first
and subsequently compute the normal form of (X=x)(xu#). Thus
The maximal suffix of xu# that is not rewritten during any computation of the normal form
of (X=x)(xu#) now has to be isolated. The concept introduced in the following definition is
useful for this purpose.
Definition 4.2. A word v of \Sigma   # is a right irreducible divisor with respect to an sts S when
f"g. The set of all right irreducible divisors w.r.t. S is written rid S .
For any u 2 \Sigma   such that rid S we say that w is a right irreducible divisor of
w.r.t. S. The set of all right irreducible divisors of u is written rid S (u). A right irreducible
divisor of u is maximal if it is not a proper suffix of a right irreducible divisor of u.
It is easy to see that rid . Consequently, since S is finite and
\Sigma   # is rational, rid S is rational too. It is also obvious that any right-irreducible divisor of
xu# is not rewritten during any computation of (X=x)(xu#:
Proposition. Any u 2 rid S and w 2 \Sigma   satisfy
Let us go back to the problem of finding Dom(xu#: X=x!U) assuming that
z is the maximal right irreducible divisor of xu#. Suppose also that
for some V ' \Sigma   . Then V in view of the above proposition, and V y#z ' U .
Therefore V y# ' U=z. Consequently In sum, taking (4.4) and (4.5)
into account, we have
The next definition is not indispensable since it just combines Definitions 4.1 and 4.2.
Nevertheless, it will allow a more concise presentation of the procedure.
Definition 4.3. A left-overlap triple of a word u w.r.t. S is a triple of words hx;
that x is a minimal left-overlap completion of u, z is the maximal right
irreducible divisor of xu#. The set of all left-overlap triples of u is written lot S (u), or simply
lot (u) if S is determined by context. A triple  2 lot (u) is written hx  ; y  ; z  i.
The procedure is given in the form of three deduction rules (see Table 4.1) Generate,
and Eliminate, the first of which corresponds, roughly speaking, to the transformations
discussed up to now in this section. Using (4.1), (4.2), (4.6) and Definition 4.3, those
transformations may be summarized in the next lemma.
Henceforth we speak of an irreducible expression to mean that the expression denotes a
language, each word of which is irreducible.
Lemma 4.4. Let X be an irreducible rational expression and u an irreducible word. Then
U=z 6=?
The proof of the lemma follows from the above discussion together with the following remarks.
When X=x ?. For this reason, such
cases are excluded from (4.7). Also the first member of the above sum is written in a way
that slightly differs from the righthand side of (4.3), since Xi
and
x  is the set of all minimal left-overlap completions of u (formerly L min ).

Table

4.1. Deduction rules
U=z 6=?
U=z 6=?
Xx
is a set of fresh variables.
We shall now describe the procedure by explaining its principles and related notations.
As mentioned at the beginning of this section, in case of success the result of the procedure
consists of a system of equations. Consequently, we still need some variables. In a derivation
of such a system, an expression of the form X 7! u: X!U with a variable X represents a
goal. This means that Dom(u: X!U) should be the least solution w.r.t. X of the resulting
system of equations. Each rule of a procedure is applied downwards with reference to some
goal. The goal is then said to have been treated ; otherwise it is called fresh. We denote by F
(resp. T) the set of fresh (resp. treated) goals. Thus, the procedure manipulates sequents of
the form T; F ' E where E is a system of equations.
Let us now explain the r"ole of each rule of the procedure. First note that any rule is
applied with reference to some fresh goal. Given a fresh goal X 7! u: X!U the rule Generate
introduces a new equation based on (4.7)
U=z 6=?
and for each  2 lot(u) such that X=x  6= ? and U=z  6= ?, it introduces a fresh goal
Each X  is some new variable. The goal X 7! u: X!U is then no
longer fresh and is transferred to T. Notice that if, in the new equation, we replace X by
Dom(u:X!U) and each X  by Dom(y  : X=x  !U=z  ) we thereby obtain (4.7). The fresh
introduced in this step correspond to the principle that the
procedure should be applied recurrently in order to compute Dom(y  : X=x  !U=z  ).
The following idea underlies the rule Identify. If there is a treated goal X 7! u: X!U (the
one for which an equation has been generated) and a fresh goal X 0 7!
then we should not generate an equation from X 0 7!
Instead, we identify the two variables X and X 0 . This is realized by renaming X 0 as X in the
current system of equations. The goal X 0 7! simply removed from F.
The rule Eliminate deals with goals X 7! u: X!U such that Xu is irreducible. A fresh
goal satisfying this condition is moved to T and the equation is added to the
systems of equations.
The applicability of the above rules is restricted by certain conditions (written beside each
rule). We insist on the fact that within such applicability conditions Xu be understood as a
language and r(T) as a set of word-functions (the range of the assignment T). The notation
F q fX 7! u: X!Ug is used for a partition of some F 0 such that X!Ug.
Derivations and related concepts are defined as follows.
A derivation is a (possibly finite) sequence of sequents s 0
for each there exists an instance of a rule, the upper and lower sequent of
which are equal resp. to s i and s i+1 , and s i satisfies the applicability conditions of the rule.
A derivation is called maximal when it is not a proper prefix of another derivation. We say
that a sequent is pure when it is of the form ?; fX 0 7! stands
for an irreducible rational expression and u 0 an irreducible word such that rid f"g. We
say that a derivation is pure when it starts with a pure sequent.
In order to compute Dom(u: X!U) the procedure starts with the pure sequent
f"g. This is not restrictive since, if
with z the maximal right irreducible divisor of u such that z 6= ", then
X!U=z). The procedure may therefore start with the pure sequent
?; fX 7! y: X!U=zig ' ?. Next, the rules are applied as long as possible in order to
produce a maximal derivation. The procedure stops if no rule may be applied. This happens
only if there are no more fresh goals. It then remains to compute the solution of the resulting
system of equations w.r.t. X . Let us illustrate this by applying the procedure to an example.
Example 4.5. Let cts 1 be the convergent Thue specification of Fig. 2.1. We want to compute
We check that both $(a are irreducible, and rid(c) =
f"g. Taking into account that lot (c) = fha; "; bi; hb; c; aig, we have the following derivation.
Generate
ae
oe
Eliminate
ae
oe
ae
oe
ae
oe
ae
oe
Finally, the least solution of the resulting system w.r.t. X 0 yields
as expected, according to the minimal model of cts 1 depicted on Fig. 2.1.
5 Correctness of the procedure
Usually, a sequent-like calculus is correct, when it transforms valid sequents into valid ones.
We shall use the following notation in order to explain what we mean by "valid sequent".
Notation 5.1. For any set G of goals, we denote by oe G the following substitution:
The following definition is used as basis for the correctness.
Definition 5.2. A sequent T; F ' E is valid, when Dom(v: Y !V ) is the least solution of
goal Y 7! v: Y !V 2 T.
We need to establish the following theorem.
Theorem 5.3. Any sequent of a pure derivation is valid.
Proof. Let be a derivation satisfying the assumptions of the theorem and let us
prove the statement for any sequent s k by induction on k 2 N.
Basis: The statement holds trivially for any pure sequent ?; fX 0 7!
Induction step. We have to distinguish 3 cases depending on the rule that yields s k from s k\Gamma1 .
By induction hypothesis s k\Gamma1 is valid and the validity of s k is obtained using the correctness
of each rule, established in Appendix B. 2
The main result on the correctness is given as corollary of Theorem 5.3.
Corollary (Correctness). Let D be a maximal finite derivation starting with a pure sequent
be the last sequent of D. Then Dom(u: X!U) is
the least solution of
Proof. It is obvious that on the first sequent either Generate or Eliminate may by applied.
Consequently, X 7! u: X!U 2 T. On the other hand, F is empty, since no rule can be
applied on the last sequent of D. Hence oe F is empty and by Theorem 5.3 we conclude that
Dom(u:X!U) is the least solution of
6 Incompleteness and termination
Since the procedure computes only linear systems of equations, the rationality of the domain
of a word-function is obviously a necessary condition for the termination. To see that this
condition is not sufficient, consider the following specification: cts
apply the procedure to the
goal . The procedure generates the following infinite sequence of goals:
This example points out that the procedure is not complete. In fact, as established in the
next subsection, a complete procedure for this problem cannot exist. (By the completeness of
the procedure we mean that it terminates on each instance of the problem that has a rational
solution.) In Subsection 6.2, we therefore investigate a sufficient condition for the termination
of our procedure.
6.1 Incompleteness of the problem
The question as to whether a complete procedure exists for this problem is equivalent to the
recursive enumerability of the following set of tuples:
More precisely, we consider an even simpler version of the problem where the cts's are of
the form h\Sigma ; \Sigma   #; Si and the word functions are of the form a: X!U . We establish that the
following set of pairs is not recursively enumerable:
is a convergent sts over \Sigma , a 2 \Sigma ;
To this end we reduce the complement of the halting problem for Turing machines. More
precisely, given a deterministic Turing machine T and an input word w, we construct a
convergent sts and a word-function the domain of which is rational if and only if T does
not halt on w.
be a deterministic single-tape Turing machine, where
is the tape alphabet, denotes the blank symbol, Q is the set of states,
is the transition function and q 0 is the initial (resp.final) state. Observe that without
loss of generality it is implicitly assumed here that T cannot print . Consequently, each tape
cell that has been visited by the tape head contains a letter of \Pi and the language accepted
by T is


stands for the reflexive-transitive closure of the single-step computation relation ' T
Without loss of generality we assume in addition that
is the set of "overlined"
versions of letters of \Pi and 0; 0; are additional symbols. Let
be an input word.
Given the pair hT ; wi we construct the pair hS T ; %:Xw!U# i where
and the sts ST over \Sigma is the union of several groups of rules below. When two rules differ
only by 0 or 1, we abbreviate them by meta-variables  ;  0 that rank over f0; 1g. The use of
"overlined" ("underlined") symbols is not relevant for the understanding of the construction
but aims at avoiding overlaps between lefthand sides of ST . At first reading the reader may
assume that the "underlined" and "overlined" versions of any symbol and the symbol itself
are identical.
ffl Initialization rules
ffl Rules that simulate the behaviour of T
ffl Rules that apply immediately after each simulation step that does not result in a configuration
where the tape head reads
ffl Rules that apply before each simulation step (after (ii) or (vii))
ffl Terminal rules
Before justifying this construction, we state several lemmas. The first one is obvious.
Lemma 6.1. Both Xw and U# are irreducible w.r.t. ST . 2
Lemma 6.2. ST is confluent.
Proof. To establish the confluence, it is enough to check that the lefthand sides of ST do not
Lemma 6.3. ST is terminating.
Proof. Consider the following precedence  on \Sigma : 0  1  a  # for all a 2 \Pi. Let  1 be
the following (rewrite) ordering on (\Sigma r f%g)   :
where  lex is the lexicographic extension of  and e
x (resp. e y) stands for the reversal of x
(resp. y). Using  1 we define a (reduction) ordering  2 on (\Sigma r f%g)   :
It is easy to check that, except for (i), each rule of l ! r 2 ST satisfies l  2 r. Let then  3
be a (reduction) ordering on \Sigma   defined, for all x; y 2 \Sigma , as follows: x  3 y if
stands for the number of occurrences of % in x
- or
where  lex
stands for the lexicographic extension of  2 .
Now, for Rule (i) we have %  3 % because (0; ")  lex
1). For each
other rule l ! r 2 ST we have also l  3 r because  2 and  3 coincide on (\Sigma r f%g)   . 2
Justification of the construction Given a word q 0 wx 2 Xw , where
some a below the
rewrite steps of the word q 0 wx%. (The sequences of rules used in these steps are denoted by
rational expressions.)
Starting from q 0 wx% we get
and by
we end up with u n q n v n #, provided that
Thus if T does not halt on w, Dom(%:Xw!U#
Assume then that T halts after k steps in a configuration u k q f v k and let v
for some b the length 2n of x may be arbitrarily big, without loss of
generality we assume that n ? k l. Starting with q 0 wx% analogous rewrite steps lead to
We then get
(viii)   (ix)(vii)
Now, using the rules (x) and (xi) we end up with u k q f # if and only if the word
belongs to the Dyck language D   . Consequently Dom(%:Xw!U#
which is not a rational language.
We have therefore established the following lemma.
Lemma 6.4. Dom(%:Xw!U# ) is rational if and only if T does not halt on w. 2
Putting together Lemmas 6.1, 6.2, 6.3 and 6.4 and taking into account the fact that the
complement of the halting problem for Turing machines is not recursively enumerable, we
conclude that the following holds.
Theorem 6.5. DomRat is not recursively enumerable. 2
In view of this theorem a complete procedure for the problem of computing rational domains
of (even single-letter) word-functions cannot exist. Otherwise, there would exist a Turing
machine that "semi-decides" the complement of the halting problem.
6.2 Termination
In this subsection we state a sufficient condition for the termination of the procedure. The
following definition provides a tool for studying the termination.
Definition 6.6. The generation set for a goal X 7! Dom(u:X!U) w.r.t. an sts S, written
is the smallest subset of \Sigma   \Theta  (\Sigma   )\Theta  (\Sigma   ) that contains hu; X;Ui and such that
whenever hv; Y; W for each  2 lot (v).
In the sequel, we shall consider the first, the second and the third projections of a generation
set G S (u; X;U
We now state two lemmas that lead to the final result of this section. The first one is
obvious.
Lemma 6.7. A maximal derivation starting with a pure sequent ?; fX 7! Dom(u: X!U)g '
? is finite if and only if its generation set is finite. 2
Lemma 6.8. The second (resp. third) projection of a generation set G S (u; X;U) is finite if
Proof. Note first that for a given rational set Z the cardinality of the set
is not greater than 2 k , where k is the number of states of a minimal automaton accepting Z.
In order to prove the statement, it is therefore enough to show that  2 (G S (u; X;U)) ' QX
According to Definition 6.6,
there exist hy
and
lot (y n ). Thus
Hence
The following theorem states a simple sufficient condition that guarantees that the procedure
will terminate.
Theorem 6.9. Starting with a pure sequent ?; fX 7! Dom(u: X!U)g ' ?, the procedure
terminates if
1. S is finite,
2. for each minimal left-overlap completion x of u, the word xu belongs to l(S) and
3. for each rule r 2 S, any  2 lot (r(r)) and any minimal left-overlap completion x 0 of y  ,
the word x 0 y  belongs to l(S).
Proof. According to Lemmas 6.7 and 6.8, the termination of the procedure is guaranteed
if  1 (G S (u; X;U)) is finite. An easy induction allows us to establish that for any y in
there exists a rule r 2 S such that xy and x is a minimal left-overlap
completion of y. Hence  1 (G S (u; X;U)) is included in a finite set, namely suff (l(S)). 2
Note that, when used for computing the domain of a word-function that is a single letter
(as e.g. in the case of the Hennessy-Milner logic), the procedure terminates provided that
both the first and the third requirements of Theorem 6.9 hold. We may use this observation
for more general word functions. In order to compute Dom(u: X!U) w.r.t. h\Sigma ; R; Si such
that when the second requirement of Theorem 6.9 is not satisfied
whereas the first and the third ones are, we may proceed as follows.
1. compute the ranges of the word functions:
(For the computation of ranges we may use the procedure of [11]. This procedure always
terminates for a single-letter word-function under conditions 1 and 3 of Theorem 6.9.)
2. Set D apply the procedure for computing the domains
of the word functions:
Now
It is important to point out three classes of semi-Thue systems that may successfully
be used for letterwise computation of domains of word-functions, namely left-basic sts's,
monadic sts's and special sts's.
Corollary 6.10. Starting with a pure sequent ?; fX 7! Dom(a: X!U)g ' ?, where a 2 \Sigma ,
the procedure terminates if S is finite and is special, monadic or left-basic.
Proof. When S is monadic, for any r 2 S, the set lot (r(r)) may contain only elements of the
form hx; a; "i or hx; "; ai such that xr(r) ! a is a rule of S. When S is left-basic or special,
obviously for each rule r of S. Hence the sts's that are special, monadic or
left-basic satisfy the third requirement of Theorem 6.9. 2
In view of the corollary, we may assert that the requirements of Theorem 6.9 provide a
reasonably weak sufficient condition for termination. These requirements capture the well
known classes of sts's, the models of which are related to the transition graphs of pushdown
machines [7,12]. For the latter, the rationality of the domains of word-functions follows from
the well known fact that the pushdown store languages are rational. In addition, the scope
of Theorem 6.9 is not restricted to special, monadic or left-basic sts's. Indeed, S 1 (see Fig.
2.1) is neither special, monadic nor left-basic whereas it satisfies the requirements 1 and 3 of
the theorem.
Last but not least, the example of non termination that opened this section has one
interesting property, namely that cts 0
equivalent to cts 2 , in the sense that cts 2 and cts 0
2 have the same initial model. But this
time the procedure terminates for cts 0
2 on any single-letter word-function.
7 Final Example
We consider a simple portion of a railway network. A single track connects two stations: left
(L) and right (R), via a central one (C), as on the following picture.
L C R
All trains run from L to R or vice-versa with a stop at C, never being allowed to go in reverse.
The central station has two platforms allowing train crossing. Each of these platforms may
be occupied by at most one train.
To model this situation we may consider the following state alphabet \Sigma
a train has left L but has not yet arrived at C
a train has left R but has not yet arrived at C
\Gamma! D C
a train has left C for R but has not yet arrived at R
a train has left C for L but has not yet arrived at L
\Gamma! A C
a train coming from L is waiting on a platform of C
a train coming from R is waiting on a platform of C
The underlined versions of \Gamma! D C
have no special meaning for the modeling but aim
at making easier the definition of transitions by a semi-Thue system. For this reason, we
distinguish the set \Sigma 0 of non-underlined letters of \Sigma : \Sigma
g. We also
consider the following event alphabet \Gamma .
d L
departure of a train from L
departure of a train from R
\Gamma! d C
departure of a train from C for R
departure of a train from C for L
\Gamma! a C
arrival of a train from L at C
a C
arrival of a train from R at C
a L
arrival of a train at L
a R
arrival of a train at R
In our modeling a train "appears" when it leaves one of the stations L or R and "disap-
pears" when it arrives at the opposite terminal station. Since the trains do not go in reverse,
there is a deadlock, when two trains run towards one another on the same portion of the track
(between L and C or between C and R).
In order to define the set R of states, we need the following expressions, each of which
describes, in fact, a subset of R satisfying some characteristic property (stated between paren-
(D L
trains run from L to R),
trains run from R to L),
(D L
trains run towards C),
trains run from C to L or R).
Using the above expressions, the states of the system may be defined as follows:
A word of R reflects the events in the reverse chronological order (a state alphabet letter
corresponding to the most recent event is written immediately after $). For instance
means that there is one train, say t running from R to C, a second one, say t running from
L to C and a third one, say t 3 , on a platform of C coming from R. The events corresponding
to this configuration occurred in the following has left R before t 2 left L and t 2 has
left L before t 3 arrived at C.
The transitions are defined by the following sts S over f$g [
xd L
\Gamma! d L
x for x 2 \Sigma 0 xd R
\Gamma! d R
x for x 2 \Sigma 0
$d L
\Gamma! $D L
\Gamma! $D R
x \Gamma! a C
\Gamma! \Gamma! a C
x /\Gamma a C
\Gamma! /\Gamma a C
x for x 2 \Sigma 0 r f \Gamma! D C
\Gamma! a C
\Gamma! \Gamma! A C
a C
\Gamma!
x \Gamma! A C
\Gamma! \Gamma! A C
\Gamma!
x for x 2 \Sigma 0
\Gamma!
\Gamma!
x \Gamma! d C
\Gamma! \Gamma! d C
x for x 2 \Sigma 0 r f \Gamma! A C
x /\Gamma d C
\Gamma!
\Gamma! A C
\Gamma! d C
\Gamma! \Gamma! D C
\Gamma!
x \Gamma! D C
\Gamma! \Gamma! D C
\Gamma!
x for x 2 \Sigma 0
\Gamma!
\Gamma!
\Gamma! a R
x for x 2 \Sigma 0 r f \Gamma! D C
xa L
\Gamma! a L
\Gamma! D C
a R
a L
When a meta-variable x is replaced by all possible values, we get a system of 62 rules with
non-overlapping lefthand sides. This system is therefore finite and confluent. The termination
of S may readily be established using standard reduction orderings, thus concluding that S
is convergent.
As an example of questions concerning this system, we may ask about the set of states in
which a departure of a train may occur at one of the terminal stations, for instance at R. The
set of such states, say P
, is precisely given by P d R
computing P d R
it is useful to ask about the termination of the procedure in this case. Since the first condition
of Theorem 6.9 is obviously satisfied by S, we check whether the second one is satified by d R
Indeed, for each letter a and any left-overlap completion x of a, we have xa 2 l(S). Hence, it
remains to verify the third condition of Theorem 6.9. To this end, we compute the set T of
left-overlap triples of r(S):
For the rules with a meta-variable, we obtain, for all x 2 \Sigma 0
lot(d L
xig
lot (d R
xig
lot ( \Gamma! a C
lot ( /\Gamma a C
lot( \Gamma! A C
xig
lot(
xig
lot ( \Gamma! d C
lot ( /\Gamma d C
lot ( \Gamma! D C
xig
lot ( /\Gamma D C
xig
lot(a R
lot (a L
For the rules with $, we have
lot($D L
\Gamma! A C
and for other rules
lot(
\Gamma! A C
ig
lot(
ig
lot ( \Gamma! D C
ig
lot ( /\Gamma D C
ig
We observe that  2 (T ) f"g. Thus, for each u 2  2 (T ) and each left-overlap
completion x of u we have xu 2 l(S), meaning that the third condition of Theorem 6.9 is
satisfied. We may therefore conclude that the procedure terminates here for any single-letter
word-function.
Starting with a pure sequent ?; fX 7! Dom(d R
the procedure terminates
and yields a system of equations, the least solution of which is, as expected,
ing
Thus, the departures of trains from R are possible only when there are no trains running from
C to R.
Concluding remarks
Semi-Thue systems and transition systems have been related here by sketching a formal
specification technique named as Thue specifications. A particular emphasis has been placed
on the simplest case of the problem of formal verification. A pragmatic point of view has been
necessary in approaching the latter problem, since this has been shown not to be recursively
enumerable, and has led to an incomplete procedure. This problem is perhaps the most
elementary problem that one has to deal with in order to be able to approach the model
checking of Thue specifications, at least in the special case of the Hennessy-Milner logic.
However, since the expressive power of the latter is not very rich, our work needs be extended
so as to handle more powerful logics. With this aim in view, our attention is currently focussed
on the monadic second-order logic [13].
One of the most important problems concerns the specification of systems that consist of
interacting processes. This problem may be handled via the concept of synchronized product
[2]. Within that approach, each component of the system is specified separately and interactions
between components are defined as synchronization constraints. For instance, the
railway example may be specified in an even more elegant way as four interacting processes:
the two portions of the track and the two platforms of the central station. When a system
is specified componentwise by several Thue specifications provided with a synchronization
constraint, these specifications may be combined into a single one, (on wich the procedure
may be applied for checking the properties of the system). Based on the ideas of [14], an
algorithm that performs the latter task, has been developed [18].

Acknowledgments

We wish to thank Marie-Catherine Daniel-Vatonne and '
Etienne Payet
for careful reading of a preliminary version of this paper, and Adrian Bondy for correcting
the English text. We are grateful to Damian Niwi'nski for numerous suggestions that helped
improving the presentation of this paper. Special thanks are due to Serge Burckel whose
comments have influenced Subsection 6.1 and to an anonymous referee that has suggested
the current proof of Lemma 6.3 which is more elegant than our original proof.



--R

Finite Transition Systems.
Comportements de processus.

Theorem proving in hierarchical clausal specifications.
Regular canonical systems.

A string-rewriting characterization of context-free graphs
On the regular structure of prefix rewriting.
Algebraic laws for nondeterminism and concurrency.
Introduction to Automata Theory
A procedure for computing normal forms of certain rational sets.

The graphs of finite monadic semi-Thue systems have a decidable monadic second-order theory
Thue specifications and their monadic second-order properties

A method for enumerating cosets of a group presented by a canonical system.
Finite automata.
Mathematical Logic.
An automata-theoretic approach to automatic program verification
Automated reasoning: Introduction and Applications.
first occur all self-Identify steps
and other Identify steps occur after.
--TR

extracted:['file assignment' 'feedback controls' 'feature weights'
 'fault-tolerant software systems' 'fault-tolerant routing algorithm'
 'fault-tolerant routing' 'fault-tolerant algorithms' 'flow analysis'
 'zero storage biometric authentication' 'computational complexity']
marked:['formal verification', 'infinite-state systems', 'automated deduction', 'string-rewriting']
--T
SAT-Based Decision Procedures for Classical Modal Logics.
--A
We present a set of SAT-based decision procedures for various classical modal logics. By SAT based, we mean built on top of a SAT solver. We show how the SAT-based approach allows for a modular implementation for these logics. For some of the logics we deal with, we are not aware of any other implementation. For the others, we define a testing methodology that generalizes the 3CNFi>K methodology by Giunchiglia and Sebastiani. The experimental evaluation shows that our decision procedures perform better than or as well as other state-of-the-art decision procedures.
--B
Introduction
Propositional reasoning is a fundamental problem in many areas of
Computer Science. Many researchers have put, and still put, years of
eort in the design and implementation of new and more powerful SAT
solvers. Most importantly, the source code of many of these implementations
is freely available and can be used as a basis for the development
of decision procedures for more expressive logics (see, e.g., (Giunchiglia
and Sebastiani, 1996a; Cadoli et al., 1998)).
In this paper, we present a set of SAT-based decision procedures for
various classical modal logics (Montague, 1968; Segerberg, 1971). By
SAT-based, we mean built on top of a SAT solver. We show how the
SAT-based approach allows for a modular implementation for these
logics. For some of the logics we deal with, we are not aware of any
other implementation. For the others, we dene a testing methodology
which generalizes the 3CNFK methodology by Giunchiglia and
(1996a). The experimental evaluation shows that our deci-
1999 Kluwer Academic Publishers. Printed in the Netherlands.
A. Tacchella
sion procedures perform better than or as well as other state-of-the-art
decision procedures.
Our approach, rst suggested by Giunchiglia and Sebastiani (1998),
consists of two steps:
1. take o the shelf one of the fastest SAT procedures available, and
2. use it as the basis for your modal decider.
Our approach diers from most of the previous works in decision
procedures for modal logics, which are either tableau-based, (i.e., based
on Smullyan's tableau for propositional logic (Smullyan, 1968): see,
e.g., (Fitting, 1983; Massacci, 1994; Baader et al., 1994)), or translation-based
(i.e., based on a reduction to rst order logic: see, e.g., (van
Benthem, 1984; Ohlbach, 1988; Hustadt and Schmidt, 1997a)). Some
of the dierences and advantages of the SAT-based approach with respect
to the tableau based and translation based approaches have been
already pointed out in (Giunchiglia and Sebastiani, 1996b; Giunchiglia
et al., 1998). Here we show how the SAT-based approach provides for a
simple and natural schema for the development of decision procedures
for modal logics.
Our approach diers also from the approaches by Horrocks (1998)
and Patel-Schneider (1998). These authors developed their own SAT
checker and used it as a basis for their modal decider. This approach
allows for a better integration among the dierent modules of the system
and for a ner tuning of the reasoning strategies. On the other
hand, it does not exploit the great amount of ongoing work on SAT
deciders. Each year, new and faster SAT engines are proposed. With
some standard modications of the source code we can inherit in the
modal setting all the benets of state-of-the-art propositional checkers. 1
Finally, our system, called *sat, improves also on previous works
by Sebastiani and ourselves (1996a; 1996b; 1998). In particular,
*sat is built on top of the SAT decider sato (Zhang, 1997). *sat
inherits many of the sato congurable options, and allows for
some more options (including early pruning, caching, two forms of
backjumping, and the choice of various splitting heuristics); and
*sat is able to deal with 8 modal logics, namely E, EM (=M),
EN, EMN, EC, EMC (=R), ECN, EMCN (=K) (see (Chellas,
1980)). For EN, EMN, EC and ECN, we do not know of any
similar point is made by Kautz and Selman (1998) as an explanation for the
better performances of SATPLAN with respect to specialized engines for planning
problems.
SAT-Based Decision Procedures for Classical Modal Logics 3
other implemented decision procedure, nor of any reduction to a
formalism for which a decision procedure is available.
The paper is structured as follows. In Section 2 we review some
basic denitions about classical modal logics. Then, in Section 3, we
present the idea of the SAT-based approach to modal reasoning, and
discuss in detail the algorithms for the 8 logics we consider. Section 4
is devoted to *sat: it discusses some of the motivations underlying
its construction, and some of its features. The experimental analysis
comparing *sat with other state-of-the-art decision procedures is done
in Section 5. Our analysis is restricted to K (for which there are several
systems available) and E (for which there exists a reduction to bimodal
K). We end in Section 6 with the conclusions and the future work.
2. Classical modal logics
Following (Chellas, 1980), a modal logic is a set of formulas (called
theorems) closed under tautological consequence. Most modal logics
are closed under the rule
for certain values of n. If then the logic is said to be monotone.
then the logic is said to be regular. If n  0 then the logic is
said to be normal. The smallest monotone, regular and normal modal
logics are called M, R and K respectively (see, e.g., (Chellas, 1980)).
Classical modal logics (Montague, 1968; Segerberg, 1971) are weaker
than normal modal logics. In fact, the only requirement is that the set
of theorems is closed under the rule
As a consequence, the schemas
2 The symbols > and ? are 0-ary connectives representing truth and falsity
respectively.
4 E. Giunchiglia, F. Giunchiglia and A. Tacchella
which are theorems in K do not necessarily hold in classical modal
logics. The three principles N, M, and C enforce closure conditions on
the set of provable formulas which are not always desirable, especially if
the 2 operator has an epistemic (such as knowledge or belief) reading.
If we interpret 2' as \a certain agent a believes '", then N enforces
that a believes all the logical truths, M that a's beliefs are closed under
logical consequence, and C that a's beliefs are closed under conjunction.
These three closure properties are dierent forms of omniscience, and
|as such| they are not appropriate for modeling the beliefs of a real
agent (see, e.g., (Giunchiglia and Giunchiglia, 1997) and (Fagin et al.,
Chapter 9). We can easily imagine situations involving a's beliefs,
where only an arbitrary subset of the above properties holds.
There are eight possible ways to add the three schemas M, C, and
N to the smallest classical modal logic E. The resulting modal logics
are called E, EM (equivalent to the logic M), EN, EMN, EC, EMC
(equivalent to R), ECN, EMCN (equivalent to K), where EX denotes
the logic obtained adding the schemas in X to E.
3. SAT-based procedures for classical modal logics
We say that a conjunction  of propositional literals and formulas of the
form 2' or :2' is an assignment if, for any pair ; 0 of conjuncts in
, it is not the case An assignment  satises a formula
' if  entails ' by propositional reasoning. A formula ' is consistent
in a logic L (or L-consistent) if :' is not a theorem of L, i.e., if :' 62
L.
Consider a formula '. Let S be a set of assignments satisfying ', and
let L be a modal logic. As noticed by Sebastiani and Giunchiglia (1997),
the following two facts hold:
If at least one assignment in S is L-consistent then ' is L-consistent.
If no assignment in S is L-consistent then ' is not L-consistent as
long as the set S is complete for ', i.e., as long as the disjunction
of the assignments in S is propositionally equivalent to '.
From these facts, it follows that the problem of determining whether '
is L-consistent can be decomposed in two steps:
generate an assignment  satisfying ', and
test whether  is L-consistent.
In all the logics we consider, testing the consistency of an assignment
amounts to determining the consistency of other formulas whose
SAT-Based Decision Procedures for Classical Modal Logics 5
depth (i.e., the maximum number of nested 2 operators) is strictly minor
than the depth of . This implies that we can check the consistency
of these other formulas by recursively applying the above methodology,
at the same time ensuring the termination of the overall process. The
above methodology can be implemented by two mutually recursive
procedures:
Lsat(') for the generation of assignments satisfying ', and
Lconsist() for testing the L-consistency of each generated assignment
.
In order to simplify the presentation, we rst present Lconsist(),
and then Lsat(').
3.1. Lconsist()
Whether an assignment is consistent, depends on the particular logic
being considered. Furthermore, depending on the logic L considered,
the consistency problem for L (i.e., determining whether a formula is
consistent in L) belongs to dierent complexity classes. In particular,
the consistency problem for E, EM, EN, EMN is NP-complete, while
for EC, ECN, EMC, EMCN it is PSPACE-complete (see (Vardi, 1989;
Fagin et al., 1995)). Here, to save space, we divide these eight logics in
two groups. We present the algorithms for checking the L-consistency
of an assignment rst in the case in which L is one of E, EM, EN, EMN,
and then in the case in which L is one of the others.
3.1.1. Logics E, EM, EN, EMN
The following Proposition is an easy consequence of the results presented
in (Vardi, 1989).
PROPOSITION 1. Let
be an assignment
in which
is a propositional formula. Let L be one of the logics E, EM,
EN, EMN.  is consistent in L if for each conjunct :2 j in  one of
the following conditions is satised:
L-consistent for each conjunct 2 i in , and L=E;
L-consistent for each conjunct 2 i in , and L=EM;
are L-consistent for each conjunct 2 i in
, and L=EN;
are L-consistent for each conjunct 2 i in ,
and L=EMN.
6 E. Giunchiglia, F. Giunchiglia and A. Tacchella
function Lconsist(
foreach conjunct 2 j do
foreach conjunct 2 i do
if M [i;
if M [j;
if M [j; False then return False
if M [j;
if M [j; False then return False
return True.

Figure

1. Lconsist for E, EM, EN, EMN
When implementing the above conditions, care must be taken in order
to avoid repetitions of consistency checks. In fact, while an exponential
number of assignments satisfying the input formula can be generated by
Lsat, at most n 2 checks are possible in L, where n is the number of \2"
in the input formula. Given this upper bound, for each new consistency
check, we can cache the result for a future possible re-utilization in a
This ensures that at most n 2 consistency checks will
be performed. In more detail, given an enumeration of
the boxed subformulas of the input formula, M[i,j], with i 6= j, stores
the result of the consistency check for (' i ^:' j ). M[i,i] stores the result
of the consistency check for :' i . Initially, each element of the matrix
M has value Undef (meaning that the corresponding test has not been
done yet). The result is the procedure Lconsist in Figure 1.
Consider Figure 1 and assume that L=E or L=EN. Given a pair of
conjuncts 2 i and :2 j , we split the consistency test for (
in two simpler sub-tests:
rst, we test whether
only if this test gives False, we test whether (:
SAT-Based Decision Procedures for Classical Modal Logics 7
Notice also that, in case L=EN or L=EMN, if we know that, e.g.,
consistent and we store this
result in M[j,j].
PROPOSITION 2. Let
be an assignment
in which
is a propositional formula. Let L be one of the logics E, EM,
EN, EMN. Assume that, for any formula ' whose depth is less than
the depth of , Lsat(')
returns True if ' is L-consistent, and
False otherwise.
returns True if  is L-consistent, and False otherwise.
Proof: The Proposition is an easy consequence of the hypotheses and
Proposition 1.
3.1.2. Logics EC, ECN, EMC, EMCN
The following Proposition is an easy consequence of the results presented
in (Vardi, 1989).
PROPOSITION 3. Let
be an assignment
in which
is a propositional formula. Let  be the set of formulas  i
such that 2 i is a conjunct of . Let L be one of logics EC, ECN,
EMC, EMCN.  is consistent in L if for each conjunct :2 j in  one
of the following conditions is satised:
L-consistent for each non empty subset
0 of , and L=EC;
L-consistent for each subset  0 of , and
is empty or ((
Assume that L=EC or L=ECN. The straightforward implementation
of the corresponding condition may lead to an exponential number of
checks in the cardinality jj of . More carefully, for each conjunct
in , we can perform at most jj
1. for each formula  i in , we rst check whether (:
consistent in L. Let  0 be the set of formulas for which the above
test fails. Then,
8 E. Giunchiglia, F. Giunchiglia and A. Tacchella
function Lconsist(
is a conjunct of g;
foreach conjunct 2 j do
foreach conjunct 2 i do
if M [j;
if M [j;
if not Lsat(
return True.

Figure

2. Lconsist for EC, ECN, EMC (R), EMCN (K)
2. in case L=ECN or  0 6= ;, we perform the last test, checking
whether ((
consistent in L.
Furthermore, the result of the consistency checks performed in the rst
step can be cached in a matrix M analogous to the one used in the
previous subsection.
If L=EC or L=ECN, the procedure Lconsist in Figure 2 implements
the above ideas. Otherwise, it is a straightforward implementation
of the conditions in Proposition 3.
PROPOSITION 4. Let
be an assignment
in which
is a propositional formula. Let L be one of logics EC, ECN,
EMC, EMCN. Assume that, for any formula ' whose depth is less than
the depth of , Lsat(')
returns True if ' is L-consistent, and
False otherwise.
returns True if  is L-consistent, and False otherwise.
Proof: The Proposition is an easy consequence of the hypotheses and
Proposition 3.
SAT-Based Decision Procedures for Classical Modal Logics 9
function Lsat(')
return Lsatdp (cnf('), >).
function Lsatdp (', )
if f a unit clause (l) occurs in ' g
then return Lsat dp (assign(l;
l := choose-literal('; );
return Lsat dp (assign(l; '), ^ l) or
base */
/* backtrack */
/* unit */
/* split */

Figure

3. Lsat and Lsat dp
3.2. Lsat(')
Consider a formula '. Let L be a modal logic. The generation of assignments
satisfying ' is independent of the particular logic L being
considered. Furthermore, it can be based on any procedure for SAT:
if the SAT decider is complete, then we can generate a nite and
complete set of assignments for ' as follows:
at step 0, ask for an assignment satisfying ', and
at step i for an assignment satisfying ' and the
negation of the assignments generated in the previous steps.
By checking the L-consistency of each assignment, we obtain a
correct and complete decider for L.
if the SAT decider is correct but incomplete, then we cannot generate
a complete set of assignments for ', but we can still build
a correct but incomplete decider for L by checking each generated
assignment. Of course, whether an incomplete eective procedure
for SAT can be turned into an eective incomplete procedure for
a modal logic L, is still an open point.
The above method for generating a complete set of assignments
for ' has the advantage that the SAT decider is used as a blackbox.
A. Tacchella
The obvious disadvantage is that the size of the input formula checked
by the SAT solver may become exponentially bigger than the original
one. A better solution is to invoke the test for L-consistency inside the
procedure whenever an assignment satisfying the input formula
is found. In the case of the Davis-Putnam (DP) procedure (Davis and
Putnam, 1960), we get the procedure Lsat represented in Figure 3. In
the gure,
cnf(') is a set of clauses |possibly with newly introduced propositional
variables| such that, for any assignment  in the extended
language, the following two properties are satised: 3
1. if  satises cnf(') then the restriction of  to the language of
2. if  satises ' then there exists an assignment in the language
of cnf(') which (i) extends  and (ii) satises cnf(').
Examples of such conversions are the \classical conversion" (which
given a formula in negative normal form recursively distribute
conjunctions over disjunctions), and the conversions based on \re-
naming", such as those described in (Tseitin, 1970; Plaisted and
Greenbaum, 1986; de la Tour, 1990).
choose-literal('; ) returns a literal occurring in ' and chosen
according to some heuristic criterion.
if l is a literal, l stands for A if l = :A, and for A if l = :A;
for any literal l and formula ', assign(l; ') is the formula obtained
from ' by (i) deleting the clauses in which l occurs as a disjunct,
and (ii) eliminating l from the others.
As can be observed, the procedure Lsat dp in Figure 3 is the DP-
procedure modulo the call to Lconsist() when it nds an assignment
satisfying the input formula. Notice that in this procedure the pure
literal rule, used by some DP implementations, is not implemented.
In fact, our main interest is in correct and complete modal deciders.
With the pure literal rule, the set of assignments checked by Lconsist
|assuming that each of such call returns False| is not ensured to be
complete. 4
3 Let  be an assignment in a language L. Let L 0  L be a language. The
restriction of  to L 0 is the assignment obtained from  by deleting the conjuncts
not in L 0 . Let  0 an assignment.  0 extends  if each conjunct of  is also a conjunct
of  0 .
4 According to some authors (see, e.g., Freeman (1995)) the pure literal rule only
helps for a few classes of SAT problems. Furthermore, when this rule does help, it
does not result in a substantial reduction in search tree size.
SAT-Based Decision Procedures for Classical Modal Logics 11
In the following, for any formula ' and for any assignment  in
the (possibly extended) language of cnf('),  ' is the restriction of
to the language of '. Analogously, for any set of assignments ,
g.
PROPOSITION 5. Let ' be any modal formula. Assume that, for
any assignment , Lconsist() prints  and returns False. Let be
the set of assignments printed as the result of invoking Lsat('). The
disjunction of the assignments in ' is propositionally equivalent to '.
Proof: In the hypotheses of the Proposition, what we need to prove is
that
_
First, it is clear that cnf(')
2 . For the thesis, the right-to-left
implication is an easy consequence of the rst of the two properties of
cnf('). Assume that the left-to-right implication is false. This means
that there exists an assignment  in the language of ' such that
(1) for any propositional atom A, either A or :A is a conjunct of ,
(2) for any formula 2 in the language of ', either 2 or :2 is a
conjunct of ,
(3)  satises ', and
does not satisfy
.
Given (3) and the second of the properties of cnf('), there must exist
an assignment  0 in the language of cnf(') which
extends ,
thus also
.
means that  0 entails
2  by propositional reasoning. Thus,
there must exist an assignment  00 in such that |given (1), (2), (5)|
each conjunct of  00
' is also a conjunct of  0
' and thus of . This implies
that  entails  00
' . Since  00
, contradicting (4).
We can now state and prove soundness and completeness results for
our procedures.
Theorem [Soundness and Completeness] Let L be one of the logics
E, EM, EN, EMN, EC, ECN, EMC, EMCN. Lsat is sound and
12 E. Giunchiglia, F. Giunchiglia and A. Tacchella
complete for L, i.e., for any modal formula ', Lsat(') returns True if
' is L-consistent, and False otherwise.
Proof: First observe that Lsat(') returns
True if there exists a call to Lconsist() which returns True, and
False if each call to Lconsist() returns False.
The proof is by induction on the depth d of '. If 0 the Theorem
is trivial. Assume that 1. By induction hypothesis, for any
formula whose depth is less than d, Lsat( ) returns True if is L-
consistent, and False otherwise. In the following, we use the following
two facts:
(1) For any two assignments  and  0 diering only for propositional
conjuncts,  is L-consistent i  0 is L-consistent.
This is a trivial consequence of Proposition 1 and Proposition 3.
(2) Let  be an assignment such that Lconsist() is invoked during
the execution of Lsat('). Lconsist() returns True if  is L-
consistent and False otherwise.
Given that the depth of  is less than or equal to d, this is a
consequence of the the induction hypothesis, Proposition 2 and
Proposition 4.
There are two cases:
1. ' is L-consistent. From Proposition 5, it follows that there exists a
call to Lconsist() in which the assignment  (i) satises cnf('),
and (ii) is such that  ' is L-consistent. Since  possibly extends  '
in that it may assign some newly introduced propositional variables,
from (1) it follows that  is L-consistent, and from (2) it follows
that Lconsist() |and thus also Lsat(')| returns True.
2. ' is not L-consistent. From Proposition 5, it follows that for each
call Lconsist(), the assignment  ' is not L-consistent. As above,
from (1) it follows that  is not L-consistent, and from (2) it follows
that Lconsist() returns False.
4. *sat
*sat is built on top of sato ver. 3.2 (Zhang, 1997). The choice of
adopting sato as the basis for our system has been driven by the
following motivations:
SAT-Based Decision Procedures for Classical Modal Logics 13
sato is fast. We have not performed and are not aware of any
up-to-date extensive comparison among the dierent SAT solvers
publicly available. 5 However, according to some experiments we
have done and to the results presented in (Zhang, 1997), sato
seems to behave better than most of the currently available SAT
solvers.
sato has many options, including various splitting heuristics and
backjumping. We have inherited some of these options, and they
are available for experimentation.
sato has been written using some Software Engineering conventions
which have made and will make much easier to tune it for
our goals.
Besides the options inherited from sato, our system allows for other
possibilities that we have developed while implementing the system.
It is out of the goals of this paper to describe *sat structure, optimizations
and congurable options. For a more detailed presentation,
see (Tacchella, 1999) and the manual distributed with *sat. For our
goals, it suces to say that the core of *sat is a C implementation of
the procedures Lsat and Lconsist in Figures 1, 2, 3. In particular,
with reference to Figure 3, in *sat
cnf(') is the set of clauses obtained from ' by applying a conversion
based on renaming, such as those described in (Tseitin,
1970; Plaisted and Greenbaum, 1986).
choose-literal('; ) returns a literal according to a MOMS heuristic
(Maximum Occurrences in clauses of Minimum Size) (Freeman,
1995).
assign(l; ') is a highly optimized procedure which takes time linear
in the number of occurrences of l in '.
*sat also implements two important optimizations which have been
used in the tests presented in the next Section:
Early-pruning. Before each splitting step in Lsat, the L-consistency of
the assignment generated so far is checked by a call to Lconsist.
As in KsatC, care is taken to avoid the repetition of L-consistency
checks on the same branch of the propositional search tree. 6 Early
for a list of publicly available SAT solvers and more.
6 This is obtained through a pointer in the assignment stack which keeps track
of the portion of the current assignment which has been already veried to be
L-consistent.
14 E. Giunchiglia, F. Giunchiglia and A. Tacchella
pruning has proved to be very eective at least on some of the
tests presented in the next Section (see (Giunchiglia et al., 1998)).
Caching for K. In the case of the logic K, *sat uses an additional data
structure which allows to associate to any formula ' the result of
Lsat('). Before invoking Lsat on a formula , Lconsist checks
whether the K-consistency of has already been determined. As in
caching introduces some additional costs, but it may produce
dramatic speedups (see (Horrocks and Patel-Schneider, 1999b)).
We have not yet conducted an exhaustive experimental analysis to
see, for each class of formulas, which combination of *sat options leads
to the best results (see (Horrocks and Patel-Schneider, 1999b) for a
similar study of Dlp options). In all the tests in the next Section, we
used *sat options which seemed more reasonable to us. In particular,
we have set *sat as to use
early pruning on all tests, and
caching (for K) when the depth of the input formula is greater
than 1.
5. A comparative analysis
The availability of decision procedures for the logics we consider varies
signicantly. For EMCN, that we recall is equivalent to K, there are
many implemented decision procedures available, see, e.g.,
et al., 1998; de Swart, 1998). For E, EM and EMC, Gasquet and
Herzig (1996) provide a reduction to normal modal logics: by implementing
this reduction we indirectly obtain decision procedures for
these logics. Fitting (1983) calls U the logic EM, and denes a tableau
system for it. More recently, Governatori and Luppi (1999) dene a
tableau-like proof system for classical, monotonic and regular modal
logics. We are not aware of any implementation of these tableau sys-
tems. For EN, EC, EMN and ECN we are not aware of any other
implemented decision procedure, nor of any reduction into a formalism
for which a decision procedure is available.
Our comparative analysis is restricted to K and E. In fact, both our
decision procedures for E and EM, and Gasquet and Herzig's reductions
for E and EM, are similar. We expect that the experimental analysis
for EM would lead to results similar to the ones we have for E. For
SAT-Based Decision Procedures for Classical Modal Logics 15
EMC, Gasquet and Herzig's reduction is to a normal modal logic for
which we do not have a system available. 7
5.1. Modal K
As we said, there are several systems able to solve the consistency
problem for K. In our comparative experimental analysis, we consider
the four systems *sat, KsatC (Giunchiglia et al., 1998), Dlp (Patel-
Schneider, 1998) and
among the fastest solvers for K. We remember that TA, given a modal
rst determines a corresponding rst order formula '  and
then it performs conventional rst-order theorem proving. In our tests,
as in (Hustadt and Schmidt, 1997a), uses Flotter to convert '
in a set of clauses Cl('  ), and then the theorem prover Spass to solve
Cl('  ). For a brief description of Flotter and Spass, see (Weidenbach
et al., 1996). 8
We test these systems on three problem sets of randomly generated
formulas. A 3CNFK formula is a conjunction of 3CNFK
clauses, each with three disjuncts. Each disjunct in a 3CNFK clause
is either a propositional literal or a formula having the form 2C or
:2C, where C is a 3CNFK clause. See (Giunchiglia and Sebastiani,
1996a) for a more detailed presentation. We only remark that for any
formula ' there exist a 3CNFK formula which is K-consistent i ' is
K-consistent.
Sets of 3CNFK formulas can be randomly generated according to
the following parameters:
(i) the modal depth d;
(ii) the number L of clauses at depth
7 The reduction maps the consistency problem for a formula ' in EMC, into the
consistency problem for a formula ' 0 in the smallest normal modal logic with two
modal operators 21 , 22 and augmented with the schema
See (Gasquet and Herzig, 1996) for more details.
8 The experimental results have been obtained with Dlp ver. 3.1,
ver. 1.4 (Spass/Flotter ver. 0.55), KsatC ver. 1.0, and *sat ver. 1.2.
To compile the systems we have used sml-nj 110.0.3, sicstus prolog
3, ACL 5.0, and gcc 2.7.2.3. The tests have been run on several Intel
PCs, whose conguration varies from P200MHz with 64MbRAM, up to a
PII350MHz with 256 MbRAM. All platforms are running Linux 5.x RedHat. Dlp
is available at http://www-db.research.bell-labs.com/user/pfps.
at http://www.doc.mmu.ac.uk/STAFF/U.Hustadt/mdp. KsatC is available at
ftp://ftp.mrg.dist.unige.it/ in pub/mrg-systems. *sat is available at the WEB
page http://www.mrg.dist.unige.it/~tac/StarSAT.html.
A. Tacchella
(iii) the number N of propositional variables;
(iv) the probability p with which a disjunct occurring in a clause at
depth < d is purely propositional.
Care is taken in order to avoid multiple occurrences of a formula in a
clause, at the same time ensuring that the modal vs. the propositional
structure of each generated formula only depends on p. In more detail,
a clause is generated by randomly generating its disjuncts. When generating
a disjunct, we rst decide whether it has to be a propositional
literal or not. Then a disjunct of the proper type is repeatedly generated
as long as it does not occur in the clause generated so far.
In the tests we consider, a problem set is characterized by N and
p: d is xed to 1, while L is varied in such a way to empirically cover
the \100% satisable { 100% unsatisable" transition. For each pair
in a problem set, 100 3CNFK formulas are randomly generated,
and the resulting formulas are given in input to the procedure under
test. Then, for each run, we consider the time the systems take for
the main processing of the formula, thus excluding the negligible time
the systems take to read and somehow normalize the input formula.
(In particular, for this means that we take into account only the
time needed by Spass to solve the formula generated at the end of
the translation process.) For practical reasons, a timeout mechanism
stops the execution of the system on a formula after 1000 seconds of
CPU time. Even more, for any pair N; p, the execution of *sat, KsatC
and stopped after the system exceeds the timeout on 51 of the
100 samples corresponding to the xed N; p. Dlp instead, stops its
execution on the 100 samples corresponding to a pair N; p if a supermajority
of the rst n tests timeout. 9 When this happens, it is assumed
that Dlp exceeds the timeout for more than 50% of the tests with that
The rst three problem sets we consider have
xed to 0%: according to Hustadt and Schmidt (1997b), xing
corresponds to particularly dicult tests. We call these problem sets
PKN4p0, PKN5p0, and PKN6p0 respectively. PKN4p0 and PKN6p0
are called PS12 and PS13 respectively in (Hustadt and Schmidt, 1997b).
In order to better highlight the behavior of *sat and KsatC, we also
run these systems on a problem set (called PKN7p0) having
In Figure 4, satisability percentages and the median of
the CPU times for the four systems are plotted against the number
9 In more detail, Dlp stops its execution if there are at least 5 tests so far and
more than 90% of them timeout, or if there are at least 10 tests so far and more
than 75% of them timeout, or if there are at least 20 tests so far and more than 55%
of them timeout (Patel-Schneider, 1999).
SAT-Based Decision Procedures for Classical Modal Logics 17
Systems comparison - N=4, d=1, %p=0
CPU
KsatC
Systems comparison - N=5, d=1, %p=0
CPU
KsatC
Systems comparison - N=6, d=1, %p=0
CPU
KsatC
Systems comparison - N=7, d=1, %p=0
CPU
KsatC

Figure

4. Logic K. *sat, KsatC, Dlp, and 7.
samples/point. Background: satisability percentage.
of clauses L. Notice the logarithmic scale on the vertical axis, which
causes that values equal to 0:00 do not get plotted.
Consider Figure 4. The rst observation is that *sat and KsatC
are the fastest. The two systems perform roughly in the same way, with
*sat performing better when 5. For 7, the two systems
have similar performances, one system performing better than the other
for some values of L, but worse for other values of L. This comes at no
surprise: the two systems have the same underlying structure, both use
early pruning and a MOMS heuristic to select the splitting literal. The
two systems do not have an identical behavior because they use dierent
data-structures and implement slightly dierent MOMS strategies.
Considering the other systems, for PKN4p0 the gap between *sat/KsatC
and Dlp [resp. TA] is of more than one order of magnitude at the
cross-over point of 50% of satisable formulas, and goes up to almost
[resp. 4] orders of magnitude at the right end side of the horizontal
axis. For PKN5p0 and PKN6p0, values exceed the timeout
for respectively, while the corresponding values of
A. Tacchella
*sat [resp. KsatC] are 1.22 [resp. 1.83] and 3.32 [resp. 5.64] seconds.
keeps exceeding the timeout for all the successive values. The gap
between *sat/KsatC and Dlp on PKN5p0 is of more than one order
of magnitude at the cross-over points of 50% satisable formula, and
goes up to almost 3 orders at the very right of the plot. When
Dlp median values exceed the timeout for
does not terminate gracefully. 10 Comparing *sat and
Dlp on PKN4p0 and PKN5p0, we see that
the gap between *sat and Dlp seems to increase with L. Both for
PKN4p0 and PKN5p0, the dierence in logarithmic scale between
*sat and Dlp is (almost always) monotonically increasing.
At the crossover point of 50% satisable formulas, the gap between
*sat and Dlp [resp. TA] is roughly 1s [resp. 60s] for PKN4p0; and
220s [resp. 1000s] for PKN5p0.
Such good performances by *sat and KsatC are due to early pruning,
which has revealed to be very eective on these problem sets. For
example, if we disable early pruning in *sat and rerun it on PKN5p0,
the system keeps exceeding the time limit for
When the better behavior of *sat and KsatC than Dlp
and TA, is conrmed by the Q%-percentile graphs in Figure 5, corresponding
to 4. Formally, the Q%-percentile of a set S of values is
the value V such that Q% of the values in S are smaller or equal to
. The median value of a set thus corresponds to the 50% percentile
of the set. Figure 5 reports the 50%, 60%, 70%, 80%, 90% and 100%
percentile values of the CPU-times when *sat (top left), KsatC (top
right), Dlp (bottom left) and (bottom right) are run on PKN4p0.
The percentile plots for PKN5p0 and PKN6p0 look similar to the plots
in

Figure

5. This means that all the systems perform in roughly the
same way on the 50 most dicult samples of the 100 tests corresponding
to a xed N and L.
We also run the four systems on problems with
p is xed to 50%. We call these problem sets PKN4p50, PKN5p50,
PKN6p50, and PKN7p50. In Figure 6 the satisability percentages and
the median of the CPU times for the four systems are plotted against
the number of clauses L. As for
roughly in the same way and are the fastest. Dierently from the tests
in which 0%, at the transition point of 50% of satisable formulas,
the gap between *sat/KsatC and Dlp seems to diminish when the
number of variables increases. Horrocks and Patel-Schneider (1999a)
ver. 3.2 is able to successfully handle formulas with
SAT-Based Decision Procedures for Classical Modal Logics 19206010050709010 -2
percentiles
CPU
KsatC CPU TIME - N=4, d=1, %p=0
percentiles
CPU
percentiles
CPU
percentiles
CPU

Figure

5. Logic K. 50%{100% percentile CPU times of *sat, KsatC, Dlp, and TA.
samples/point.
show that for values of L bigger than 7, Dlp performances are superior
to those of KsatC when
that Dlp performs better than KsatC when p is high and worse
when p is low. We have not yet done such a broad comparison using
*sat instead of KsatC. However, we believe that Horrocks and Patel-
Schneider's conclusions should extend also to *sat, if *sat is used with
the parameter settings we have currently used, i.e., those which make
*sat most similar to KsatC. Of course, a big role can be played by
*sat already available congurable options, and this will be the issue
of future research. In any case, both *sat and KsatC perform better
than the other systems for large values of L, when the formulas are
trivially unsatisable. This is due to the fact that for large values of
L, formulas become propositionally unsatisable, and thus both *sat
and KsatC mostly take advantage of their SAT-based nature, e.g., of
their optimized data structures for handling large formulas.
As for the percentile plots of the timings of the systems
on PKN7p50, do not show big dierences with respect to the plots of
20 E. Giunchiglia, F. Giunchiglia and A. Tacchella
Systems comparison - N=4, d=1, %p=50
CPU
KsatC
Systems comparison - N=5, d=1, %p=50
CPU
KsatC
Systems comparison - N=6, d=1, %p=50
CPU
KsatC
Systems comparison - N=7, d=1, %p=50
CPU
KsatC

Figure

6. Logic K. *sat, KsatC, Dlp, and 7.
samples/point. Background: satisability percentage.
the medians. Considering Dlp and *sat 100% percentile plots, it is
interesting to observe that Dlp has a lower maximum than *sat; on
the other hand, *sat values decrease more rapidly than those of Dlp.
Finally, we see that for most of the problem sets we consider, all the
systems seem to have an easy-hard-easy pattern, whose peak roughly
correspond to the 50% of satisable formulas. When
phenomenon is best evident for *sat and KsatC.
We also consider the benchmarks formulas for K used at the Comparison
of Theorem Provers for Modal Logics at Tableaux'98 (see (de
Swart, 1998)). These consist of nine provable parameterized formulas
(ending with \ p") and nine unprovable parameterized formulas
(ending with \ n"). For each parameterized formula A(n), the test
consists in determining the greatest natural number n  21 satisfying
the following two conditions:
1. the prover returns the correct result for the formulas A(1);
in less than 100 seconds, and
SAT-Based Decision Procedures for Classical Modal Logics 21
percentiles
CPU
KsatC CPU TIME - N=7, d=1, %p=50
percentiles
CPU
percentiles
CPU
percentiles
CPU

Figure

7. Logic K. 50%{100% percentile CPU times of *sat, KsatC, Dlp, and TA.
samples/point.
2. the prover cannot do the formula A(n+ 1) in less than 100 seconds
or
Even though it has been proved that most of these tests can be easily
solved by current solvers, these are still interesting because
they are not 3CNFK formulas, and
some of these tests have not been solved yet.
The results for *sat, and Dlp are reported in Table I. KsatC has
not been tested since KsatC is able to deal with 3CNFK formulas only.
We also show the CPU time requested by the system to solve the last
instance A(n). Notice that *sat has been run with caching enabled,
since the depth of all the formulas in the Table is greater than 1. For
TA, we do not take into account the time needed to compute the rst
corresponding to A(n) (which is negligible), but we
do take into account the time requested by Flotter to convert A  (n)
22 E. Giunchiglia, F. Giunchiglia and A. Tacchella

Table

I. Logic K. *sat, Dlp and performances on Tableaux'98 benchmarks
*sat Dlp
Test Size Time Size Time Size Spass Flotter
branch p 21 0.21
lin n 21 47.80 21 0.70 21 16.07 63.94
lin
path n 21 0.96
path p 21 0.72
ph n 12 0.60 9 40.16 9 45.21 9.92
ph p 8 48.54 6 11.34 6 42.19 0.97
poly n 21 2.25
poly p 21 1.73
into a set Cl(A  (n)) of clauses (reported in the Flotter column), and
the time requested by Spass to determine the consistency or inconsistency
of (reported in the Spass column). Furthermore, we
stopped on A(n) with n < 21, either because Flotter does not
terminate gracefully when computing
or Flotter exceed the 100 seconds time limit. In the table, these three
cases correspond to the rows in which the value for n/Spass/Flotter
respectively is underlined.
As can be observed from Table I, the three systems are able to solve
all the instances of a formula in four cases. *sat and Dlp are able
to solve all the instances except for k branch n, k branch p, k ph n,
ph p. Except for the rst of these four parameterized formulas, *sat
is able to solve more instances than Dlp. For k branch n, both *sat
and Dlp are able to solve the 12th instance, with Dlp taking less time
than *sat to solve it.
SAT-Based Decision Procedures for Classical Modal Logics 23
5.2. Modal E
Gasquet and Herzig (1996) provide a translation which maps any formula
' into a formula 'GH in K 2 , i.e., the smallest normal modal logic
with two modal . The translation is such that ' is
satisable in E i 'GH is satisable in K 2 . This translation is dened
in the following way:
is a propositional variable,
and homomorphic for the cases of the propositional connectives.
Consider a formula '. We compare *sat performances on ' with
respect to *sat, Dlp and performances on 'GH . We could not
run KsatC on 'GH , since KsatC accepts only 3CNFK formulas with
at most one modality. To make evident when a system is run using
Gasquet and Herzig's translation, we append the string \+GH" to
the name of the system. Therefore, in the following, we will have the
systems *sat, *sat+GH, Dlp+GH, and TA+GH.
In E, the 3CNFK test methodology is not suited. Indeed, it is no
longer the case that for any modal formula ' there exists a 3CNFK
formula which is E-satisable i ' is E-satisable. Furthermore, checking
the consistency of an assignment  in E amounts to determine the
consistency of (  :) for each pair of conjuncts 2 and :2 in :
most of these tests, in case  and  are 3CNFK clauses, can be trivially
We therefore consider sets of 3CNFE formulas. A 3CNFE formula
is a conjunction of 3CNFE clauses, each with three disjuncts. Each
disjunct in a 3CNFE clause is either a propositional literal or a formula
having the form 2C or :2C, where C is a 3CNFE formula. For
example,
where each l i (1  i  8) is a propositional literal, is a 3CNFE formula.
For any formula ', there exist a 3CNFE formula logically equivalent
to ' in E.
Sets of 3CNFE formulas can be randomly generated according to the
parameters used to generate 3CNFK formulas, and a new parameter C
representing the number of clauses at depth d > 0. A 3CNFK is thus
a 3CNFE formula in which C = 1. As in the previous subsection, a
problem set is characterized by N and p: d and C are xed to 1 and L
respectively; L is given increasing values in such a way to empirically
cover the \100% satisable { 100% unsatisable" transition. We also
A. Tacchella
Systems comparison - N=4, d=1, %p=0
CPU
TA+GH
Systems comparison - N=5, d=1, %p=0
CPU
Systems comparison - N=6, d=1, %p=0
CPU
Systems comparison - N=7, d=1, %p=0
CPU
TA+GH

Figure

8. Logic E. *sat, *sat+GH, Dlp+GH, and TA+GH median CPU time.
samples/point. Background: satisability percentage.
check that in each sample there are no multiple occurrences of a formula
in a clause, at the same ensuring that the propositional vs. the
modal structure of the formula only depends on p. Notice that while
increasing L also C is increased. As a consequence, for each pair of
in an assignment satisfying a 3CNFE formula,
the recursive E-consistency check for ( has itself a phase
transition from 100% satisable to 100% unsatisable when increasing
L. Overall, for low [resp. high] values of L we expect that each satisfying
assignment should be trivially determined to be E-consistent [resp. not
E-consistent].
As before, for each value N in a problem set, 100 3CNFE formulas
are randomly generated, and the resulting formulas are given in input to
the procedure under test. A timeout stops the execution of the system
on a formula after 1000 seconds of CPU time. We consider the following
problems sets:
SAT-Based Decision Procedures for Classical Modal Logics 25
percentiles
CPU
percentiles
CPU
percentiles
CPU
percentiles

Figure

9. Logic E. 50%{100% percentile CPU times of *sat, *sat+GH, Dlp+GH,
and TA+GH. samples/point.
PEN4p0, PEN5p0, PEN6p0, PEN7p0 in which
respectively, and
PEN4p50, PEN5p50, PEN6p50, PEN7p50 in which
Given the huge amount of time that Flotter takes to prepare the
formula for Spass, we run TA+GH only on the problems sets PEN4p0
and PEN4p50. For PEN7p0 and PEN7p50, we run TA+GH only on the
initial points. As in the preceding subsection, we only take into account
the time the systems take for the main processing of the formula. In
particular, for each system, we do not take into account the time needed
to perform the Gasquet and Herzig's conversion; and for TA+GH we
take into account only the time taken by Spass. The median and the
percentile plots of the systems on PEN4p0, PEN5p0, PEN6p0, PEN7p0
are shown in Figure 8 and Figure 9 respectively.
Consider Figure 8. As can be observed, *sat is the fastest: the gap
with the other systems is of more than one order of magnitude for
26 E. Giunchiglia, F. Giunchiglia and A. Tacchella
Systems comparison - N=4, d=1, %p=50
CPU
TA+GH
Systems comparison - N=5, d=1, %p=50
CPU
Systems comparison - N=6, d=1, %p=50
CPU
Systems comparison - N=7, d=1, %p=50
CPU

Figure

10. Logic E. *sat, *sat+GH, Dlp+GH, and TA+GH median CPU time.
samples/point. Background: satisability percentage.
certain values of L. However, both *sat+GH and Dlp+GH perform
quite well, better than one could have imagined given that the consistency
problem for E and K 2 belongs to two dierent complexity classes.
However, a closer look to Gasquet and Herzig's reduction reveals that,
considering a 3CNFE formula ', and an assignment
(as usual we assume that
is a propositional formula)
in the language of ',
1.  satises ' i GH satises 'GH .
2. for checking the E-consistency of , *sat performs at most 2mn
consistency checks involving the formulas
3. for checking the K 2 -consistency of GH , both *sat+GH and Dlp+GH
perform at most 2mn consistency checks involving the formulas
SAT-Based Decision Procedures for Classical Modal Logics 27
percentiles
CPU
percentiles
CPU
percentiles
CPU
percentiles
CPU

Figure

11. Logic E. 50%{100% percentile CPU times of *sat, *sat+GH, Dlp+GH,
and TA+GH. samples/point.
The rst two points are obvious. To understand the last, it suces to
notice that GH is propositionally equivalent to
Given that both *sat+GH and Dlp+GH use caching, these procedures
will perform at most a quadratic number of checks in the number of
subformulas of 'GH . This is not the case for TA+GH, since Spass does
not have any caching mechanism. This explains the good behavior of
*sat+GH and Dlp+GH, and the bad behavior of TA+GH.
It is interesting to compare *sat+GH and Dlp+GH performances.
As can be observed, Dlp+GH performs better than *sat+GH for low
values of L, but worse for high values of L. This behavior re
ects
the dierent mechanisms used by *sat+GH and Dlp+GH to prune
the search space when checking the K-consistency of an assignment.
As we said in Section 4, in all the tests we have set *sat+GH as to
28 E. Giunchiglia, F. Giunchiglia and A. Tacchella
use the early pruning strategy. Dlp+GH instead implements a backjumping
schema in the spirit of (Baker, 1995): when an assignment is
discovered to be not K-consistent, backtracking to a point which does
not lead to the same contradiction is enforced. While implementing
early pruning does not introduce overheads, this is not the case for
backjumping, where a dependency set of each derived clause has to be
maintained (see (Patel-Schneider, 1998) for more details). Despite the
additional costs introduced, backjumping clearly wins if compared to
early pruning in logic K, for low values of L. In this case, almost each
assignment is K-consistent and early pruning may cause additional (i.e.,
not performed by a backjumping strategy) checks. On the other hand,
for high values of L, when the formula under test is not K-consistent but
there are still assignments satisfying it, *sat+GH is able to greatly cut
the search by checking the inconsistency of the assignment generated
so far. Dlp+GH instead checks the consistency of an assignment only
when it satises the current formula. Dlp+GH may therefore generate
many assignments which, even though they satisfy the input formula,
are not K-consistent. When L is so high that the input formulas become
propositionally unsatisable, *sat+GH may still perform additional
K-consistency checks, but these get compensated by (i) *sat+GH
SAT-based nature and (ii) the costs Dlp+GH has because of back-
jumping. Considering Figure 9, we see that Dlp+GH has a better
behavior than the other systems on some of the hardest instances.
Evidently, on these tests, backjumping leads to a more uniform behavior
than early pruning. Horrocks and Patel-Schneider (1999a) show that
for some randomly generate 3CNFK formulas, early pruning leads to a
more uniform behavior than backjumping.
For
TA+GH median and percentile times are plotted in Figure 10 and

Figure

respectively. As it can be observed, the situation is very
similar to the case in which 0%. The only dierence is that
now *sat+GH performs better than Dlp+GH for a lower value of L.
This is reasonable, since for each L, the number of consistency checks
performed by *sat+GH because of early pruning, diminishes when p
increases.
Finally, notice the easy-hard-easy pattern of *sat. To better appreciate
it, Figure 12 shows the number of calls to Lsat done by Lconsist
on PEN4p0-PEN7p0 (left), PEN4p50-PEN7p50 (right) against the ratio
between L and N . As can be observed, *sat performs a number of
Lsat calls whose maximum roughly correspond to the 50% of satisable
formulas. This transition happens when L
N is close to 5 for
to 6 for This behavior re
ects the above stated intuition
SAT-Based Decision Procedures for Classical Modal Logics 29
checks d=1 %p=0
OF
CHECKS
7 variables
6 variables
5 variables
4 variables
Consistency checks d=1 %p=50
OF
CHECKS
7 variables
6 variables
5 variables
4 variables

Figure

12. Logic E. *sat median number of calls to Lsat. 7.
samples/point. Background: satisability
percentages.
according to which for low [resp. high] values of L all the formulas
should be easily determined to be E-consistent [resp. not E-consistent].
6. Conclusions and future work
We have presented a set of SAT-based decision procedures for eight
classical modal logics. We have shown how the SAT-based approach
allows for ecient and modular implementations for these logics. We
have presented *sat. *sat is the only system that is able to deal with
EN, EC, ECN and EMN. In the case of the logic E, we have dened
a testing methodology which generalizes the 3CNFK methodology by
Giunchiglia and Sebastiani (1996a), and which is suitable for testing
systems for non-normal modal logics. The experimental evaluation
shows that *sat performances are superior to or comparable to the
performances of other state-of-the-art systems.
In the future, we plan to conduct an extensive experimental analysis
(similar to that presented in (Horrocks and Patel-Schneider, 1998; Horrocks
and Patel-Schneider, 1999b)) to understand, for each class of
formulas, which combination of *sat options leads to the best results.
We also plan to extend *sat in order to handle more expressive decidable
logics. We will also consider logics, like S4, for which more
sophisticated methods than that described in Section 4 have to be
employed in order to ensure the termination of the decision procedure.
A. Tacchella

Acknowledgments

We are grateful to Hullrich Hustadt, Peter F. Patel-Schneider, and
Hantao Zhang for the assistance they provided on their systems. Special
thanks to Roberto Sebastiani for the many useful discussions related
to the subject of this paper. Thanks also to the anonymous reviewers
for their helpful comments and suggestions. The rst and last authors
are partially supported by the Italian Spatial Agency.



--R


Intelligent backtracking on the hardest constraint problems.
An algorithm to evaluate quanti
Modal Logic - an Introduction
A computing procedure for quanti

Swart, editor. Automated Reasoning with Analytic Tableaux and Related Methods: International Conference Tableaux'98
Reasoning about knowledge.
Proof Methods for Modal and Intuitionistic Logics.

Improvements to propositional satis
PhD thesis
From classical to normal modal logics.
Ideal and Real Belief about Belief.
Building decision procedures for modal logics from propositional decision procedures - the case study of modal

of the 5th International Conference on Principles of Knowledge Representation and Reasoning - KR'96
More evaluation of decision procedures for modal logics.
Labelled tableaux for non-normal modal logics
Comparing subsumption optimizations.
Advances in propositional modal
Optimising description logic subsumption.
Journal of Logic and Computation
Using an expressive description logic: FaCT or
On evaluating decision procedures for modal logic.
On evaluating decision procedures for modal logic.
BLACKBOX: A new approach to the application of theorem proving to problem solving.
Strongly analytic tableaux for normal modal logics.

Translation methods for non-classical logics - an overview
DLP system description.
Personal communication
A Structure-preserving Clause Form Translation
Journal of Symbolic Computation
From Tableau-based to SAT-based procedures - preliminary report
An Essay in Classical Modal Logic.



On the complexity of proofs in propositional logics.
Correspondence theory.
On the complexity of epistemic reasoning.


--TR

--CTR
Andrei Voronkov, How to optimize proof-search in modal logics: new methods of proving redundancy criteria for sequent calculi, ACM Transactions on Computational Logic (TOCL), v.2 n.2, p.182-215, April 2001
Enrico Giunchiglia , Armando Tacchella, A Subset-Matching Size-Bounded Cache for Testing Satisfiability in Modal Logics, Annals of Mathematics and Artificial Intelligence, v.33 n.1, p.39-67, September 2001
SAT-based planning in complex domains: concurrency, constraints and nondeterminism, Artificial Intelligence, v.147 n.1-2, p.85-117, July
Enrico Giunchiglia , Yuliya Lierler , Marco Maratea, Answer Set Programming Based on Propositional Satisfiability, Journal of Automated Reasoning, v.36 n.4, p.345-377, April     2006
Alessandro Armando , Claudio Castellini , Enrico Giunchiglia , Marco Maratea, The SAT-based Approach to Separation Logic, Journal of Automated Reasoning, v.35 n.1-3, p.237-263, October   2005
Franz Baader , Diego Calvanese , Deborah L. McGuinness , Daniele Nardi , Peter F. Patel-Schneider, Bibliography, The description logic handbook: theory, implementation, and applications, Cambridge University Press, New York, NY,

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-based testing'
 'zero storage biometric authentication' 'model checking']
marked:['modal logic', 'SAT-based decision procedure']
--T
Fundamental Laws and Assumptions of Software Maintenance.
--A
Researchers must pay far more attention to discovering
and validating the principles that underlie software maintenance
and evolution. This was one of the major conclusions reached
during the International Workshop on Empirical Studies of Software
Maintenance. This workshop, held in November 1996 in Monterey,
California, brought together an international group of researchers
to discuss the successes, challenges and open issues in software
maintenance and evolution.This article documents
the discussion of the subgroup on fundamental laws and assumptions
of software maintenance. The participants of this group included
researchers in software engineering, the behavioral sciences,
information systems and statistics. Their main conclusion was
that insufficient effort has been paid to synthesizing research
conjectures into validated theories and that this problem has
slowed progress in software maintenance. To help support this
vision they identified the following supporting conclusions.
(1) need to develop a genuine scientific approachmore
than just using empirical methods, (2) other disciplines can
help, (3) need to explore a wider range of approaches to empirical
studies, (4) need to study both evolution and maintenance.
--B
Introduction
All software systems evolve [1]. As they evolve they undergo numerous, successive changes - to fix errors,
improve performance or other attributes, and to adapt to new environments. The longer a system stays in
service, the larger maintenance costs are, and, therefore, it's not surprising that maintenance costs often
dominate initial development costs [2]. Clearly, improved tools, techniques, and processes can save a great
deal of time and money throughout the software industry.
To realize these savings, researchers are studying maintenance from many perspectives.
Making individual modifications. To make changes developers have to understand the existing system,
evaluate the effect of proposed changes, and then implement and validate it. To address these issues,
researchers study topics such as design recovery, program comprehension, impact analysis and regression
testing.
Coping with evolution. Because systems undergo many changes, not just one, other researchers focus on
the nature of change and how to limit its impact. Research in this area includes structured programming,
object-oriented programming languages, software architectures and configuration management.
Managing the maintenance process. Maintenance is costly and somewhat unpredictable. Thus,
management tools for controlling maintenance are in great demand. Many researchers have developed
metrics and models to predict when components are likely to need changing and how much that will cost.
These are only a few examples of maintenance research. There are many others. Clearly, software
maintenance is a significant problem that continues to foster considerable activity. However, as with other
software engineering areas, there is a concern that our research efforts lack hard evidence and critical
evaluation, and that without these, we can't develop a deep understanding of what tools and processes work,
when they work, or why. Consequently, many people believe that rigorous empirical methods must be one
of the cornerstones of research in our field.
This belief brought a large group of researchers to the International Workshop on Empirical Studies of
Software Maintenance. The workshop was help on November 8, 1996 in Monterey, California and involved
researchers from software engineering, the behavioral sciences, information systems and management, and
statistics. The goal of this meeting was to discuss the strengths, weaknesses, and open issues in empirical
methods and determine how they could be profitably applied to improve software maintenance research.
The workshop attendees were divided into four groups.
1. Study and assessment of (new) technologies,
2. Methodologies for performing empirical studies,
3. Studies of the maintenance process: fundamental laws and assumptions, and
4. Maintenance process improvement in practice.
This article summarizes the findings of subgroup #3: Studies of the maintenance process: fundamental laws
and assumptions. This subgroup is distinguished from others because their studies are not specific to
particular tools or methods. Instead it tries to identify general principles. The group's participants have
expertise in empirical methods, but approach their studies in different ways. Their backgrounds are diverse:
software engineering, behavioral science, business, statistics, and others.
2. Summary of Discussion
Subgroup #3 had two working sessions. During the first session, each participant presented their research
and took questions from the other participants. Before the workshop each participant received the following
list of questions about their work.
1. What were the study's goal and questions?
2. What empirical methods did you use (e.g., controlled experiment, case study, survey)?
3. Describe your study's design.
4. What threats to validity did you consider (construct, internal, external)?
5. What did you do well and what will you change in the future?
6. How much did it cost to perform the study?
7. What new questions do your results raise?
8. Can other researchers replicate your study?
2.1 Synopsis of Presentations.
This section summarizes each presentation. Of course, the few paragraphs given to each presentation cannot
capture the careful thought that goes into designing and conducting empirical research. Please see the
complete proceedings. Any mistakes or misrepresentations are unintentional, and are the sole responsibility
of the session chair, not the workshop participants.
. Maintenance of Reuse-Based Domain-Specific Software Product Lines[3]. Presented by William
Thomas:
Reuse saves money and time. Therefore, greater levels of reuse promise even greater savings.
Although this seems to be correct, the authors urge us to look more closely. A common way to get high
reuse levels is to split systems into two parts: one specific to the domain and the other to the
application. The domain-specific part provides the infrastructure: interconnection support, common
functions, and system architecture. Everything else is the application-specific part.
The authors strongly agree that this approach reduces coding effort, but warn us that it may also
redefine maintenance. Traditionally, applications are developed and maintained separately. However,
when applications share code, they may need to be maintained as a unit. Of course, developers can still
choose to maintain all applications individually, but may lose the benefit of domain-specific assets.
They might also choose to maintain all related applications as a unit, but that may constrain individual
applications.
Since the best way to manage this situation is unclear, the authors will conduct empirical studies using
historical data. They will explore such issues as measuring the cost of evolving applications and
domains, coupling between applications and domain, patterns of change, and the effect of domain
maturity on maintenance strategies.
This work reminds us that new technology can challenge, even invalidate, our basic assumptions.
. Using a Didactic Model to Measure Software Comprehension[4]. Presented by Elisabeth Dusink
One of the first steps in changing a system is understanding it. Documentation is supposed to help
understanding because it collects system knowledge and presents it in a structured way. The authors of
this article claim that developers learn about a system by reading its documentation. Furthermore, they
suspect that qualities of the learning process affect the quality of maintenance. Based on a model of
learning by Bloom[5], they argued that learning progresses from gaining knowledge, to understanding
it, applying it, analyzing it, and, finally, to synthesizing new knowledge from it.
The authors discussed a controlled experiment to link these levels of learning to maintenance success.
The experimental subjects, second-year CS students, will be placed into three- and four-person groups.
Each group will be given a 7,000 line email system written in C, and asked to modify it. They expect
the change to take about 80 hours per person to complete. For each modification the experimenters will
determine whether the group made the correct changes. Each person's understanding level will be
measured via a questionnaire.
One potential outcome of this research is a low-cost index for maintenance success. That is, we may be
able to test new technology by measuring its effect on understanding levels rather than by measuring its
effect on maintenance tasks themselves.
. Change-Prone Modules, Limited Resources and Maintenance[6]. Presented by Warren Harrison
Maintainers often work under tight deadlines with meager resources. So sometimes they take short cuts
even though they'd prefer to design and implement each change carefully. This kind of change is "ad
hoc". It can be made rapidly, but often degrades structure. In contrast "planned, structure preserving"
changes may preserve structure, but are more expensive. Harrison argues that ad hoc changes aren't as
damaging as has been claimed. He discussed data that showed that, in practice, changes tend to be
confined to a small part of the system. His conclusion is that modules that rarely change cannot degrade
and, thus, can tolerate ad hoc changes.
The author also outlined a model to help decide when to make ad hoc changes and when to restructure.
In the model, the cost of a maintenance request is a function of the request's complexity, the module's
state, and the kind of change made. Although the authors are still developing the model, it captures two
important notions: (1) ad hoc patches degrade structure more than planned, structure preserving
changes do, and (2) the effect of ad hoc changes will be compounded in modules that change often.
One interesting feature of this research is that it uses mathematical techniques to model long-term
evolution. Since, by definition, evolution takes time, researchers must find ways to reason about it.
Tools like these are a first step in that direction.
. An Empirical Exploration of Code Decay[7]. Presented by Adam Porter
Software systems must tolerate numerous, successive changes. As this happens, systems deteriorate and
changes become increasingly difficult. Eventually, new functionality cannot be added and the system
must be redesigned. Since solutions to this problem would be valuable, these researchers are
conducting a long-term, multidisciplinary project to examine the fundamental causes, symptoms, and
remedies for code decay.
The project team contains researchers in Statistics, Experimentation, Organizational Theory,
Programming Languages, Software Engineering, and Visualization. Their primary data source is the
AT&T 5ESS^TM switching system and its development data. This includes the switch's source code and
change control history (-700,000 changes) covering the last 15 years, its planned and actual
development milestones, effort and testing data, organizational history, development policies, and
coding standards.
To help the project achieve its goals quickly, they have constructed a set of model code artifacts that
they call the "code decay testbed". To do this they developed several small systems, attempted to
induce code decay in them, and used the resulting systems to evaluate ideas about code decay.
This research is a good example of how laboratory and field studies can work together to strengthen
results.
. On Increasing Our Knowledge of Large-Scale Software Comprehension[8]. Presented by Anneliese
von Mayrhauser
Far too often researchers create solutions (tools) without understanding the problem. Thus, there's a
tendency to focus on a tool's novelty or performance rather than on its efficacy. The authors believe
that program comprehension is a fundamental maintenance problem. Consequently, they have begun to
study how software developers understand programs and how maintenance tools can make it easier.
They presented a case study in which they analyzed how 11 professional programmers performed
maintenance. Then they tried to correlate what they saw with known theories of program
comprehension. These theories suggest that several factors may affect maintenance: task type, prior
exposure to the system, domain expertise, and language expertise.
The authors felt that limited sample size was one of the biggest problems that they and other
experimenters face. Therefore, they argue that we will have to combine data from multiple studies.
Further, this will require better methods for comparing studies, sharing data and terms, and analyzing
aggregate data.
One implication of this is that experimental results must generalize beyond the environment from which
they were taken. This may be one of the most important open issues in empirical software engineering.
. The Software Maintenance Process in Portugal[9]. Presented by Helena Mendes-Moreira
Maintenance costs are the dues of success. In this work the authors surveyed 37 software development
organizations in Portugal to characterize their maintenance processes. They selected companies with
more than 500 developers and revenues in excess of 13 million dollars. After they examined the survey
responses they conducted interviews with several of the respondents.
They found that most of the maintenance performed in these organizations involved quick fixes, rather
than planned enhancements. Also, in many cases, support activities such as updating documentation
were not done. More importantly, they found that the ratio of maintenance effort to new development
effort was growing rapidly. This might be expected in companies that enter the software market,
develop new products, and now, for the first time, must maintain them. Several participants noted
similar patterns in US industry.
This work raises the possibility of comparing historical data from established organizations with
current data from emerging ones. Would they find common patterns of organizational growth despite
the changes in technology?
. Predicting Aspects of Maintainability from Software Characteristics[10]. Presented by Jarrett
Rosenberg
We have to change the way we study change. The author presented several studies that tried to predict
the likelihood and cost of certain maintenance tasks. His results cast doubt on the soundness of current
metric-based modeling approaches. First he found little correlation between static complexity metrics
and repair activity. Second, he found considerable and unexplained variation in the metric values,
suggesting that other, unknown, factors are driving maintenance.
The author stressed two flaws in current metrics research. Metrics tend to be purely syntactic, which
excludes vital semantic information. They also tend to be static, ignoring information about a system's
development and evolution (e.g., testing history). The solution, however, is not to define new measures.
Instead, he argued, we will need to focus on creating deep theories of maintenance-related factors.
This presentation stressed two important research goals: to continue raising our scientific standards,
and to borrow wisely from the ideas and approaches of other research areas.
. Operational System Complexity[11]. Presented by Scott Schneberger
Maintenance bottlenecks can shift as technology changes. The industry is moving more and more from
centralized to distributed architectures. Will this shift change our assumptions about maintenance?
Schneberger suggests that it might. Since, he claims, there is little data on the maintenance of
distributed applications, he performed an exploratory study.
In this work he modeled a system's complexity in terms of its components, their internal complexity,
and their interactions. To test this idea he conducted a survey. His initial results were that as
distribution increased (i.e., components spread over more processors), system complexity increased,
while component complexity decreased. One of his conclusions is that distribution shifts complexity
out of individual components and into their interconnections.
The author also points out that, in some domains, there is a trend back toward centralization. These
results suggest that the benefits of solving certain maintenance problems can change over time.
. The Impact of Documentation Availability on Software Maintenance Productivity[12]. Presented by
Eirik Tryggeseth
One way to evaluate technology is to ask what would happen if it didn't exist. The author takes this
approach to see how documentation affects maintenance success. He also asks whether this effect is
different for more- or less-skilled programmers. To do this he performed a controlled experiment with
34 undergraduate students in computer science as subjects. Before the experiment he also measured the
students' skill in reading and writing C++ programs.
During the experiment the participants were asked to modify a 2700 line C++ program. Half of them,
group B, were given documentation, the rest, group A, were not. The experimenters measured the
amount of time that the participants spent understanding and modifying the system, and they measured
the quality of the modification. They found people who had documentation, group B, spent less time
understanding the system and made higher quality changes. They also found that the performance
measures for group B were correlated with the skill measures, while those of group A were not. One
explanation might be that Group B understood the system quickly, so programming skill became the
limiting factor for task performance. Group A, however, found it hard to understand the system, and,
therefore, were unable to profit from their programming skill.
One implication of this result might be that system-level knowledge has a greater (or, at least, more
immediate) effect on maintenance quality than individual programming skill does.
. Assessing Maintenance Processes Through Controlled Experiment[13]. Presented by Guiseppe
Visaggio
Sometimes the simplest approach is the most cost-effective. In this work the author explores the cost-benefit
tradeoffs between quick-fixes and a more thorough change process, called iterative
enhancement. The goal of this work is similar to Harrison's (described earlier), but uses a controlled
experiment rather than mathematical modeling.
The participants in this experiment were asked to modify two different systems, once using the quick-fix
approach, once using an iterative enhancement approach. The author measured the correctness,
completeness, effort, and traceability of each change. They found that quick fixes were less reliable and
degraded structure more than iterative enhancements did. One other interesting result was that quick-
fixes were done faster only when modification affected fewer than ten modules.
One fascinating observation is that these results appear to agree with those discussed by Harrison even
though the researchers used very different research methods.
2.2 Organizing the Current Literature
In the second session the group analyzed their presentations and other existing research to see what, if any,
fundamental laws of software maintenance are known. Because maintenance has so many facets, we first
developed a very rough taxonomy of the factors that might affect maintenance. Then for each category we
tried to synthesize common results, hoping to identify potential laws. We thought this might also indicate
open areas that should be, but are not being, addressed. The categories represent factors related to product,
people, process, and task.
1. Product. These factors relate to how attributes of system artifacts affect maintenance. We divided
product factors into two subcategories: complexity and structure. We assumed that complexity affects
our ability to understand the product. Research in this area includes software complexity metrics and
structured programming. Structure refers to how system components are organized. We assumed that
structure affects how changes impact a system. Research in this area includes studies of ripple-effect,
development of design patterns, and studies of software architecture and domain-specific languages.
2. People. These factors relate to how the attributes of individuals and groups affect maintenance. We
discussed three levels of this factor: Individual, Team, and Organizations. We assumed that
abilities, group dynamics, and organizational constraints affect maintenance. Research in
these areas include program comprehension, groupware, and cycle-time reduction studies.
3. Process. These factors relate to how the activities that individuals and teams carry out routinely affect
maintenance. We assumed that process factors affect an organization's ability to predictably achieve
their development goals. Research in this area includes documentation approaches, and configuration
management tools.
4. Task. These factors relate to the viewpoint from which maintenance is studied. Some research focuses
on individual changes while other focuses on longer-term evolution processes.
Within each of these categories, we tried to extract common findings. One topic for which we had some
success was modularization. Several people stated that Parnas' early work on information hiding [14]
illustrated some benefits of modularization, and that the effect of this and other research can be seen in
today's object-oriented programming languages. See Kemerer [15] for a survey of results in this area.
Unfortunately, we were unable to find too many more. Certainly some exist. But they didn't spring to mind
quickly. Although the group members knew of many studies, we couldn't distill their messages. Sometimes
group members disagreed about the interpretation of single paper. Sometimes one paper's findings were in
conflict with another paper's. Which one, if either, was correct? In many other cases we didn't think the
work proposed any general findings.
Our interpretation is that as a field, we've asked many questions and taken many measurements, but what
we've learned is unclear. Thus, our first conclusion is that we need to review the many studies published in
the literature to synthesize potential theories.
2.3 The Next Steps
Given more time, we probably could have found more common results, but the difficulties we had were
startling. One problem is that there hasn't been enough emphasis on synthesizing individual results into
theories. But another, more fundamental, problem is that many studies aren't designed to produce general
results.
Consider the following kinds of empirical research.
Feasibility studies. These studies are meant to validate new technology. Typically, the experimenter
exercises his or her tool or method to show that it performs better than some other method or tool. These
studies compare performances, but they rarely focus on the properties that make one tool better than
another.
Statistical Modeling. Many researchers have modeled the relationships between various software metrics
and maintenance attributes, such as change effort, severity, and locality. Since these models fit data without
understanding it (correlation vs. causality), there's a very little reason to believe that they will apply to other
data sets.
Observational studies. We also see many studies that document the behavior of a single project or
organization. These studies may be useful as benchmarks of typical behavior, but they aren't intended to
test any hypotheses. In fact, their authors rarely draw any actual conclusions.
Each of these types of studies serves a purpose and can further our understanding of software engineering.
However, they are not designed to and are not likely to produce general theories. Thus, our second
conclusion is that theory building must be designed into our empirical research.
3. Future Challenges:
In the previous sections we reviewed some of the subgroup's initial discussions. The group's activity took
place in two sessions. In the first session, the participants presented their studies and discussed their goals,
strengths, and weaknesses. During the second session we created a scheme for classifying maintenance
studies, classified studies using this scheme, and then looked for common research findings in each class.
We found some common findings, but not as many as we thought we should have. This led us to look more
carefully at the field and ask why common results were so hard to find. Our main conclusion is that a critical
part of scientific activity has been neglected in software maintenance research. That part is theory building -
which can be done either by synthesizing individual results, or by proposing initial hypotheses and testing
and refining them.
The last issue we wrestled with was how to remedy this situation. In this section we discuss some of our
recommendations for dealing with this problem. We divided these recommendations into three groups:
rethinking the goal of empirical methods, supporting interdisciplinary research, and expanding the use of
empirical methods.
3.1 Rethinking the Goals of Empirical Methods
The quality of empirical research has improved tremendously in the last few decades. But it must continue
to improve. In particular, we cannot forget that measurement is only one part of scientific inquiry. We
routinely use measurement to describe, predict, and test. This is important, but, by itself, it does not give us
the deep understanding we need to control software development.
. To gain control over software development we need to have validated theories that are (1) general, (2)
causal, and (3) suggestive of control strategies.
General theories hold across several environments. Relationships that only hold in one environment are
still important (for instance, for process improvement), but as scientists we should not be satisfied with
them. Thus, we should strive to draw theories from our studies (even if they turn out to be wrong) so
that they can be tested in other environments.
Theories should also be causal. Although the literature describes many reasonably-good predictive
models, they only capture correlations. We shouldn't confuse correlations with the underlying
principles we really care about. One of the key challenges to developing causal theories is to focus
more on discovering underlying principles and less on measuring high-level performance.
Finally, if we have multiple candidate theories, we should prefer theories that suggest practical control
strategies.
. Another issue to consider is human variation. Differences in natural ability affect every empirical study.
Some researchers are looking at ways to account for these differences, but much more work needs to be
done. As we discuss later, other fields have this problem as well and may have some insight that will
help us.
. Although we are ultimately concerned with professional programmers who build industrial software,
cost concerns lead us to use student subjects. Since we do not entirely understand the relationship
between the student programmers and professionals, these studies are often discounted. Consequently,
we need to develop models of the differences between student and professional populations.
. Science must be a public activity. There have been calls to make data public, but we must also share
artifacts, procedures, and terminology as well. Repositories and web sites should be set up and greater
efforts should be made to conduct collaborative research.
. Science is iterative. As we build and test theories, we will find almost all of them to be incorrect or
imprecise. As authors and as reviewers, we will have to change how we think about and how we present
our findings.
3.2 Supporting interdisciplinary research
Too often software engineering researchers think that our field is entirely unique. Software development
involves a web of individuals, groups and organizations working to build a complex array of products.
Therefore, it's certain that other fields have tools and techniques that we will find useful. For example,
. Behavioral scientists study how people work together (among other things). These have theories about
how people learn and understand, and how they work together. One of the group members, Jarrett
Rosenberg, coined the term "theory reuse" to describe this kind of interdisciplinary collaboration.
. Cognitive scientists study how people think. Consequently, they have experience defining instruments
to measure aspects of human skill. As we discussed earlier, such instruments could play a large role in
factoring human variation out of our empirical studies.
. Statistics already plays a big role in data analysis, experimental design, and hypothesis testing. We can
also benefit from their knowledge of simulation methods, visualization methods, and mathematical
modeling.
. Business disciplines such as organization theory and information systems management have a great
deal of knowledge about how process, organizational structure, and business strategies affect people's
ability to get work done. Understanding these effects may help us to reconcile studies from multiple
organizations. Also economists have considerable expertise in modeling complicated phenomena.
3.3 Expanding the Use of Empirical Methods
Controlled experiments are the standard method for verifying theories. However, they are not perfect. They
are expensive, have limited external validity, and it may be very difficult or, even unethical, to use them in
certain situations. Therefore, in addition to traditional analysis techniques, we should consider other
methods for generating and testing theories.
. Qualitative analysis. Refers to the analysis of data that is represented in words and pictures rather than
as numbers [16]. These approaches do not have as many supporting analyses as traditional quantitative
methods do, but may provide a richer description of the phenomena being studied.
. Meta-analysis. Gathering enough data to draw sound conclusions is a major problem in empirical
research. One way to solve this problem may be to integrate the data and results of multiple studies.
This can be done on an ad hoc basis, but there also approaches for statistically integrating the data.
These approaches are called meta-analysis.
. Mathematical modeling and simulation. This approach has been widely used in computer science, but
not in software engineering. This is unfortunate because mathematical models can be powerful and
much less expensive then experimenting with an actual system or process. They can also be very
effective as a compliment to other approaches.
. Case studies. Many case studies are simply retrospective descriptions of a project. Sometimes we call
these "lessons learned" articles or "experience reports". We should try to do more case studies in which
a hypotheses is stated and data is analyzed to see whether it is consistent with the hypothesis.
. Surveys. Surveys can be a very inexpensive way to acquire lots of information at low cost. They have
problems, but again, there is a large community of researchers that use them and understand their
limitations.
One final point to make here is that each of these approaches has strengths and weaknesses. Sometimes the
best thing to do will be to combine two or more approaches.
4.

Summary

Because software vendors must respond frequently and correctly to changing user expectations, hardware
platforms, and alternative products, software systems must tolerate numerous, successive changes. In
practice, however, systems deteriorate and changes become increasingly difficult to implement. Thus it's
easy to see why the costs of maintenance often dominate the costs of initial development.
Research that makes maintenance easier will save considerable time and money throughout the software
industry. Therefore, researchers have identified some sources of maintenance problems and developed
potential remedies for it. For example, some researchers believe that repeated changes complicate a system's
internal structure, so they have developed code metrics to model structural complexity. Others think that
some software designs are inherently less flexible than others so they have focused on design patterns and
software architecture. Still others argue that changes become more difficult because, over time, developers
lose their understanding of the system They have developed reverse engineering tools, and formal
documentation.
The exact manner and degree in which different factors affect maintenance is unclear. It is clear, however,
that this information is vital if our research is to have sustained, predictable improvement. Because many
researchers share this belief, they attended a workshop on the topic of empirical studies of software
maintenance. This article tries to summarize the ideas of the subgroup on Studies of the maintenance
process: fundamental laws and assumptions.
This group's main conclusion was that research community in software maintenance has produced many
interesting results, but failed to consolidate them into useful theories. To help remedy this situation the
group made three recommendations.
1. Researchers should design studies whose goal is to generate, test, and refine useful theories, not simply
to describe behavior. This implies that we must use much more sophisticated empirical methods than
we currently do.
2. Researchers should look to other disciplines for helpful insights, tools, and theories. However, they
should borrow wisely.
3. Empirical research is severely limited by our inability to find adequately-sized samples. Researchers
need to consider non-traditional techniques for combining data from multiple sources, and need to use
multiple data collection and analysis approaches to make better use of small data sets.
5. List of participants
The session chair would like to thank the subgroup participants for helping to make the workshop a success:
Thomas, Helena Mendes-Moreira, Elisabeth Dusink, Warren Harrison, Jarrett Rosenberg,
Giuseppe Visaggio, Marie Vans, Anneliese von Mayrhauser, Eirik Tryggeseth, Scott Schneberger.
6.



--R

A Model of Large Program Development.
System Structure and Software Maintenance.
Maintenance of Reuse-Based Domain-Specific Software Product Lines
Using and Didactic Model to Measure Software Comprehension.
Taxonomy of Educational Objects.

An Empirical Exploration of Code Evolution.
On Increasing our Knowledge of Large-Scale Software Comprehension
The Software Maintenance Process in Financial Organizations.
Problems and Prospects in Quantifying Software Maintainability.
Position Paper for the International Workshop on Empirical Studies of Software Maintenance.
The Impact of Documentation Availability on Software Maintenance Productivity.
Assessing Maintenance Processes Through Controlled Experiment.
On the Criteria for Decomposing Systems into Modules.
Software Complexity and Software Maintenance: A Survey of Empirical Research.
Methods in Qualitative Family Research
--TR

extracted:['file assignment' 'feedback controls' 'feature weights'
 'fault-tolerant software systems' 'fault-tolerant routing algorithm'
 'fault-tolerant routing' 'fault-tolerant algorithms' 'fault-tolerance'
 'fault-based testing' 'information retrieval']
marked:['empirical studies', 'interdisciplinary research', 'software maintenance', 'theory development']
--T
Further Experiences with Scenarios and Checklists.
--A
Software inspection is one of the best methods of verifying
software documents. Software inspection is a complex process,
with many possible variations, most of which have received little
or no evaluation. This paper reports on the evaluation of one
component of the inspection process, detection aids, specifically
using Scenario or Checklist approaches. The evaluation is by
subject-based experimentation, and is currently one of three
independent experiments on the same hypothesis. The paper describes
the experimental process, the resulting analysis of the experimental
data, and attempts to compare the results in this experiment
with the other experiments. This replication is broadly supportive
of the results from the original experiment, namely, that the
Scenario approach is superior to the Checklist approach; and
that the meeting component of a software inspection is not an
effective defect detection mechanism. This experiment also tentatively
proposes additional relationships between general academic performance
and individual inspection performance; and between meeting loss
and group inspection performance.
--B
Introduction
Software inspection is a method for statically verifying documents. It was first
described by Michael Fagan in 1976 [6]. Since then there have been many variations
and experiences described, but a typical inspection involves a team of
three to five people reviewing and understanding a document to find defects.
The benefits of inspection are generally accepted, with success stories regularly
published. In addition to Fagan's papers describing his experiences [6, 7], there
are many other favourable reports. For example, Doolan [4] reports industrial
experience indicating a times return on investment for every hour devoted to
inspection of software requirement specifications. Russell [20] reports a similar
return of 33 hours of maintenance saved for every hour of inspection invested.
This benefit is derived from applying inspection early in the lifecycle. By inspecting
products as early as possible, major defects are caught sooner and are
not propagated through to the final product, where the cost of removal is far
greater.
One point of disagreement between inspection advocates is the role of detection
aids. Many practitioners do not use any (The Ad Hoc approach), while
others use Checklists to aid and regularise their process. Recently, Parnas and
Weiss[16] have argued that even Checklists are not sufficiently focused to reliably
aid inspectors, and have suggested that inspectors should be allocated
specific responsibilities. Parnas argues that the Checklist approach results in
an inspector repeatedly covering the same ground, while ignoring other ar-
eas, resulting in reduced coverage. This idea has been further developed by
19], who has developed the Scenario approach in an attempt to
address this problem. As well as developing the idea, Porter has attempted to
show, via subject-based experimentation, that Scenarios are indeed superior to
both Ad Hoc and Checklist approaches.
To prove any hypothesis, results of experimental studies must be reproducible
and hence, much to their credit, Porter et al. have made their experimental
material available. This paper reports on our experiences and results in
undertaking a partial replication of Porter's experiment. It will also draw upon
experiences and results from another partial replication undertaken by Lanubile
and Visaggio[13]. The paper will attempt to 'mirror' the organisation of
Porter et al.[19] for ease of comparison. Hence the hypothesis of this replicated
experiment is the same as the hypothesis in the original experiment:
. that nonsystematic techniques with general reviewer responsibility
and no reviewer coordination, lead to overlap and gaps, thereby
lowering the overall inspection effectiveness; while systematic approaches
with specific coordinated responsibilities reduce gaps, thereby
increasing the overall effectiveness of the inspection. (Porter et
2 The experiment
To further evaluate this hypothesis, the authors have replicated the original
experiment. As with all subject-based experiments the design of Porter's experiment
can be criticised, and hence a modified experimental design was undertaken
to explore the hypothesis from a slightly different viewpoint. The alterations
to the experiment will be discussed as they arise within the paper. As
with the original experiment, the study uses students taking a formal university
course 1 and commenced with a training phase in which the subjects were taught
inspection techniques and the experimental procedures, during which they inspected
two sample software requirements specification documents 2 ; this was
followed by the experimental phase.
2.1 Experimental design
This partial replication will focus on only 2 detection methods, Checklists (non-
systematic) and Scenarios (systematic). This will simplify the design allowing
greater focusing at the cost of being unable to generalise any results to include
the Ad Hoc approach (nonsystematic), and hence our real hypothesis is less
generic than the original experiment.
1 The original experiment used graduate students, this study used undergraduate students
(3rd Year).
2 The original only used one sample document. All the documents inspected in all three
experiments are software requirements specification documents.
Variables
The experiment manipulates three independent variables 3 :
ffl The detection technique - Checklist or Scenario (Treatment Variable).
ffl The specification to be inspected - two were used during the experiment
(CRUISE, WLMS - see Section 2.2 for a brief description of these specifications

ffl The subjects or groups of subjects - 50 subjects organised into 16 groups.
The original experiment included another independent variable, the order in
which the specifications were inspected. It was decided that it was unfeasible to
allow variation in the order in which the specifications were inspected; this was
because the inspections were part of an assessed class and hence the possibility
of subject plagiarism corrupting the experimental data could not be discounted.
The original experiment included this variation, but does not discuss how it
avoids threats from plagiarism.
The original experiment also measured four dependent variables:
ffl The individual fault detection rate,
ffl The team fault detection rate,
ffl The percentage of faults first identified at the collection meeting (meeting
gain rate), and
ffl The percentage of faults first identified by an individual, but never reported
at the collection meeting (meeting loss rate).
The replication also measured the above variables, plus 3 additional variables

ffl The time spent by each participant for the individual inspection,
ffl The time spent by each group for the collection meeting, and
ffl Average academic performance of each subject.
Also the replication collected additional data from 4 debriefing questionnaires
per specification - one individual and one group).
Design
Unfortunately the traditional fractional factorial design is unsuitable for this
experiment because, as stated by Porter[19]:
3 The original experiment included an internal replication and thus included another independent
variable.
Round 1 Round 2
WLMS CRUISE WLMS CRUISE
Ad Hoc 1B, 1D, 1G 1A, 1C, 1E 1A 1D, 2B
1H, 2A 1F, 2D
Checklist 2B 2E, 2G 1E, 2D, 2G 1B, 1H
Scenario 2C, 2F 2H 1F, 1C, 2E 1G, 2A, 2C

Table

1: Subject allocation in the original experiment
. they require some teams to use the Checklist method after they
have used the Scenario method. Since the Checklist reviewers create
their own fault detection techniques during the inspection (based on
their understanding of the Checklist), our concern was that using
the Scenario method in an early round might imperceptibly distort
the use of the other methods in later rounds. Such influences would
be undetectable because, unlike the Scenario method, the Checklist
method does not require reviewers to perform specific, auditable
tasks.
Hence it was decided that a very straightforward design was the only alternative
available. Each subject would participate in two inspections using the
same technique. This design suffers from the problem that any ability effect
between the two groups can distort any results applied to the treatment vari-
able. Unfortunately, we have no direct way of measuring the subject's ability
at software inspection, and hence it was decided that a random allocation of
subjects to each group was the best alternative. In an attempt to estimate any
effect, the average academic performance for each group was calculated. This
indicated that the groups were extremely well balanced in terms of academic
performance further post-analysis has revealed
a strong linear association between average academic performance and average
inspection performance (Pearson, hence it is believed
that the experimental design has had little or no impact on the results
affecting the treatment variable.
The original experiment used a partial factorial design in which each group
participated in two inspections, using some combination of the three detection
methods, provided the above rule is not broken. Table 1 details the group
allocation (A-H), where a 1 denotes the first run of the experiment and a 2
denotes the second run (or internal replication).
Threats to internal validity
All experiments suffer from the problem that some factor may affect the dependent
variables without the experimenter's knowledge. This possibility must be
minimised. In the original experiment Porter discusses five such threats:
ffl Selection Effects
The replication's approach is described in the previous section. The original
experiment employed the following allocation strategy, in conjunction
with the partial factorial design detailed above:
. Create teams with equal skills. For example, rate each par-
ticipant's background knowledge and experience as either low,
medium, or high and then form teams of three by selecting one
individual at random from each experience category.
ffl Maturation Effects
In the replication, each subject uses the same detection method through-out
the experiment and the order of attempting the specifications is fixed,
hence any learning effect should either be symmetrical across the two
groups (i.e. derived from a common source, e.g. experimentation proce-
dure) or directly related to the treatment variable (i.e. increased understanding
of, or ability with, the detection method). The situation with
the original experiment is more complicated, due to the asymmetrical allocation
of subjects and the variation in the order of both applying the
detection methods and inspecting the specifications, see Table 1.
ffl Replication Effects
Since this external replication does not contain an internal replication
component, this potential threat is of no concern.
ffl Instrumentation Effects
This variation is impossible to avoid and is controlled by having each
subject inspect both specifications.
ffl Presentation Effects
In the replication, the CRUISE specification is presented before the WLMS
specification. Hence it is possible that the subjects can apply knowledge
from their CRUISE inspection experience to their WLMS inspection. (See
below for an explanation of why a fixed order of presentation was chosen
- Plagiarism.) It is believed that any such effect should be symmetric
between the groups and presents a far lesser risk than the risk of plagia-
rism. The original experiment varies the order to control this effect, as
described in Table 1.
In considering the experiment and conducting initial trials with the experimental
material, it was found that a larger list of internal threats exist than
discussed in the original.
ffl Plagiarism
It was decided that it was unfeasible to allow variation in the order in
which the specifications were inspected; this was because the inspections
were part of an assessed class and hence the possibility of subject plagiarism
corrupting the experimental data could not be discounted and was
considered to be a greater risk than any asymmetric maturation effects.
ffl Time Effects
The original experiment allocated two hours for an individual inspection.
It is believed that this fixed time period could have introduced a large
bias into the results. The two specification documents are 24 and 31
pages long. Hence an inspection rate of 15.5 pages per hour is required
for the longer document. This is well above the figure quoted in many
inspection texts, e.g. Ebenau and Strauss[5] recommend 5 to 7 pages for
specification documents, and Gilb and Graham[8] are even more conservative
recommending 0.5 to 1.5 pages for software development documents.
These figures by themselves do not show that the inspection rate in the
original experiment was incorrect because of variations in the assumptions
- the specifications are in a semi-formal notation, which the texts don't
explicitly address and no precise definition of what constitutes a page are
given. But they are a cause for concern. Also in their replication Lanubile
and Visaggio[13] state that "the time limit was too short" and during our
training of the subjects, it was found that the subjects required more than
2 hours to inspect a 16 page specification document.
Hence to minimise this threat to validity, the original experiments strict
time limit was abandoned and the subjects were given as much time as
they required. Post-analysis shows that the average subject took 374 and
430 minutes to inspect the 24 and 31 page specifications respectively 4 .
ffl Effort Effect
The original experiment asked the subjects to either inspect the
document using a Checklist or 2) inspect the document using one of the
three Scenarios. These two tasks require different amounts of time and
effort, the former requiring significantly more effort than the latter. This
problem is compounded, as in the original, when this division of labour
is used in conjunction with the fixed time limit. Asymmetric effort requirements
in the application of the different detection methods leads to
results parameterised by the percentage of task completed rather than a
dimensionless quantity. This viewpoint is also supported by Cheng and
Jeffery[2], who have carried out independent studies investigating the relationship
between Scenarios and Checklists, and is consistent with the
work of Kelly et al.[11], who found that increasing the amount to be
inspected caused a decrease in inspector's performance.
Hence to minimise this internal threat, the subjects using the Scenario
technique were asked to apply all three Scenarios to balance the effort.
Post-analysis shows that this alteration does balance the effort. A comparison
of times taken can be found in Table 2.
ffl Measuring multiple events
The hypothesis of the original and this replication compares the various
detection methods. The original experiment waited until after the group
4 Note: These times will have a small error associated with them. Given the length of the
study, the subjects had to self-time parts of the study.
5 ignoring the Ad Hoc approach.
Checklist Scenario Checklist Scenario
Mean 460 403 379 370
St. Dev. 180 168 158 137
F ratio 1.34 0.04
Prob.

Table

2: Average time taken to complete each inspection
collection meeting to collect data for this comparison. Data collected
at this point will be distorted by meeting gains and losses which are
independent of the method used.
Hence to minimise this threat, it was decided to collect data from before
the collection meeting.
ffl Statistical power
All experiments should consider this most important design factor. Although
statistical power is difficult to estimate, it is important to consider
it, often in conjunction with effect size, in any well designed experiment.
Given that the sample size was fixed (size of class undertaking the course)
and assuming a standard setting for the power level (0.8), an experimenter
can find out what size of effect must exist for their experiment to be considered
well designed 6 . In the case of the original experiment, a large 7
effect size must be found, i.e. a large difference must be expected for the
experiment to have an appropriate design. Unfortunately we do not know
the estimated effect size of the original experiment, as it is not contained
within the paper.
Hence to minimise this threat, it was decided to reduce the required effect
size by increasing the sample size. This was achieved in two ways: first
increased subject numbers and second by measuring for the effect at the
individual rather than the group level. This is especially appropriate given
the decision on "Measuring multiple events". Hence with the increased
sample size the replication can now reliably find significant results for
small to medium 8 effect sizes.
ffl Group dynamics
Obviously the effectiveness of the group meeting component cannot be
divorced from the dynamics of the group undertaking the inspections,
and since the experiment has a repeated measure design, concern exists
that the group dynamics will alter between the two inspection phases.
Hence to minimise this threat, it was decided to have different group
allocations for each phase, and therefore the group component resembles
6 The real situation is slightly more complicated than this, see Miller et al.[15] for a discussion
on the impact of Statistical power on empirical research.
7 about 0.75, for a definition of large, medium and small power, see Cohen[3].
an experiment with an internal replication.
There also exists some internal threats which we were unable to minimise fully.
ffl Degree of application of the detection aids
It is impossible to strictly enforce application of the detection aids all
of the time. In fact, that situation is unnatural. Inspectors start their
inspection by reading the document and obviously may well find defects
during this initial preparation. Hence it is reasonable to assume that
a proportion of the reported defects are not directly attributable to the
application of the detection aid, even if the subject using it applies it
fully. But which proportion? Should the defects for the initial read of
the specification be discounted? The inspector may already be implicitly
applying the detection aid while reading. Applying the detection aid to
check a particular fact on a particular page leads the inspector to uncover
an indirectly related fact, should this be discounted? What about an
inspector applying an aid in a slightly non-standard fashion again leading
to an error not directly attributable to the detection aid. Further imagine
an inspector who chooses to augment their detection aided work with
some additional Ad Hoc based inspection, during which they find errors
directly related to the detection aids, how could these defects to reliably
found and discounted? The authors believe that this is a very grey area
with few distinguishing boundaries, and hence to attempt to subdivide the
reported errors is highly dangerous and unadvisable, without large data
sets to help to account for these potential variations. The experiment
has relatively small data sets, especially with regard to defect coverage
of individual Scenarios, e.g. as reported in the original experiment, the
missing or ambiguous functionalities Scenario has a strict coverage of one
defect in the CRUISE specification, which was not discovered by any
subject, and hence all defects reported by application of this aid are at
best indirect or secondary effects of applying the aid.
Hence, rather than attempting to subdivide the recorded defects, the
subjects will be asked to answer questions in the debriefing questionnaire
about their level of application of the detection aid. Answers to these
and other related questions should provide evidence for the validity of the
undertaken experiment.
ffl Typographical errors in specifications
The specifications from the original experiment contained a significant
number of typographical errors. Despite removing many of these errors,
some still existed during the experiment and may have caused confusion
for some of the subjects. Also these typographical errors preclude any
analysis of the false positives reported. This is a major deficiency in the
original and subsequent replications.
ffl Initial Data Analysis
Initial data analysis consists of the experimenter deciding on which reported
defects are correct and which are incorrect. Defects are unstructured
English descriptions of what the subjects' believe to be incorrect,
and they are required to supply an accurate description of the fault. This
is a inexact process and has potentially a large degree of variability. To
illustrate, the experimenter checks possible defects against the master defect
list (a list of all the defects contained within the document); Lanubile
and Visaggio[13] state that they believe that this list from the original
experiment is in error and provide several additional defects which they
believe are missing from the original list, i.e. the researchers can't even
agree on the perfect solution. It is concluded that this experiment has
a natural in-built risk which is difficult, if not impossible, to safeguard
against, unless the experimenter has the resources to employ multiple
researchers cross-checking the initial analysis.
ffl Learning Curve - Novice Subjects
All the subjects had to learn both the inspection process and the specification
language. It is quite possible that the experiment has experienced
effects due to subjects lack of understanding of either of these topics.
Apart from careful teaching of this material, it is difficult to employ further
measures to safegaurd against this effect. Instead any effect will try
to be 'measured' via the debriefing questionnaire and may subsequently
alter the analysis of the independent variables.
Threats to External Validity
Threats to external validity limit the ability to generalise the results from an
experiment to a wider population, under different conditions. The threats to
external validity for the replication are the same as for the original experiment:
ffl The subjects in our replication may not be representative of the general
software engineering population, e.g. this study used students rather than
software professionals. This threat is always a problem, because of the
lack of sampling frame, and hence even studies using professionals will be
exposed to this threat.
ffl The specification documents may not be representative of industrial prob-
lems. The documents used in this study are smaller and less complex than
industrial specifications.
ffl The inspection process may not be representative of industrial software
development practice. Despite using a well known and widely used inspection
technique[8], many other inspection processes exist in industry
which pose a threat to the ability to generalise from this experiment.
2.2 Experiment Instrumentation
This experiment has re-used, except as described above, the material from the
original experiment, see Porter et al.[19] for a full description of the materials.
The following is an abridged description of the materials to enhance the reader's
appreciation of the experimental results.
Software Requirements Specifications
The specifications used in this experiment are: an automatic cruise control
system[12] (CRUISE) and a water level monitoring system[22] (WLMS). The
CRUISE specification is 31 pages long and the WLMS specification is 24 pages
long. Each specification has four sections: overview, specific function require-
ments, external interfaces, and a glossary. The overview is written in En-
glish, whereas the other sections are specified using a tabular requirements
All the faults in both documents are natural, with the CRUISE
specification believed to contain 26 technical faults and the WLMS specification
believed to contain 42 technical faults.
Fault detection methods
The two detection methods are designed to search for a well-defined population
of faults. The original experiment used a general fault taxonomy to
define the responsibilities of Ad Hoc reviewers and consequently directly derived
a set of Checklist responsibilities from the Ad Hoc responsibilities. The
individual Checklist questions were selected from several industrial checklists.


Appendix

A gives a complete description of the Checklist. The Scenarios were
then derived from the Checklist by replacing individual Checklist items with
procedures designed to implement them. The Scenario procedures have been
further grouped into three distinct sub-groups: data type inconsistencies, incorrect
functionalities, missing or ambiguous functionalities. Hence the Scenarios
search for an exact subset of the faults defined by the Checklist's responsibili-
ties. A post-experiment investigation by the original experimenters estimated
that the Scenarios include about half of the faults covered by the Checklist.


Appendix

contains a complete list of all the Scenarios.
3 Data and Analysis
Due to the alterations of the design of the experiment, the collected data cannot
be analysed using the same strategy as the original experiment. Having said
this, comparisons will be drawn whenever possible.
3.1 Verification of experimental validity

Table

3 shows the average defect group detection rates for both the original,
the first external replication 9 and this subsequent replication. This table shows
that the subjects in this experiment performed at least as well as the subject's
in the other experiments, and subsequently if we assume that the alterations in
the experimental procedure have not increased the defect detection rate, this
verifies these subjects suitability for this study. As stated earlier, it is believed
that this group defect detection rate represents a composite result of using
the detection method plus a collection meeting component. Hence rather than
9 Figures for this experiment are approximate, as Lanubile and Visaggio only give a graphical
representation of this result.
Checklist Scenario Checklist Scenario
Original 0.24 0.45 0.41 0.57
1st. Rep. 0.24 0.20 0.32 0.33
This Rep. 0.39 0.42 0.48 0.49

Table

3: Average proportion of defects found in the three experiments, at the
group level.
1. Do you think you understand Software Inspections?
(a) Completely
(b) Well
(c) Reasonably Well
(d) Not too sure
(e) Not at all
2. Do you think you understand the notation?
(a) Completely
(b) Well
(c) Reasonably Well
(d) Not too sure
(e) Not at all

Figure

1: Questions regarding validity of experiment
examining the above figures to analyse the treatment variable, the individual
defect detection rates will be considered. Neither of the other experiments
report these figures. This analysis is described in the following section.
The experiment has one further source of data for analysis: the debriefing
questionnaire. Each individual was asked two questions, one about their
understanding of the inspection process and the other about the semi-formal
specification language; Figure 1 details these questions and Table 4 summarises
the results. The results are sufficiently positive, to verify these aspects of the
experiment, i.e. the subjects have gained sufficient ability in these two areas to
limit the impact of the learning curve. Further questions with a direct impact
on assessing validity are discussed in future sections. It is believed that all these
questions yield a sufficiently positive picture of the experimental procedures as
to add further weight to the belief that no unknown external factor or factors
have made the experiment invalid.
3.2 Individual Inspection Performance
The following two figures (Figures 2 and
the two specifications) inspection performance given a detection methodology.
As can be seen, on average the Scenario technology out performs the Checklist

Table

4: Summary of answers to questions regarding validity
Normalised Defects
Number
of
Subjects
S. D.11

Figure

2: Histogram of average subject performance using Scenarios
technology, but taking a closer look (Table 5), despite a noticeable difference in
the means the experiment falls just short of providing a statistically significant
result. Although not directly comparable, it is worth noting at this point, the
treatment variable analysis from the other experiments: the original,
and the first replication, Hence it is reasonable to
conclude that the result from this experiment is more supportive of the original
experiment than the first replication. We will return to the question of whose
treatment variable results (at the group level) later in this paper.
The only other variable to provide a significant result in either of the experiments
was the specification variable. Unfortunately this experiment's specification
variable could also hide a maturation effect, but given the experiences
of the other two experiments and the fact that nothing has been altered which
might directly impact on this effect, it is believed that this is unlikely. The specification
variable shows a statistically significant effect, in line with the other
Normalised Defects
Number
of
Subjects
S. D.11
Figure

3: Histogram of average subject performance using Checklists
Detection Method
Scenario Checklist
Subjects 26 24
Mean 0.36 0.31
St. Dev. 0.11 0.11
St.
F Ratio 2.80
F Prob. 0.10

Table

5: Basic detection method analysis
two experiments (original first replication:

Table

6. What is also of interest, is that this replication experienced
an asymmetric effect with respect to detection method, again see Table
6, whereas the original experiment did not experience
any 11 .
These results suggest that a closer investigation is required and suggest that
the treatment variable should be investigated against each specification sepa-
rately. Table 7 shows this information. As can be seen, a large difference exists
between the relative performance of the detection aids across the two specifications
- with the first phase of the experiment (using the CRUISE specification)
Remember these figures include results from using the Ad hoc approach and hence are
not directly comparable.
11 The first replication does not quote any figure.
Both Detection Method
Methods Scenario Checklist
Subjects 50 26 24
Mean -0.05 -0.01 -0.08
St. Dev. 0.11 0.12 0.12
St.
F Ratio 7.81 4.85

Table

Specification effect: independent and dependent of detection method;
the negative means indicate a preference for the WLMS specification
Scenario Checklist Scenario Checklist
Subjects 26 24 26 24
Mean 0.35 0.27 0.37 0.35
St. Dev. 0.14 0.13 0.10 0.12
St.
F Prob. 0.03 0.63

Table

7: Detection method analysis by specification
showing a strong effect, while the second phase (using the WLMS specification)
shows an extremely weak effect. The same analysis can be conducted for the
original experiment, albeit at the group level. This analysis is shown in Table
8, and is derived from Table 3 (pp. 569) in the paper by Porter et al.[19] Again
we see a strong performance bias towards one specification, further analysis is
not possible given the reported data 12 . Hence further analysis was undertaken
to attempt to explain this effect more fully.
One possibility is an asymmetric change in the effort or commitment between
the two groups. Obviously it is impossible to measure these effects di-
rectly, and hence we must rely on the related measure of time spent when
looking for an explanation. Table 9 summarises the time spent on individual
inspections, it analyses the difference in time taken by each subject (CRUISE -
WLMS), grouped by detection method. Note three subjects using the Scenario
method failed to complete their time estimates, and are hence excluded from
this analysis. This table shows that a small asymmetric effect does exist, but it
is believed that it is too small to account for the effect. Taking the difference in
means (31.3) and comparing it with the average time to complete (403), shows
that the effect accounts for 7.5% of the average time spent on inspecting, and
hence by itself is rejected as having sufficient likely impact to explain all of the
difference.
12 The first experiment does not supply sufficient figures to allow a complementary analysis.
Scenario Checklist Scenario Checklist
Subjects
Mean 0.45 0.24 0.57 0.41
St. Dev. 0.07
St.
F Ratio 22.06 4.92
F Prob. !!0.01 0.06

Table

8: Detection method analysis by specification of the original experiment
at the group level
Scenario Checklist
Subjects
Mean 50.2 81.5
St. Dev. 152.6 162.7
St.
F Prob. 0.50

Table

9: Difference in time spent between inspecting each specification
Two further possible explanations were explored 1) that particular faults or
particular types of faults are easier to find using one technique or the other; and
differences in coverage, between the two detection aids. The original experiment
states that in effect due to differences in coverage have been minimised
by:
" . deriving the Scenarios from the Checklist by replacing individual
Checklist items with Scenario procedures designed to implement
them."
Unfortunately the experiment has only transposed a subset of the Checklist
items into Scenario procedures and hence the possibility of a coverage effect
cannot be ignored. For the CRUISE specification which has 26 potential defects,
the Checklist items cover 24 of the defects, whereas the Scenario procedures
only cover 14 (10 Data type inconsistencies, 1 Incorrect functionalities and 3
Missing or Ambiguous functionalities) defects; and for the WLMS specification
which has 42 potential defects, the Checklist items cover 38 of the defects,
whereas the Scenario procedures only cover 24 (14 Data type inconsistencies,
5 Incorrect functionalities and 5 Missing or Ambiguous functionalities) defects.
These facts could be considered to add extra weight to the claim that the
Scenario procedures are superior - increased performance, for lower coverage.
This is probably true given the decision to give the subjects as much time
as they required. It is more difficult to estimate the potential impact on the
original with its fixed time limit.
Mean
Difference
(Scenario
Checklist)Defect

Figure

4: Difference between technologies against individual defects in CRUISE-8Mean
Difference
(Scenario
Checklist)
Defect

Figure

5: Difference between technologies against individual defects in WLMS
The following figures (Figures 4 and 5) shows the difference in performance
between the two technologies (Scenario - Checklist) against the individual de-
fects. The first figure illustrates the behaviour in the CRUISE specification. As
can be seen, subjects using the Scenario technique perform better than Check-list
users for nearly every defect and by a relatively consistent amount, with
little difference between the areas explicitly covered by the Scenario procedures
and those which are not. Turning our attention to the second figure, detailing
the results from the WLMS specification, we see that this consistent picture
disappears, and that the Checklist users out-perform the Scenario users in 36%
of the defects. Here many of the items covered by the Checklist but not by
the Scenario procedures, show results in favour of the Checklist technique. But
again there is no consistency in the interaction between the detection aids and
the specifications.
A further possibility that exists is that these defects fall into a single type of
3. Did you use your technique (Scenario or Checklist)?
(a) Always
(b) Most of the time
(c) Sometimes
(d) Occasionally
Never
4. Do you understand your technique?
(a) Completely
(b) Well
(c) Sort of
(d) Not to sure
(e) Not at all
5. Do you think your technique was better or worse than the Ad Hoc approach
previously employed?
(a) Definitely Better
(b) Probably Better
(c) About the same
(d) Probably Worse

Figure

Questions regarding technique used.
defect. To investigate this, the defects were characterised using the taxonomy
developed in the original experiment, which is a composite of two schemes
developed by Schneider et al.[21] and Basili and Weiss[1]. Unfortunately this
classification failed to reveal any insights into the effects, as the defects are
spread across the taxonomy.
Finally, the questionnaires were investigated to address this issue. The
questionnaires contain three questions directly relevant to this point (Figure
6).
Question 3 attempts to find the rate of use of each technique; Question 4
how well each technique is understood by the relevant subjects; and Question 5
elicits the subject's opinion on each techniques performance against a common
control technique. Table 10 summarises the subjects responses. Analysing
table yields two relevant facts. Firstly the more regular use of the Scenario
technique, compared with the Checklist technique; in fact the latter might be
more correctly named Checklist plus Ad Hoc. It is impossible to estimate the
impact this would have on the previous calculations, but any impact should be
symmetrical with regard to the specifications. The second observation is the
drop in usage of the Scenario technique between inspecting the CRUISE and
WLMS specifications, while the Checklist technique does not experience any
decline in application. This could well lead to an asymmetric effect, especially
if the Scenario technique is superior to simply adopting an Ad Hoc approach
to inspection.
To summarise the asymmetric effect between the two specifications is difficult
to explain. A number of factors have been found each of which could have
Q. 3 Q. 4 Q. 5 Q. 3 Q. 4 Q. 5
Checklist C%

Table

10: Detection method analysis by questionnaire (Figure 6), all percentages
rounded to 5%.
Scenario Checklist Scenario Checklist
No. of groups 8 8 8 8
Mean 0.42 0.39 0.49 0.48
St. Dev. 0.08 0.09 0.07 0.11
St.
F Prob. 0.49 0.75

Table

11: Initial analysis of group performance
a small asymmetric impact on the experiment. The most likely explanation of
the asymmetric effect is a combination of these factors, namely:
ffl Asymmetric effort effect
ffl Differences in coverage
ffl Decline in application of the Scenario technique
ffl Natural variation within the experiment
ffl The performance of the detection aids is dependent on the nature of the
specification
3.3 Analysis of Group Meeting Data
Earlier it is stated that this experiment deviated from the original design because
of concern of multiple events obscuring the basic hypothesis. Table 11
investigates the original hypothesis at the group level. Comparing this table
with the same analysis at the individual level (Tables 5 and 7) clearly shows
Scenario Checklist Scenario Checklist
No. of groups 8 8 8 8
Mean 0.62 0.48 0.64 0.59
St. Dev. 0.09 0.11 0.08 0.09
St.
F Ratio 7.46 1.14
F Prob. 0.02 0.30

Table

12: Analysis of Potential maximum group results
Scenario Checklist Scenario Checklist
No. of groups 8 8 8 8
Mean -5.13 -2.38 -6.00 -4.75
St. Dev. 2.30 3.46 4.20 3.69
St.
F Prob. 0.29 0.60

Table

13: Analysis of the relationship between meeting effects and detection
aid. Figures are quoted as per defect, rather than as percentages, due to the
small numbers involved.
that the group component has had a large effect upon the analysis of the treatment
variable, especially with regard to analysing the results of the CRUISE
specification. Recalling the results for the original experiment (p ! 0.01) and
the first replication (p ! 0.92), the results at the group level are now more supportive
of the first replication rather than the original experiment. This change
of support is directly attributable to (some aspects of) the group inspection.
This can be clearly seen by comparing Table 11 with Table 12, which estimates
the group score by forming a union of the three individual scores. In Table 12,
the familiar pattern from the individual analysis re-establishes itself, if anything
the effect (pro-Scenario) is even stronger than at the individual level. So why
the change? It is believed that the meeting component is introducing an effect
unrelated to the detection aids, and this effect is demonstrated in the difference
between Tables 11 and 12. The difference can be characterised by two variables:
meeting loss and meeting gain. Meeting gain represents new defects found at
the meeting, whereas meeting loss represents those defects found during the individual
phase which are 'lost' during the meeting session. (Meeting loss occurs
for a number of reasons which will be discussed later in the paper.)

Table

13 explores this possible relationship further, directly exploring the
possibility of a relationship between the meeting component (meeting gain -
meeting loss) and the detection aid. It shows that no significant result exists
between these two concepts and the larger negative results experienced by the
Meeting Gain Meeting Loss Meeting Gain Meeting Loss
No. of groups
Total Defects 26 26 42 42
Mean 1.13 4.88 1.63 7.00
St. Dev. 0.81 2.80 1.20 3.27

Table

14: Meeting Gain and Loss by Specification
Score Maximum
Loss p 0.14 !!0.01
WLMS r -0.51 0.52
Loss

Table

15: Correlation (Pearson) between Meeting Loss, Group Score and Maximum
Possible Group Score
Scenario subjects is believed to be directly related to the fact that these subjects,
on average, found more defects than any indirect association with the detection
aid. This conjuncture is discussed more fully in the following section and Table
15 shows the correlation between the meeting effects and the number of defects
found. Examining Tables 11-13 and 15 shows that a strong case can be made
that the meeting component is independent of the detection aid used, and
hence this experiments choice to explore the treatment variable at the individual
level rather than at the group level, as conducted in the original, is the more
appropriate experimental design.
3.4 Analysis of Meeting Losses and Gains
Another outcome of the original experiment was the rejection of the meeting
component as a fault detection technique. The original experiment reports
that the number of meeting losses outweighs the number of meeting gains, this
viewpoint is also supported by the first replication. The results here are no
different, with meeting losses comfortably outweighing meeting gains. Given
the low resolution in these values, complex statistical evaluation is inappropri-
ate, for example the median value of the meeting gain is one defect for both
specifications, i.e. one unit of resolution. Table 14 gives a brief summary of
meeting gains and meeting losses for each specification. The figures in this table
broadly agree with the original and the first replication, that on average meeting
losses are greater than meeting gains, and are also in line with the results
reported by McCarthy et al.[14], who reported that inspection techniques which
rely more on the individual component were more productive than those which
relied more upon the meeting component. Further, with regard to the meeting
gain component, these results are in line with the results reported by Votta[10].
Votta reported on his experiences observing a series of professional inspection
meetings and found that the meeting component only contributed 4%, on av-
erage, to the number of defects found. This experiment produced an average
meeting gain of 9% from our novice inspectors, i.e. over twice the professional
rate. It is clearly highly unlikely that our subjects are able to perform at twice
the level of seasoned professionals. Much more likely, and certainly the view of
the authors, is that this increase is due to an experimental process, and shows
the danger of extrapolating from such small scale, low resolution data. The
meeting loss figures broadly follow the trend of the other experiments, but at
a more extreme level, i.e. this experiment has larger meeting loss values than
the other two experiments. So why the large meeting loss? Table 15 shows
that the loss is a systematic effect, and that the meeting loss correlates with
the meeting score, and correlates extremely strongly with the potential maximum
group score, i.e. the greater the diversity of the defects discovered by the
individuals in the group, the greater the number of defects lost. One aspect of
the experimental material which may be influencing this is the typographical
errors contained within the specification. The subjects were told these errors
were not defects and to discard them. The typographical errors don't cause
any problem with the correctness of the specifications, but they do contribute
an additional difficultly in understanding the document. Given that the experiment
uses novice inspectors, it is believed that many groups had some difficulty
distinguishing between defects and typographical errors, leading to some genuine
defects being discarded at the meeting. Unfortunately it is impossible to
measure the impact of the typographical errors on the meeting component, and
even on the individual component, but it is believed that the greatest potential
impact will be on the meeting loss estimation, especially given the lower sample
size at the group level.
This numerical concern of the effectiveness of the meeting component is
not shared by the subjects. The subjects were asked if they thought that the
meeting component was worthwhile, via the questions in Figure 7.
In general, their responses must be categorised as positive (Table 16). Looking
in more detail we can see that the responses to the second questionnaire
are significantly less positive than the first. That is, there exists a large negative
trend from the first exercise to the second exercise in the subjects opinions
about the effectiveness of the meeting component as a vehicle for defect detec-
tion. This trend, rather than the individual responses, is more in line with the
quantifiable data from the inspection meeting- unfortunately there is no way
of predicting if this trend would continue into subsequent phases of such an
experiment.
Conclusions
Software inspection are undoubtedly one of the best ways to verify software
documents. Having said this, the technique remains relatively unexplored, in
terms of which variations upon the basic theme work best. This paper explores
one such variation - detection aids, specifically it attempts to compare Scenarios
6. In considering the effective use of an inspector's time,
how would you rate the collection meeting against the individual component?
A. Far superior
B. Useful
C. Of Similar Worth
D. Of limited use
E. A waste of time
7. Which (you may answer yes to more than one) of the following objectives
were achieved by your collection meeting?
A. Defect Merging
B. Extra Defect Detection
C. Group Bonding/Team Spirit
D. Education of Weak group Members
E. Ensure all individuals adequately prepare their defect lists
F. Ensure common practices amongst individual inspectors

Figure

7: Questions regarding technique used.
and Checklists, to see which approach is better, if any.
Importantly this paper is not a one-off study, but is part of a large piece
of work involving several other researchers investigating the same hypothesis.
Multiple independent studies of the same hypothesis are essential if software
engineering is going to produce empirically evaluated theories and procedures.
The paper attempts to compare its results with the other studies whenever
possible.
With regard to the hypothesis of the experiment, the results are ambiguous,
but on balance are generally supportive of the results in the original experiment.
Hence, given the current weight of evidence, from the three experiments, there
seems to be emerging support for the conjuncture that the Scenario approach
is better than the Checklist approach. More work is required to finally confirm
this conjecture.
As normal when conducting an experiment several other effects, or more
accurately possible effects, were revealed. These additional effects include:
ffl a strong correlation between general academic ability and the ability to
successfully inspect software.
ffl an interaction between the detection aids and the specifications.
ffl that the subjects were more willing to use Scenarios compared with Check-list

ffl that the effectiveness of the meeting component is independent of the
detection aid used.
ffl that meeting losses outweigh meeting gains, suggesting the meeting component
is not an effective defect detection mechanism.
Q. 6 Q. 7 Q. 6 Q. 7
A%
B%
C%

Table

Analysis of Questions 6 and 7 (Figure 7)
ffl that meeting losses correlate strongly with various measures of the number
of defects found by a group.
These new potential effects also require further investigation to verify their
validity. Looking ahead and assuming that they exist, then the above results
suggested several new lines of research within the software inspection area, e.g.
inspection models with alternatives to the meeting component.
A Checklist Method
ffl General
- Are the goals of the system defined?
- Are the requirements clear and unambiguous?
- Is a functional overview of the system provided?
- Is an overview of the operational modes provided?
- Have the software and hardware environments been specified?
- If assumptions that affect implementation have been made, are they
stated?
- Have the requirements been stated in terms of input, output, and
processing for each function?
- Are all functions, devices, constraints traced to requirements and vice
versa?
- Are the required attributes, assumptions and constraints of the system
completely listed?
ffl Omission
Missing Functionality
* Are the described functions sufficient to meet the system objectives

* Are all inputs to a function sufficient to perform the required
* Are undesired events considered and their required responses specified

* Are the initial and special states considered (e.g. system initia-
tion, abnormal termination)?
Missing Performance
* Can the system be tested, demonstrated, analyzed, or inspected
to show that it satisfies the requirements?
* Have the data type, rate, units, accuracy, resolution, limits, range
and critical values for all internal data items been specified?
* Have the accuracy, precision, range, type, rate, units, frequency,
and volume of inputs and outputs been specified for each function

Missing Interface
* Are the inputs and outputs for all interfaces sufficient?
* Are the interface requirements between hardware, software, per-
sonnel, and procedures included?
Missing Environment
* Have the functionality of hardware or software interacting with
the system been properly specified?
ffl Comission
Ambiguous Information
* Are the individual requirements stated so that they are discrete,
unambiguous, and testable?
* Are the transitions specified deterministicly?
Inconsistent Information
* Are the requirements mutually consistent?
* Are the functional requirements consistent with the overview?
* Are the functional requirements consistent with the actual operating
environment?
- Incorrect or Extra Functionality
* Are all the described functions necessary to meet the system objectives

* Are all inputs to a function necessary to perform the required
* Are the inputs and outputs for all interfaces necessary?
* Are all the outputs produced by a function used by another function
or transferred across an external interface?
Wrong Section
* Are all the requirements, interfaces, constraints, etc. listed in the
appropriate sections.
B.1 Data Type Consistency Scenario
1. data object's mentioned in the overview (e.g. hardware com-
ponents, application variable, abbreviated term or function)
(a) Are all data objects mentioned in the overview listed in the external
interface section?
2. For each data object appearing in the external interface section determine
the following information:
ffl Object name:
ffl Class: (e.g. input port, output port, application variable, abbreviated
ffl Data type: (e.g. integer, time, boolean, enumeration)
ffl Acceptable values: Are there any constraints, ranges, limits for the
values of this object
ffl Failure value: Does the object have a special failure value?
ffl Units or rates
ffl Initial value:
(a) Is the object's specification consistent with its description in the
overview?
(b) If object represents a physical quantity, are its units properly specified

(c) If the object's value is computed, can that computation generate a
non-acceptable value?
3. For each functional requirement identify all data object references:
(a) Do all data object references obey formatting conventions?
(b) Are all data objects referenced in this requirement listed in the input
or output sections?
(c) Can any data object use be inconsistent with the data object's type,
acceptable values, failure values, etc.?
(d) Can any data object definition with the data object's type, acceptable
values, failure value, etc.?
B.2 Incorrect Functionality Scenario
1. For each functional requirement identify all input/output data objects:
(a) Are all values written to each output data object consistent with its
intended function?
(b) Identify at least one function that uses each output data object.
2. For each functional requirement identify all specified system events:
(a) Is the specification of these events consistent with their intended
3. Develop an invariant for each system mode (i.e. Under what conditions
must the system exit or remain in a given mode)?
(a) Can the system's initial conditions fail to satisfy the initial mode's
invariant?
(b) Identify a sequence of events that allows the system to enter a mode
without satisfying the mode's invariant.
(c) Identify a sequence of events that allows the system to enter a mode,
but never leave (deadlock).
B.3 Ambiguities Or Missing Functionality Scenario
1. Identify the required precision, response time, etc. for each functional
requirement.
(a) Are all required precisions indicated?
2. For each requirement, identify all monitored events.
(a) Does a sequence of events exist for which multiple output values can
be computed?
(b) Does a sequence of events exist for which no output value will be
3. For each system mode, identify all monitored events.
(a) Does a sequence of events exist for which transitions into two or more
modes is allowed?



--R

Evaluation of software requirements document by analysis of change data.
Comparing inspection strategies for software requirement specifications.
Statistical Power Analysis for the Behavioral Sciences.
Experience with Fagan's inspection method.
Software Inspection Process.
Design and code inspections to reduce errors in program development.
Advances in software inspection.
Software Inspection.
Specifying software requirements for complex systems.
Votta Jr.
An analysis of defect densities found during software inspections.
Example NRL/SCR software requirements for an automobile cruise control and monitoring system.
Assessing defect detection methods for software requirements inspections through external replication.
An experiment to assess cost-benefits of inspection meetings and their alternatives
Statistical power and its subcomponents - missing and misunderstood concepts in emprical software enginerring research
Active design reviews: Principles and practices.
An experiment to assess different defect detection methods for software requirements inspections.
Comparing detection methods for software requirements inspections: A replicated experiment.
Comparing detection methods for software requirements inspections: a replicated experiment.
Experience with inspection in ultralarge-scale develop- ments
An experimental study of fault detection in user requirements.

--TR

--CTR
Stefan Biffl , Bernd Freimut , Oliver Laitenberger, Investigating the cost-effectiveness of reinspections in software development, Proceedings of the 23rd International Conference on Software Engineering, p.155-164, May 12-19, 2001, Toronto, Ontario, Canada
J. Miller, On the independence of software inspectors, Journal of Systems and Software, v.60 n.1, p.5-10, 15 January 2002
Stefan Biffl , Wilfried Grossmann, Evaluating the accuracy of defect estimation models based on inspection data from two inspection cycles, Proceedings of the 23rd International Conference on Software Engineering, p.145-154, May 12-19, 2001, Toronto, Ontario, Canada
Bente Anda , Dag I. K. Sjberg, Towards an inspection technique for use case models, Proceedings of the 14th international conference on Software engineering and knowledge engineering, July 15-19, 2002, Ischia, Italy
Bjrn Regnell , Per Runeson , Thomas Thelin, Are the Perspectives Really Different?  FurtherExperimentation on Scenario-Based Reading of Requirements, Empirical Software Engineering, v.5 n.4, p.331-356, December 2000
Hidetake Uwano , Masahide Nakamura , Akito Monden , Ken-ichi Matsumoto, Analyzing individual performance of source code review using reviewers' eye movement, Proceedings of the 2006 symposium on Eye tracking research & applications, March 27-29, 2006, San Diego, California
Stefan Biffl, Using Inspection Data for Defect Estimation, IEEE Software, v.17 n.6, p.36-43, November 2000
Stefan Biffl , Michael Halling, Investigating the Defect Detection Effectiveness and Cost Benefit of Nominal Inspection Teams, IEEE Transactions on Software Engineering, v.29 n.5, p.385-397, May
Stefan Biffl , Walter J. Gutjahr, Using a Reliability Growth Model to Control Software Inspection, Empirical Software Engineering, v.7 n.3, p.257-284, September 2002
Oliver Laitenberger , Thomas Beil , Thilo Schwinn, An Industrial Case Study to Examine a Non-Traditional Inspection Implementation for Requirements Specifications, Empirical Software Engineering, v.7 n.4, p.345-374, December 2002
Susan S. Brilliant , John C. Knight, Empirical research in software engineering: a workshop, ACM SIGSOFT Software Engineering Notes, v.24 n.3, p.44-52, May 1999
F. MacDonald , J. Miller, A Comparison of Tool-Based and Paper-Based Software Inspection, Empirical Software Engineering, v.3 n.3, p.233-253, September 1998
James Miller , Fraser Macdonald , John Ferguson, ASSISTing Management Decisions in the Software Inspection Process, Information Technology and Management, v.3 n.1-2, p.67-83, January 2002
Thomas Thelin , Per Runeson , Claes Wohlin, An Experimental Comparison of Usage-Based and Checklist-Based Reading, IEEE Transactions on Software Engineering, v.29 n.8, p.687-704, August
Victor R. Basili , Forrest Shull , Filippo Lanubile, Building Knowledge through Families of Experiments, IEEE Transactions on Software Engineering, v.25 n.4, p.456-473, July 1999
Andreas Zendler, A Preliminary Software Engineering Theory as Investigated by Published Experiments, Empirical Software Engineering, v.6 n.2, p.161-180, June 2001
Oliver Laitenberger , Khaled El Emam , Thomas G. Harbich, An Internally Replicated Quasi-Experimental Comparison of Checklist and Perspective-Based Reading of Code Documents, IEEE Transactions on Software Engineering, v.27 n.5, p.387-421, May 2001
Oliver Laitenberger , Dieter Rombach, (Quasi-)experimental studies in industrial settings, Lecture notes on empirical software engineering, World Scientific Publishing Co., Inc., River Edge, NJ,

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'finite automata'
 'zero storage biometric authentication' 'parallel programming']
marked:['experiments', 'software inspection', 'partial replication', 'defect detection aids']
--T
A Function Point-Like Measure for Object-Oriented Software.
--A
We
present a method for estimating the size, and consequently effort
and duration, of object oriented software development projects.
Different estimates may be made in different phases of the development
process, according to the available information. We define an
adaptation of traditional function points, called Object Oriented
Function Points, to enable the measurement of object oriented
analysis and design specifications. Tools have been constructed
to automate the counting method. The novel aspect of our method
is its flexibility. An organization can experiment with different
counting policies, to find the most accurate predictors of size,
effort, etc. in its environment. The method and preliminary results
of its application in an industrial environment are presented
and discussed.
--B
Introduction
Cost and effort estimation is an important aspect of the management
of software development projects. Experience shows that accurate estimation
is difficult: an average error of 100% may be considered "good"
and an average error of 32% "outstanding" (Vicinanza, Mukhopadhyay
and Prietula 1991).
Most methods for estimating effort require an estimate of the size
of the software. Once a size estimate is available, models can be used
that relate size to effort.
Cost estimation is not a one-time activity at project initiation. Estimates
should be refined continually throughout a project (DeMarco
1982). Thus, it is necessary to estimate size repeatedly throughout
development.
2 Giuliano Antoniol et al.
Accurate estimation of size is vital. Unfortunately it has proved to
be very difficult, especially early in development when the estimates
are of most use.
Most research on estimating size has dealt with traditional applications
and traditional software development practices. Few methods
have been proposed for object oriented software development.
This paper presents a method for estimating the size of object oriented
software development projects. It is based on an adaptation of
the classical Function Point method to object oriented software.
In the following sections, we present our approach to measurement.
We explain how we map the concepts of function points to object oriented
software. We describe the process of counting Object Oriented
Function Points (OOFPs), and give an example. Results from a pilot
study are presented and discussed.
2. Measurement Perspective

Figure

1 shows the main phases of an object oriented (OO) development
process, and measurements that can be obtained at different points
in development. The purpose of these measures is to give the project
manager something from which to estimate the size, effort, and duration
of a project. These estimates can be revised as new artifacts become
available during development.
OO Analysis OO Design
System Requirements
Definition
Implementation
USER DESIGNER
CUSTOMER/

Figure

1. Perspectives and measures in the software development process.
As we move through the phases of the process, the measurement
perspective changes from that of the user to the designer.
At the end of the requirement specification phase, the classical Function
Point counting method is applied on the requirements doc-
ument. The function point method takes the perspective of the end
user. What is actually measured are the functions of the system that
main.tex; 23/06/1999; 14:27; no v.; p.2
A Function Point-like Measure for Object-Oriented Software 3
are visible to the end user. This measure is generally considered to be
independent of any particular implementation.
Some aspects of a system (e.g. a graphical user interface) are not
included in the classical function point count. Nevertheless, they contribute
to the final size of the system. If the objective of measuring
functionality is to estimate the size of an implementation of a sys-
tem, and from that the effort and duration of a software project, these
aspects should be taken into account. This changes the perspective
from that of the user to that of the customer i.e. the organization that
acquires the system, accepts it and pays the development costs; and
of the system designer, who has to produce an implementation of the
given specifications.
Once object oriented modelling begins (i.e. from the OO analysis
phase on), measurements can be obtained from the object models;
OOFP are used in place of FP. As development proceeds, this gives
a natural shift of perspective.
In the OO analysis phase, most of the elements in the object models
are still related to the application domain, so the perspective is still
that of the user.
At the OO design and later phases, the object models reflect implementation
choices. This includes aspects of the system that are not
specified in the requirements documents. The count of OOFP on these
models will thus include such functionalities. The measurement perspective
is now that of the designer.
Different size estimation models can be developed for different stages
of development. More detailed information is available, as the system
evolves from an abstract specification to a concrete implementation. It
should be possible to refine a size estimate repeatedly, as long as the
estimation process is not too difficult. Since we have developed tools to
automate the counting of OOFPs, re-calculation is easy at any time.
3. Object Oriented Function Points
Practitioners have found function points (Albrecht and Gaffney 1983,
IFPUG 1994) to be very useful within the data processing domain, for
which they were invented. We aim to exploit the experience that has
been obtained with function points in traditional software development,
in the OO paradigm. In adapting function points to OO, we need to
map function point concepts to object oriented concepts, and must
decide how to handle OO-specific concepts such as inheritance and
polymorphism.
4 Giuliano Antoniol et al.
Our presentation uses notations from the OMT method (Rumbaugh,
Blaha, Premerlani, Eddy and Lorensen 1991). It would not be much
different if the Booch notation (Booch 1991) or Unified Modeling Language
(Rational Software Corporation 1997b) was used instead, since
the main models and views in the different methodologies carry similar
information.
The OMT method uses three different, orthogonal views to describe
a software system:
Object Model: a static representation of the classes and objects in
a system, and their relationships.
Functional Model: data flow diagrams provide a functional decomposition
of the activities of the system.
Dynamic Model: state machines represent the dynamic and control
aspects of a system.
Although all three models provide important information about an
object-oriented system, the object model is the most important for
our purposes. It is usually the first to be developed, and so can be
measured earliest. It is the one that represents what is actually to
be built. In a sense the other models help in completing the object
model: the functional model helps in identifying and designing some
of the methods; the control model helps in identifying attributes that
are needed to maintain state information, and events that must be
implemented as classes or methods.
There is, however, an ongoing discussion in the practitioners community
on the content and role of those models. The functional model
seems to have fallen into disuse and is not required any more by some
methodologies (Rational Software Corporation 1997b). The dynamic
model is often replaced with a list of use cases and scenarios. The
object model is the only one that is present in all methodologies and
describes the system using specifically object-oriented concepts. For
these reasons, we decided to restrict our attention to object models.
In traditional developments, the central concepts used in counting
function points are logical files and transactions that operate on those
files. In OO systems, the core concept is no longer related to files or
data bases; instead the central concept is the "object".
The central analogy used to map function points to OO software
relates logical files and transactions to classes and their methods. A
logical file in the function point approach is a collection of related user-
identifiable data; a class in an object model encapsulates a collection of
data items. A class is the natural candidate for mapping logical files into
main.tex; 23/06/1999; 14:27; no v.; p.4
A Function Point-like Measure for Object-Oriented Software 5
the OO paradigm. Objects that are instances of a class in the OO world
correspond to records of a logical file in data processing applications.
In the FP method, logical files (LF) are divided into internal logical
files (ILFs) and external interface files (EIFs). Internal files are those
logical files that are maintained by the application; external files are
those referenced by the application but maintained by other applica-
tions. This division clearly identifies the application boundary. In the
OO counterpart, the application boundary is an imaginary line in an
object model, which divides the classes belonging to the application
from the classes outside the application. External classes encapsulate
non-system components, such as other applications, external services,
and reused library classes (both directly instantiated and subclassed
and parameterized classes). Classes within the application boundary
correspond to ILFs. Classes outside the application boundary correspond
to EIFs.
Transactions in the FP method are classified as inputs, outputs and
inquiries. This categorization is not easily applicable outside the data
processing domain.
In the OO paradigm the locus of operation are class methods, which
are usually at a more fine-grained level than transactions. Since object
models rarely contain the information needed to tell whether a method
performs an input, an output or is dealing with an enquiry, we do not
attempt to distinguish the three categories. We simply treat them as
generic Service Requests (SRs), issued by objects to other objects to
delegate to them some operations.
In short, we map logical files to classes, and transactions to methods.
Issues such as inheritance and polymorphism affect the structure
of the object model, and how the model should be counted. They are
addressed in Section 4.
3.1. Related Work
Other authors have proposed methods for adapting function points to
object oriented software. They too generally map classes to files, and
services or messages to transactions.
Whitmire (1993) considers each class as an internal file. Messages
sent across the system boundary are treated as transactions. Schoon-
eveldt, Hastings, Mocek and Fountain (1995) treat classes as files, and
consider services delivered by objects to clients as transactions. This
method gives a similar count to traditional function points for one
system. A draft proposal by the International Function Point Users
Group ("IFPUG") treats classes as files, and methods as transactions
1995).
6 Giuliano Antoniol et al.
Fetcke, Abran and Nguyen (1997) define rules for mapping a use
case model (Jacobson, Christerson, Jonsson and
Overgaard 1992) to
concepts from the IFPUG Counting Practices Manual (IFPUG 1994).
Three case studies have confirmed that the rules can be applied consis-
tently. No attempt has been made to relate the results to other metrics,
such as traditional function points, lines of code, or effort.
Sneed (1995) proposed object points as a measure of size for OO soft-
ware. Object points are derived from the class structures, the messages
and the processes or use cases, weighted by complexity adjustment factors

The closest analogue to our method is Predictive Object Points
(POPs), proposed by Minkiewicz (1997). POPs are based on counts
of classes and weighted methods per class, with adjustments for the
average depth of the inheritance tree and the average number of children
per class. Methods are weighted by considering their type (con-
structor, destructor, modifier, selector, iterator) and complexity (low,
average, high), giving a number of POPs in a way analogous to traditional
FPs. POPs have been incorporated into a commercial tool for
project estimation.
Our work differs from Minkiewicz in several ways. In two respects we
consider more information: we count the data in a class as well as the
methods; and we consider aggregation and inheritance in detail, instead
of as averages. We consider less information when counting methods,
since we do not distinguish between method types. Information about
method type is seldom available at the design stage. Automatic tools
would need to gather that information from the designer, which might
be a tedious task for very large systems. For that reason we do not
attempt to base our method complexity weighting on method type;
instead we try to exploit information about a method's signature, which
is most likely to be present in a design, at least at the detailed design
stage.
The key thing which is new about our method is its flexibility, with
much scope for experimentation. For example, Fetcke et al. (1997)
define that aggregation and inheritance should be handled in a particular
way. As discussed below in Section 4.1, we define several options
(one of which is Fetcke's approach) and leave it to the user to choose.
We have written programs to count OOFPs automatically, with several
parameters to govern counting decisions. An organization can experiment
to determine which parameter settings produce the most accurate
predictors of size in its environment. Thus we have a method which can
be tailored to different organizations or environments. Moreover, the
measurement is not affected by subjective ratings of complexity factors,
like those introduced in classical function point analysis.
A Function Point-like Measure for Object-Oriented Software 7
4. Measurement Process
OOFPs are assumed to be a function of the objects in a given object
model D. D might be produced at the design stage, or extracted from
the source code.
OOFPs can be calculated as:
where:
WELF (DET
A denotes the set of objects belonging to the application, and o is a
generic object in D. DETs, RETs and FTRs are elementary measures,
calculated on LFs and SRs and used to determine their complexity
through the complexity matrices W . Details are given in Sections 4.1-
4.3.

Figure

2 shows the phases of the OOFP computation process:
1. The object model is analyzed to identify the units that are to be
counted as logical files. There are four ways in which this might
be done; which to use is a parameter of the counting process. This
step is described in Section 4.1.
2. The complexity of each logical file and service request is deter-
mined. W tables are used to map counts of structural items (DETs,
RETs and FTRs) to complexity levels of low, average, or high.
These tables can be varied, and represent another parameter of the
counting process. This step is described in Sections 4.2 and 4.3.
3. The complexity values are translated to numbers, using another
table. These numbers are the OOFP values for the individual logical
files. The table used here can also be varied, and so is yet
another parameter of the counting process.
8 Giuliano Antoniol et al.
4. If a logical file is a class which is annotated as "reused" (ie developed
by reuse of another class), its OOFP value is multiplied by a
scale factor (1.0 or less). The scale factor is another parameter of
the counting process. This step is discussed in Section 4.4.
5. The OOFP values are summed to produce the total OOFP value.
Det
Ret
Det
Ftr
Det
Ret
OOFP
OOFP
OOFP
ILF
L,A,H
L,A,H
L,A,H
GB
Module
OO Design

Figure

2. OOFP computation process.
4.1. Identifying logical files
Conceptually, classes are mapped to logical files. It may not always be
appropriate to count each class simply as a single logical file, however.
Relationships between classes (aggregations and generalization / specializations
in particular) can sometimes make it appropriate to count
a group of classes together as a logical file.
Aggregation and inheritance relationships pertain mostly to implementation
aspects (internal organization, reuse). There tend to be few
of them in an analysis object model. There may be many of them in a
design or implementation model, as whole-part assemblies and inheritance
hierarchies are identified.
How these relationships affect the boundaries around logical files
depends on the perspective chosen, and the artifact on which the OOF-
Ps are computed.
At the analysis phase, the user's perspective is the important one. It
is too early to take the designer's point of view. At this stage, most of
the classes in an object model represent entities in the application and
user domain. There are few aggregation and inheritance relationships to
complicate things. Counting each single class as a logical file is usually
appropriate.
At this stage, the origin of a class does not matter. The scale factor
used in step 4 of the counting process shouild be set to 1.0, so the class
is counted with its full inherent value.
A Function Point-like Measure for Object-Oriented Software 9
At the design phase, the object models contain much more information
related to the implementation. From a designer's perspective,
considering each single class as a logical file will again be the correct
choice.
From a designer's or implementer's point of view, reuse makes classes
easier to develop. If the OOFP count is intended now to help predict
the effort or duration needed to build the system, a scale factor of less
than 1.0 should be used in step 4 of the counting process.
Counting a design object model from the user's perspective is more
complicated. To count what can actually be perceived by the user of
the system, the original abstractions present in the requirements and
analysis models have to be recovered. Implementation details should
not affect the count. There might no longer be a strict mapping of
single classes to logical files; collections of classes may sometimes need
to be counted together as a single logical file.
There may be many different ways to identify logical files. We consider
four, which are defined by different choices of how to deal with
aggregations and generalization / specialization relationships:
1. Single Class: count each separate class as a logical file, regardless
of its aggregation and inheritance relationships;
2. Aggregations: count an entire aggregation structure as a single
logical file, recursively joining lower level aggregations.
3. Generalization/Specialization: given an inheritance hierarchy,
consider as a different logical file the collection of classes comprised
in the entire path from the root superclass to each leaf subclass,
i.e. inheritance hierarchies are merged down to the leaves of the
hierarchy.
4. Mixed: combination of options 2 and 3.
Insert
Initialize
Delete
Top-of-pile
Bottom-of-pile
Location
Visibility
Collection Of Cards
Rank
Display
Discard
Card
Deck
Shuffle
Deal
Hand
Initial State
Draw Pile
Draw
Discard Pile
Draw

Figure

3. Single class ILFs.
Insert
Initialize
Delete
Top-of-pile
Bottom-of-pile
Location
Visibility
Collection Of Cards
Rank
Display
Discard
Card
Deck
Shuffle
Deal
Hand
Initial State
Draw Pile
Draw
Discard Pile
Draw

Figure

4. Aggregations ILFs.
Insert
Initialize
Delete
Top-of-pile
Bottom-of-pile
Location
Visibility
Collection Of Cards
Rank
Display
Discard
Card
Deck
Shuffle
Deal
Hand
Initial State
Draw Pile
Draw
Discard Pile
Draw

Figure

5. Generalization/Specialization ILFs.
For example, Figures 3-6 show the different counting boundaries that
result from these four strategies, on a sample object model 1 . Aggregation
merging decreases the number of classes in the object model
from 6 to 5 as CollectionOfCards is merged with Card; the resulting
logical file contains all the data members and methods of the two
classes. Generalization/specialization merging projects the superclass
CollectionOfCards onto its subclasses, again reducing the number of
logical files from 6 to 5. Finally, combining Aggregation and Gener-
alization/Specialization merging first aggregates CollectionOfCards
with Card and then projects the result onto the subclasses of
CollectionOfCards, resulting in 4 logical files.
Conceptually, it makes sense to merge superclasses into subclasses
for OOFP counting. It seems right to count the leaf classes, with their
full inherited structure, since this is how they are instantiated. (The
non-leaf classes of a hierarchy usually are not instantiated - they are
created for subsequent re-use by deriving subclasses, and for exploiting
polymorphism.) Also, two classes linked by a generalization / specialization
relationship are intuitively less complex than two separate
classes, because the subclass represents a refinement of the superclass.
1 The model is drawn from Rumbaugh et al. (1991).
A Function Point-like Measure for Object-Oriented Software 11
Insert
Initialize
Delete
Top-of-pile
Bottom-of-pile
Location
Visibility
Collection Of Cards
Rank
Display
Discard
Card
Hand
Initial State
Draw Pile
Draw
Discard Pile
Draw
Deck
Shuffle
Deal

Figure

6. Mixed ILFs.
Associations may present a problem. If non-leaf classes of an inheritance
hierarchy participate in associations, replicating the superclass
association into each subclass would increase artificially the number of
associations. In fact, the original superclass association contributes to
complexity only in the superclass, and in code it will only be implemented
once.
Merging aggregations into a single entity for OOFP counting seems
less intuitive. The objects that form aggregations are separate objects,
that exist independently of each other and have their own methods and
data members. At run-time, different objects will be instantiated for
each class in the aggregation.
However, it can be argued that dividing a user-identifiable class into
an aggregation of sub-classes is an implementation choice. From the
point of view of the end user, and of the function point measurement
philosophy, the OOFP value should not be affected. From this perspec-
tive, the aggregation structure should be merged into a single class and
counted as a single logical file.
Whether or not it is right to merge aggregations seems to depend on
whether the user's or designer's perspective is chosen. A hybrid solution
can be adopted, in which the treatment of aggregations is considered
as a parameter of the OOFP counting process. Three options can be
identified, with the choice left to the measurer:
1. merge aggregations;
2. do not merge aggregations;
3. flag on the design which aggregations should be considered as a
unique entity and so must be merged.
4.2. Logical Files
What is considered an ILF/EIF varies, according to the particular
ILF/EIF identification strategy used. Merging aggregations or general-
12 Giuliano Antoniol et al.
izations can generate ILFs or EIFs that correspond to sets of classes in
the design. We call these composite ILFs / EIFs, to distinguish them
from those consisting of a single class, called simple.
For each ILF/EIF it is necessary to compute the number of DETs
(Data Element Types) and RETs (Record Element Types). The rules
for DET/RET computation are slightly different for simple or composite
ILFs / EIFs.
In both cases, one RET is associated to each ILF/EIF, because it
represents a "user recognizable group of logically related data" (IFPUG
1994).
When the DETs and RETs of an ILF or EIF have been counted,
tables are used to classify the ILF/EIF as having low, average, or high
complexity. We base these tables on those given in the IFPUG Counting
Practices Manual Release 4.0 (IFPUG 1994).
4.2.1. Simple ILFs/EIFs
Simple attributes, such as integers and strings, are considered as DETs,
since they are a "unique user recognizable, non-recursive field of the ILF
or EIF" (IFPUG 1994).
A complex attribute in the OO paradigm is an attribute whose type
is a class (this models the analogy of a complex attribute with a RET,
i.e. "a user recognizable subgroup of data elements within an ILF or
EIF" (IFPUG 1994)) or a reference to another class.
Associations need to be counted as well, since they contribute to
the functionality/complexity of an object. An association is usually
implemented as a data member referencing the associated objects; this
reference is used in methods to invoke the associated object's services.
Associations are counted as DETs or RETs according to their car-
dinality. A single-valued association is considered as a DET (IFPUG
suggests counting a DET for each piece of data that exists because the
user requires a relationship with another ILF or EIF to be maintained
1994)). A multiple-valued association is considered as a RET,
because an entire group of references to objects is maintained in one
attribute.
Aggregations are a special case of associations. For simple ILFs /
EIFs, they are treated as normal associations.
4.2.2. Composite ILFs/EIFs
DETs and RETs are counted for each class within the composite, and
summed to give the overall total for the composite ILF/EIF.
DETs and RETs are counted using the same rules as for simple
ILFs / EIFs, except for aggregations. Aggregations are dealt with in a
special way because in a composite ILF/EIF they represent a subgroup.
A Function Point-like Measure for Object-Oriented Software 13
One RET is counted for each aggregation, whatever its cardinality. The
RET is assigned to the container class.
In practice, the values of DET and RET for any ILF/EIF are computed
by counting DETs and RETs for each component class on its
own (this is trivial for a simple ILF/EIF), and just adding them up 2 .
4.3. Service Requests
Each service request (method) in each class in the system is examined.
Abstract methods are not counted. Concrete methods are only counted
once (in the class in which they are declared), even if they are inherited
by several subclasses, because they are only coded once.
If a method is to be counted, the data types referenced in it are
classified:
simple items (analogous to DETs in traditional function points)
are simple data items referenced as arguments of the method, and
simple global variables referenced in the method;
items (analogous to File Types Referenced - FTRs -
in traditional function points) are complex arguments, objects or
complex global variables referenced by the method.
Several approaches are possible to distinguish complex items from
simple ones. For example, compiler built-in types might be considered
as simple and all other types as complex. This choice might not be
appropriate, since all user-defined types would be counted as com-
plex, even if they were scalar types or aliases of built-in types. Another
approach is to regard a complex item as one whose type is a class or a
reference to another class. This approach is used here.
When the DETs and FTRs of a method have been counted, tables
are used to classify the method as having low, average, or high com-
plexity. We base these tables on those given in the IFPUG Counting
Practices Manual Release 4.0 (IFPUG 1994) for external inputs and
queries.
Most of the time, the signature of the method provides the only
information on DETs and FTRs. Sometimes, especially early on, even
that is not known. In such a case, the method is assumed to have
average complexity.
2 The counting rules defined make DET-RET additive. The only exception is
the aggregation relation, which is handled differently in simple and composite ILFs.
However, in practice, the contribution of aggregation in composite ILFs corresponds
to considering one RET for each class involved in the aggregation structure, which
becomes equivalent to summing the RETs of each component class separately.
14 Giuliano Antoniol et al.
4.4. Allowing for reuse
In an early count, where the main aim is to capture user-oriented func-
tionality, the scale factor used in step 4 of the counting process should
be set to 1.0. A user doesn't care where a class comes from, so the class
should be counted with its full inherent value.
From a designer's or implementer's point of view, reuse makes classes
easier to develop. In a later count, in which the OOFP count may be
intended to help predict the effort or duration needed to build the
system, a scale factor of less than 1.0 would be appropriate.
4.5. An example

Figures

3-6 show four different ways that classes in an object model
might be merged, according to which of the four different LF identification
strategies is used. Here we show the OOFPs that are computed
for each variant.
Service Requests:
Service requests (methods) can be counted immediately. Since they
are only counted once anyway, it does not matter how the classes are
aggregated into logical files.
Because the signatures are unknown for the methods in the example,
each method is assumed to have average complexity. They each receive
the four OOFPs that are scored for an average service request.
As there are 12 concrete methods in the model, service requests
contribute 12 \Theta OOFPs.
Logical files:
The counting procedure for each individual class gives the DETs
and RETs shown in Figure 7.
The class Card has three DETs (two due to the two data items
and one due to the many-to-one association with CollectionOfCards)
and one RET (since the class itself is a collection of related data items).
CollectionOfCards has two DETs due to its two data items, one RET
due to the one-to-many aggregation with Card, and one RET for its
own structure. Each other class has one RET and as many DETs as it
has data items.
Depending on which ILF identification strategy is used, there are
four different ILF variants. Each variant merges classes together in
different ways, resulting in different total DET and RET counts. Table I
shows the result of applying IFPUG 4.0 complexity tables with each
variant. The value Low is rated as 7 OOFP, according to the IFPUG
tables.
A Function Point-like Measure for Object-Oriented Software 15
Insert
Initialize
Delete
Top-of-pile
Bottom-of-pile
Location
Visibility
Collection Of Cards
Rank
Display
Discard
Card
Deck
Shuffle
Deal
Hand
Initial State
Draw Pile
Draw
Discard Pile
Draw

Figure

7. DET/RET computation for LFs on the example system.

Table

I. ILF and SR complexity contribution (S
Collection of Cards Low Low -
Card Low - Low -
Deck Low Low Low Low
Hand Low Low Low Low
Discard Pile Low Low Low Low
Draw Pile Low Low Low Low
28
SR OOFP 48 48 48 48
Total OOFP 90 83 83 76
The highest OOFP count comes when each class is counted as a
single ILF. All the other variants have the effect of reducing the OOFP
value, as they reduce the number of ILFs. Although there is an increase
in DETs / RETs in the merged ILFs, it is not enough to raise the ILF
complexity to higher values.
5. Tools for Counting OOFPs
The process for computing OOFPs has been automated, as shown in

Figure

8. Object models produced with CASE tools are translated to an
intermediate representation. The intermediate representation is parsed,
Giuliano Antoniol et al.
OOFP
Translator
Tool-AOL
Other Tool
Tool Output
AST
AOL
Parser
AOL
Specification
OOFP
Counting Rules
OOFP
Counter
OMT/STP Rational ROSE
STP Output Petal Output
OMT/STP-AOL
Translator Translator
Petal-AOL

Figure

8. OOFP Computation Process
producing an Abstract Syntax Tree (AST), to which the OOFP counting
process is applied.
In order to be independent of the specific CASE tool used, an intermediate
language, called Abstract Object Language (AOL), has been
devised. The language is a general-purpose design description language,
capable of expressing all concepts available at the design stage of object
oriented software development. This language is based on the Unified
Modeling Language (Rational Software Corporation 1997b), a superset
of the OMT notation that is becoming the standard in object oriented
design. Since UML is a visual description language with some limited
textual specifications, we had to design from scratch many parts of
the language, while remaining adherent to UML where textual specifications
where available. Figure 9 shows an excerpt from the AOL
description of the object model depicted in Figure 7.
The output of the specific CASE tool used is translated automatically
into an equivalent AOL specification. One translator has been imple-
mented, to convert the output from OMT/STP (Interactive Development
Environments 1996) to an AOL specification. Other translators
main.tex; 23/06/1999; 14:27; no v.; p.16
A Function Point-like Measure for Object-Oriented Software 17
class Deck
operations
class Hand
attributes
operations
aggregation
container class CollectionOfCards mult one
parts class Card mult many;
generalization CollectionOfCards
subclasses Deck, Hand, DiscardPile, DrawPile

Figure

9. Excerpt of the AOL specification for the example object model.
could be implemented for other CASE tools, such as Rational Rose
(Rational Software Corporation 1997a) which fully supports UML and
represents its output using a language called Petal.
The AOL specification is then parsed by an AOL parser, producing
an AST representing the object model. The parser also resolves references
to identifiers, and performs some simple consistency checking
(e.g. names referenced in associations have been defined).
The OOFP Counter implements the OOFP Counting Rules described
in Section 4. The OOFP Counter is very different from other measurement
and counting tools, because instead of assuming a specific
counting strategy it allows one of several strategies to be chosen. This
makes it suitable for experimentation. The tool is very flexible, being
parameterizable with respect to the rules used in the counting process.
The AOL parser and the OOFP counter have been implemented in
both Refine (Reasoning Systems 1990) and Lex/Yacc.
6. Pilot Study
The described methodology has been applied in an industrial environment
producing software for telecommunications. Our first study is of
the relationship between the OOFP measure of a system and its final
main.tex; 23/06/1999; 14:27; no v.; p.17
Giuliano Antoniol et al.
size in lines of code (LOC), measured as the number of non-blank lines,
including comments.
Eight sub-systems of a completed application were measured. These
eight systems were chosen for study because all were developed by
the same poeple, in the same environment, using the same language
(C++). Design documents and the final source code were both avail-
able. Measurements of design characteristics were taken from the design
documents, not "reverse engineered" from the source code.

Table

II shows the numbers of various design elements in each system

Table

III shows the size of each system, spreading from about
5,000 to 50,000 lines of code. Table III also shows the OOFP count for
each system, using each of the four different strategies for identifying
logical files.

Table

II. Design Characteristics (Atr= Attributes,
Aggregation,
es).
System Atr Ope Ass Agg Inh Cls
The four OOFP series are strongly correlated with each other. The
lowest Pearson correlation, between the Single Class (S) and Mixed
(M) strategies, is .992. Other correlations range up to .998. As shown
in

table

III, differences between the methods become appreciable only
for the projects with large LOC values.
The high correlation between the four OOFP series suggests that
they are essentially linear transformations of each other. In that case,
changing the strategy for identifying logical files might not make much
difference to the accuracy of size estimation models.
6.1. Model Evaluation
A leave-one-out cross-validation procedure (Stone 1974) was used to
measure model performance. Each model was trained on
A Function Point-like Measure for Object-Oriented Software 19

table

III. System sizes and OOFPs.
System LOC S A G M
A 5807 63 63
D 19863 1071 1057 1057 1043
F 31011 518 403 483 368
G 47057 1142 1100 1124 1072
of the data set L (sample size is currently tested
on the withheld datum. The step was repeated for each point in L and
accuracy measures averaged over n. This method gives an unbiased
estimate of future performance on new data, and enables quite different
models to be compared directly.
Model error was estimated as the cross-validation version of the normalized
mean squared error (NMSE). This is the mean squared error,
normalized over the variance of the sample:
Regression based on least square minimization assumes that the
distribution of errors is Gaussian. Statistical tests for skewness and
kurtosis do not cause the hypothesis of normality to be rejected for
any of the five distributions (LOC and the four OOFP values). But
given the small size of our data set, it is not clear that the distributions
really are normal.
The least squares approach is sensitive to outliers (data points far
removed from other data points) in the data, because it minimizes
squared deviations. An outlier can have almost as much influence on
the regression results as all other points combined. Standard box-and-
whisker plots do not identify any of our systems as outliers, although
system H is right on the edge of being considered an outlier.
The impact of such influential points can be lessened by reducing
the weight given to large residuals - for example, by minimizing the
sum of absolute residuals rather than the sum of squared residuals.
Thus another measure for errors, based on absolute values, was also
20 Giuliano Antoniol et al.
considered to check inconsistencies due to possible influential or outlier
points. This measure is normalised mean absolute error (NMAE):
jy k \Gammamed y j
where  are the mean and median
of the observed values in the sample L. Where available, the cross-validation
estimates of the standard error oe of the residuals y
and of the r-squared R 2 of the fit were also computed.
Even with cross-validation, care is needed in interpreting the results.
The small sample means that any observations must be regarded as
indicative rather than conclusive.
6.2. Models considered
Several regression techniques were considered to model the relationships
of LOC with OOFP. Other predictors of LOC, based on direct
indicators of OO size such as the number of classes or methods in the
design, were considered for comparison.
First, linear models (lms in table IV) based on minimizing the sum
of squares of the residuals were developed for each LF selection method.
Least absolute deviation, based on L 1 error, was also applied (l1s
in

table

IV). This method minimizes the sum of the absolute values of
the residuals, to reduce the effect of large error values.
Robust regression techniques were also investigated, to handle non-obvious
outliers. A family of M-estimators (see the Appendix) was considered
(rregs and rlms in table IV). The basic idea of M-smoothers
is to control the influence of outliers by the use of a non-quadratic
local loss function which gives less weight to "extreme" observations.
Examples of smoothers are Andrews, bisquare, fair, Hampel, Huber,
and logistic (Venables and Ripley 1994). Each corresponds to a different
weight function.
Finally, multivariate linear models were developed that predict LOC
directly from the numbers of OO design elements shown in table II.
6.3. Results

table

IV shows each of the models, parameterized over LF selection
methods and the type of regressor. The model coefficients b 0 and b 1
indicated were computed from the full data set. The estimated model
errors (NMSE and NMAE) are shown for each model. The estimated
R-squared measure is also included for the linear models.
A point of concern is whether an intercept term b 0 should be included
in the model. It is reasonable to suppose the existence of support code
main.tex; 23/06/1999; 14:27; no v.; p.20
A Function Point-like Measure for Object-Oriented Software 21

table

IV. Model performance for linear regressors
(lms and l1s) and robust methods (rregs and rlms).
Method
lm-A 0.43 0.66 0.69 8505 23.8
rreg-A 0.43 0.66 - 8255 24.0
not directly related to the functionalities being counted; and prediction
is improved with the term. However, the intercept term is not significant
in a non-predictive fit of the data. More importantly, the fact that the
intercept term is always larger than our smallest system might indicate
a poor fit for small OOFP values. It would be interesting to apply a
Bayesian procedure to select the intercept from given priors.
The results summarized in table IV are encouraging. For example,
the lm-G model has an NMSE of 38 %, meaning that the square error
variance is less than half of the sample variance. From another point
of view, models based on the OOFPs counted using the Generalization
strategy achieve a cross validation average error of 47 %, which is very
good.
The best model in table IV is rreg-G. Further investigation of
rreg-G was done, with the results shown in table V.
The best predictive accuracy (NMSE=0.337) was achieved by the
rreg-logistic-G model with tuning parameter This corresponds
main.tex; 23/06/1999; 14:27; no v.; p.21
22 Giuliano Antoniol et al.

table

V. Model performances for different weighting functions
of the M-estimator rreg, for the Generalization selection
method.
Method NMSE Comments
rreg-fair-G 0.48 converged after 50 steps)
to the linear predictor (This model is
very close to the basic linear model lm-G, whose equation is

table

IV suggests that in this data set the Generalization strategy
is consistently best. This is not proven statistically, though. A non-parametric
bootstrap approach (Efron and Tibshirani 1993) was used
to assess the models. The null hypothesis that there are no differences
between the errors from the lm-S and lm-G models cannot be rejected;
similar results were obtained for the other models. Thus it is not clear
that any counting strategy should be preferred over any other.
For comparison, multivariate linear models were developed that predict
LOC directly from the numbers of OO design elements (shown in

table

II). Poorer results are obtained from such models. For example, a
model based on classes and methods has
Models based on OOFP perform much better.
This pilot study was conducted in a specific project and environ-
ment, in a specific organization. The results are encouraging for size
estimation in this context. The issue of external validity must be addressed
by more extensive studies, targeting multiple organizations and different
projects. We have taken the first step in empirical investigation;
more needs to be done.
A Function Point-like Measure for Object-Oriented Software 23
7. Discussion of Results
As can be seen in table I, the complexity of each LF is always determined
to be low, even when several classes are merged together. The
same is true for service requests. The tables used to determine complexity
are based on those from the IFPUG Counting Practices Manual
(IFPUG 1994), in which quite large numbers of RETs and DETs are
needed to reach average or high complexity (for example, to obtain an
average complexity weight an LF needs a DET value between 20 and 50
and a RET value between 2 and 5). This is due to the data processing
origins of the function points method, and doesn't seem to apply as
well to all kinds of systems. Therefore the complexity tables should be
recalibrated, to provide more discrimination.
The implicit assumption in the use of these tables is that the complexity
of a class, and hence the size of its implementation and the effort
required to implement it, increases as the number and complexity of its
attributes increases. Similarly, the complexity of a method (and hence
its size and development effort) is assumed implicitly to increase as the
number and complexity of its parameters increases 3 . Whether these
assumptions are true needs to be determined experimentally.
The assumption seems reasonable for classes as a whole, but perhaps
not for methods. What works for transactions in traditional function
points may not work for methods in an object model, because transactions
tend to be much more coarse grained than methods.
At the analysis and design stages, we often have no more information
about a method than its signature. If it turns out that this is unrelated
to complexity and size, we have nothing to go by in counting OOFPs.
One possibility would be to permit the designer to annotate a method
with a complexity rating. This would introduce a subjective element
to the process, however, which we want to avoid. Another approach
would be simply to count the methods, making no attempt to classify
them by complexity. A promising approach would take advantage of the
information available in use cases and scenarios to derive a complexity
rating for methods.
On the data available to us so far, it seems that recalibration of the
OOFP tables for logical files might improve the accuracy of OOFP as
a predictor of size; recalibration of the table for methods might not.
Further experimentation is needed on this topic, with data from more
systems. In order to support such experimentation, the tool used to
3 These assumptions are fairly common. They underlie the philosophy of the
classical function point method. They also feature in the design metrics work of
Card and Glass (1990).
Giuliano Antoniol et al.
count OOFPs is designed to consider the table entries as parameters
that can be modified at any time.
The pilot study suggests that for this organization there is no reason
to prefer any of the four strategies for indentifying LFs over any other.
Other organizations may find differently. Although for this organization
the best size predictions appear to be obtained with the Generalization
strategy, its superiority is not proven statistically and may be an accident
of the data.
Modifying the complexity tables might make a difference in determining
the best strategy for selecting LFs.
Once a counting scheme has been chosen, it is important that it
be applied consistently. Consistent counting is straightforward for us,
since tools are used to automate the process.
8. Conclusions
We have presented a method for estimating the size of object oriented
software. The method is based on an adaptation of function points, to
apply them to object models. The proposed method takes full advantage
of the information contained in the object model and eliminates
the ambiguities of the traditional function points method. It can be
parameterized in order to take into account more closely the characteristics
of a specific design environment or of a particular problem.
We have defined the mapping from FP concepts to OO concepts,
and described the counting process. Tools have been developed that
automate the process. Preliminary results from a pilot study in an
industrial environment have been reported. The results from the pilot
study show promise for size estimation. This is important, since an
estimate of size is needed for many effort estimation models.
In summary, we have shown that we can apply the concepts of function
points to object oriented software and that the results are accurate
and useful in an industrial environment.
Future work will take several directions. One is to investigate the
effect of recalibrating the complexity tables. Other relationships, beyond
just OOFPs and code size, will be studied; those between OOFPs and
traditional FPs, and OOFPs versus effort, are of particular interest.
Another avenue is to consider the impact of using design patterns
(Gamma, Helm, Johnson and Vlissides 1995) on the structure within
object models; this may lead to other strategies for identifying logical
ILFs.
A Function Point-like Measure for Object-Oriented Software 25

Acknowledgements

We thank the referees for their constructive comments. This research
was funded by SODALIA Spa, Trento, Italy under Contract n. 346
between SODALIA and Istituto Trentino di Cultura, Trento, Italy.
When this work was undertaken, G. Caldiera and C. Lokan were with
the Experimental Software Engineering Group at the University of
Maryland, and G. Antoniol was with the ITC-IRST, Istituto per la
Ricerca Scientifica e Tecnologica, I-38050 Povo (Trento), Italy.



--R

Software function

Measuring Software Design Quality
Controlling Software Projects
An Introduction to the Bootstrap
Mapping the OO-Jacobson approach to function point analysis
Design Patterns: Elements of Reusable Object Oriented Software
Applied Nonparametric Regression


Software Through Pictures Manuals.

Estimating size for object-oriented software
Measuring object-oriented software with predictive object points
Unified Modeling Language
Reasoning Systems:

Measuring the size of object-oriented systems
Estimating the costs of object-oriented software

Modern Applied Statistics with S-Plus

Applying function points to object oriented software
--TR

--CTR
Maurizio Morisio , Daniele Romano , Ioannis Stamelos, Quality, Productivity, and Learning in Framework-Based Development: An Exploratory Case Study, IEEE Transactions on Software Engineering, v.28 n.9, p.876-888, September 2002
Marjan Heriko , Ivan Rozman , Ale ivkovi, A formal representation of functional size measurement methods, Journal of Systems and Software, v.79 n.9, p.1341-1358, September 2006
Gennaro Costagliola , Filomena Ferrucci , Genoveffa Tortora , Giuliana Vitiello, Class Point: An Approach for the Size Estimation of Object-Oriented Systems, IEEE Transactions on Software Engineering, v.31 n.1, p.52-74, January 2005
G. Antoniol , R. Fiutem , C. Lokan, Object-Oriented Function Points: An Empirical Validation, Empirical Software Engineering, v.8 n.3, p.225-254, September

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-based testing'
 'zero storage biometric authentication' 'program verification']
marked:['function points', 'object oriented', 'design metrics', 'size estimation']
--T
Efficient Window Block Retrieval in Quadtree-Based Spatial Databases.
--A
An algorithm is presented to answer window queries in a quadtree-based spatial database environment by retrieving all of the quadtree blocks in the underlying spatial database that cover the quadtree blocks that comprise the window. It works by decomposing the window operation into sub-operations over smaller window partitions. These partitions are the quadtree blocks corresponding to the window. Although a block b in the underlying spatial database may cover several of the smaller window partitions, b<math> is only retrieved once rather than multiple times. This is achieved by using an auxiliary main memory data structure called the active border which requires <math>O\left(n\right)<math> additional storage for a window query of size <math>n\times n<math>. As a result, the algorithm generates an optimal number of disk I/O requests to answer a window query (i.e., one request per covering quadtree block). A proof of correctness and an analysis of the algorithms
execution time and space requirements are given, as are some experimental results.
--B
Introduction
Spatial data consists of spatial objects made up of points, lines, regions, rectangles, sur-
faces, volumes, and even data of higher dimension which includes time. Examples of
spatial data range from locations of cities, rivers, roads, to the areas that are spanned
by counties, states, crop coverages, mountain ranges, etc. They are increasingly finding
their way into adaptations of conventional databases for use in applications in geographic
information systems (GIS), resource management, space, urban planning, etc. [9, 22].
There are many different representations of spatial data (see [20, 21] for an overview).
We are interested in representations that are based on spatial occupancy. Spatial occupancy
methods decompose the space from which the data is drawn (e.g., the two-dimensional
space containing the lines) into regions called buckets. They are also commonly
known as bucketing methods. Traditionally, bucketing methods such as the grid
file [19], BANG file [13], LSD trees [17], Buddy trees [25], etc. have usually been applied
to points, although they can be applied to the other types as well.
There are four principal approaches to decomposing the space from which the data is
drawn. One approach buckets the data based on the concept of a minimum bounding (or
enclosing) rectangle. In this case, the minimum bounding rectangles of the objects are
grouped (hopefully by proximity) into hierarchies, and then stored in another structure
such as a B-tree [7]. The R-tree [16] (as well as its variants such as the R   -tree [6]) is an
example of this approach.
The drawback of these hierarchies of objects is that they do not result in a disjoint
decomposition of the underlying space. The problem is that each object is only associated
with one bounding rectangle even though it may also overlap a portion of the bounding
rectangle of another object. In the worst case, this means that when we wish to determine
which object is associated with a particular point in the two-dimensional space from
which the objects are drawn (e.g., the containing rectangle in a rectangle database, or an
intersecting line in a line segment database), we may have to search the entire database.
The other approaches are based on a decomposition of space into disjoint cells, which
are mapped into buckets. Their common property is that the objects are decomposed
into disjoint subobjects such that each of the subobjects is associated with a different cell.
They differ in the degree of regularity imposed by their underlying decomposition rules
and by the way in which the cells are aggregated. The price paid for the disjointness is
that in order to determine the area covered by a particular object, we have to retrieve
all the cells that it occupies. Moreover, if we wish to report all the objects that overlap
a particular area, then we may have to report an object as many times as its subobjects
appear in the area.
The first method based on disjointness partitions the objects into arbitrary disjoint
subobjects and then groups the subobjects in another structure such as a B-tree. The
partition and the subsequent groupings are such that the bounding rectangles are disjoint
at each level of the structure. The R + -tree [26] and the cell tree [15] are examples of this
approach. Their drawback (as well as the R-tree variants) is that the decomposition is
data-dependent. This means that it is difficult to perform tasks that require composition
of different operations and data sets (e.g., set-theoretic operations such as overlay).
In contrast, the remaining two methods, while also yielding a disjoint decomposition,
have a greater degree of data-independence. They are based on a regular decomposition.
The space can be decomposed either into blocks of uniform size (e.g., the uniform grid [12])
or adapt the decomposition to the distribution of the data (e.g., a quadtree-based approach
that makes use of regular decomposition such as [24]). In the former case, all the blocks
are of the same size, while in the latter case, the widths of the blocks are maximal subject
to being restricted to be powers of two, and a restriction on their positions.
For example, Figure 1 shows the quadtree block decomposition of two square regions
of space each of which contains a rectangular subregion (termed a window) delimited by
heavy lines. The blocks are obtained by applying regular decomposition to the square
regions thereby repeatedly breaking them up into four congruent blocks until each block
is either completely within the window or completely outside the window.
window w window w
underlying spatial database underlying spatial database

Figure

1: The decomposition of (a) a 12 \Theta 12 window, and (b) a 13 \Theta 13
window into maximal quadtree blocks.
The uniform grid is ideal for uniformly distributed data, while quadtree-based approaches
are suited for arbitrarily distributed data. In the case of uniformly distributed
data, quadtree-based approaches degenerate to a uniform grid, albeit they have a higher
overhead. Both the uniform grid and the quadtree-based approaches lend themselves to
set-theoretic operations as the positions of the decomposition lines are restricted and thus
there is much less variation between the operands of the operations. Thus they are ideal
for tasks which require the composition of different operations and data sets. In gen-
eral, since spatial data is not usually uniformly distributed, the quadtree-based regular
decomposition approach is more flexible.
A window query is the spatial analog of a range query in that it retrieves all objects
that overlap the space covered by a range of x and y (and possibly z in three-dimensions)
coordinate values which form the window. In this paper we focus on performing a variant
of a window query using a regular decomposition quadtree. Both the window and the
underlying database are represented by a quadtree. In particular, the quadtree blocks
BW that make up the window are used to guide the retrieval process. The variant of the
query is one that retrieves all of the quadtree blocks BU of the underlying database that
cover the blocks that make up the window (i.e., BW ). This query differs from the classical
window operation described above which retrieves the objects in the underlying database
that cover the window instead of the blocks in the underlying database as we do here.
The rationale for using the quadtree blocks of the window is to match the quadtree
decomposition of the underlying spatial database. This makes it more straight-forward
to answer the window query since there is a direct correspondence between each window
block and some overlapping quadtree block(s) in the underlying spatial database. The
answer to the window query is the union of all the answers generated by querying the
underlying spatial database with the maximal quadtree blocks comprising the window
serving as the individual queries.
Our variant can be viewed as a preliminary step to the retrieval of the objects in that it
retrieves the blocks in the underlying database (i.e., BU ) that correspond to the window.
The next step would process blocks BU and extract the relevant objects from them. When
the underlying database is a quadtree where the objects have been decomposed so that the
blocks which contain them are disjoint, the step that extracts the relevant objects from
the blocks may in fact encounter some of the objects more than once (e.g., when a region
or line object has been decomposed into several blocks each of which contains a part of
the region or line object). In this case, this step would have to eliminate the duplicates
which is not a simple matter (but see [2, 4]).
The rationale for our variant is that we may wish to use these blocks (i.e., BU ) as
input to a subsequent operation whose underlying representation is also a quadtree thereby
facilitating the composition of several operations. Another way to characterize our variant
is that it is somewhat like a clipping operation where we are using the quadtree blocks
that make up the query window (i.e., BW ) to clip the blocks that make up the underlying
database (i.e, BU ).
In this paper we show how to retrieve the quadtree blocks from the underlying database
that cover the quadtree blocks that comprise the window. In particular, we describe a
method that retrieves each block b in BU just once even though b may cover several blocks
in BW . The rest of this paper is organized as follows. Section 2 gives an overview of
our approach. Section 3 describes our algorithm. Section 4 contains an informal proof of
correctness for the algorithm's block retrieval process, while an analysis of its worst-case
execution time and space complexity is given in Section 5. Section 6 presents empirical
results of the disk I/O behavior of the algorithm, while concluding remarks are drawn in
Section 7.
2 Overview of our Approach
A window decomposition algorithm is given in [3] which decomposes a two-dimensional
window of size n \Theta n in a feature space (e.g., an image) of size T \Theta T into its maximal
quadtree blocks in O(n log log T ) time. Once the set BW has been determined, we simply
retrieve the elements of the underlying spatial database S that overlap each of its elements.
The drawback of this algorithm is that many of the elements of S may be retrieved more
than once. For example, in Figure 2, the algorithm would retrieve block p of the underlying
spatial database four times (once for each of the maximal window blocks 1, 4, 8, and 10).
We assume that the underlying spatial database is disk-resident, and we often speak of the
operation of retrieving a block of the underlying spatial database as a disk I/O request.
This means that redundant disk I/O requests will result. 2 One solution is to keep track
of all blocks that have already been retrieved. This is not easy without additional storage
(see [2] for a discussion of the similar issue of uniquely reporting answers in a spatial
database).
s
r
Underlying
spatial
database
window w

Figure

2: Examples where more than one window block retrieves the same
block of the underlying spatial database.
The problem with using the algorithm in [3] is that the process of generating the
maximal blocks that comprise the window only depends on the query window and does
not take into consideration the decomposition of space induced by the underlying spatial
database. We overcome this problem by generating and retrieving each covering block
in the underlying spatial database just once. This is achieved by controlling the window
decomposition procedure through the use of information about blocks of the underlying
spatial database that have already been retrieved. We use an approach based on active
borders [23], at the expense of some extra storage. The algorithm that we present performs
this task with the same worst-case CPU execution-time complexity as the one in [3] (i.e.,
O(n log log T )). The difference is in the I/O cost where the new algorithm makes just M
requests to access the underlying spatial database instead of max(N; M) as in [3], where
M is the number of quadtree blocks in the underlying spatial database that overlap the
This problem can be overcome via appropriate use of buffering techniques. However, in this paper
we show how to avoid the problem by retrieving each block of the underlying spatial database just once
without relying on buffering techniques.
window and N is the number of maximal quadtree blocks in the window. A general significance
of both our algorithm and the one in [3] is that although the window contains n 2
pixel elements, the worst-case CPU execution-time complexity of the algorithms is almost
linearly proportional (and not quadratic) to the window diameter, and is independent of
other factors.
It is important to note that we retrieve blocks in the underlying spatial database by use
of information (partial) about their relationship to other blocks (e.g., containment, overlap,
subset, etc. We do not retrieve a block of the underlying database by its identifier. If we
could do this, then we could keep track of which blocks are retrieved via a hash table, for
example, and avoid retrieving them again. Instead, we are given the spatial description of
a window block, say b. The spatial description of b is used to retrieve all the blocks of the
underlying spatial database that are spatially related to b (e.g., the blocks that contain,
or are contained in, b). Blocks in the underlying spatial database can be retrieved more
than once if they satisfy some spatial relationship with respect to different window blocks.
In order to avoid retrieving the same block more than once when a different window
block is processed, we maintain a spatial analog to the hash-table mechanism above. This
is achieved through the usage of some spatial data structure, namely the active border,
tailored to match the needs of this type of spatial retrieval. The active border can also be
viewed as simulating the spatial equivalent of a sort-merge list of pages which is used in
database query processing when accessing data through secondary indexes [10].
3 Algorithm
Answering a window query by first computing the maximal quadtree blocks comprising it,
and then retrieving the corresponding covering blocks in the underlying spatial database
proceeds as follows. Assume a query window W, a spatial database S, a query function F
that performs the appropriate variant of a window query test (e.g., a containment test)
and a record of type answer set that accumulates the answer to the window query.
answer-set procedure Algorithm-1(S,W,F);
begin
reference spatial-database
value window W;
value function F;
block B;
block set C;
spatial-object set T;
answer-set RESULT;
decompose W into its maximal quadtree blocks;
foreach block B in W do
BEGIN
C:=blocks in S THAT cover B;
foreach block Q in C do
apply F to spatial objects associated with Q */
By varying the function F and the data type answer set, many window operations can be
implemented using Algorithm-1. For example, to answer the report query (i.e., reporting
the identity of all the features that exist inside a window), the function F simply identifies
all the spatial objects inside the block of the underlying spatial database, and the data type
answer set is just a set of spatial object identifiers for the qualifying objects. To answer
the exist query (i.e., determining if feature f exists in w), the function F tests whether or
not f (or f 's identifier) exists inside the block of the underlying spatial database, and the
data type answer set is the type Boolean while U is a logical or operation. To answer the
select query (i.e., reporting the locations of all instances of feature f in the window), the
function F simply tests whether or not f (or f 's identifier) exists inside the block of the
underlying spatial database, and the data type answer set is a quadtree that stores in it
the location of these blocks.
There is one principal issue in implementing this algorithm. This was discussed in
Section 2 and corresponds to the situation that block q in the underlying spatial database
covers more than one maximal quadtree block in the window. In this case, q will be
retrieved several times. This is what happens in the algorithm reported in [3]. This could
be overcome by avoiding the invocation of the retrieval step for some of the maximal
quadtree window blocks. The issue is how do we skip some of the maximal quadtree
window blocks. In order to understand this issue, we briefly focus on the relation between
the maximal quadtree blocks of the window decomposition and the quadtree blocks in the
underlying spatial database.
Assume that b is a maximal window block that is generated by the window decomposition
algorithm. Due to the quadtree decomposition of both the window and the underlying
spatial database, b can either be contained in, or contain, one or more quadtree blocks of
the underlying spatial database. In particular, there are three possible cases as illustrated
by

Figure

2. Case 1 is demonstrated in the figure by window block 2 which contains more
than one quadtree block of the underlying spatial database. All of these blocks have to be
retrieved (e.g., from the disk), and processed by the algorithm (e.g., the spatial objects
associated with these blocks will be reported as intersecting the window). The second
case is illustrated by window block 9 of Figure 2. Block 9 contains exactly one block of
the underlying spatial database which will have to be retrieved (e.g., from the disk) as
well. The third case is demonstrated by window blocks 1, 4, 8, and 10 of Figure 2 which
all require retrieving (e.g., from the disk) the same quadtree block (i.e., block p of the
underlying spatial database). Case 3 arises frequently in any typical window query, as
shown by the experiments conducted in Section 6, thereby resulting in a large number of
redundant disk I/O requests.
Our algorithm is an improvement over Algorithm-1 and is based on the following
observation (it is restated as Lemma 1, as well as proved, in Section 4):
Observation 1: Assume that a block, say b, is a maximal block that lies inside
the window w and overlaps with a block of the underlying spatial database, say
q. If q is of greater size than b, then q must intersect with at least one of the
boundaries of the window w (refer to Figure 3 for illustration).
In other words, there cannot be database blocks that are bigger than the intersecting
window blocks which are in the middle of the query window. These big database blocks
have to intersect the boundary of the query window. Our window retrieval algorithm is
based on this observation which we illustrate further later in this section.
The new algorithm consists of procedures WINDOW RETRIEVE, GEN SOUTHERN MAXIMAL,
and MAX BLOCK. They are described below, while their detailed code is given in the Ap-
pendix. The algorithm works for an arbitrary rectangular window (i.e., it need not be
square). We avoid generating non-maximal quadtree blocks in the window (or at least
generate a bounded number of them) by using the same technique as in [3], which we outline
below. Note that there are O(n 2 ) non-maximal blocks inside an n \Theta n window. Also,
each maximal quadtree block in the window is processed only once (i.e., as a neighbor of
another node) regardless of its size.
We make use of an active border data structure [23] which is a separator between the
window regions that have already been processed and the rest of the window. Note that
the active border in our case will differ from the conventional one (which looks like a
staircase) because of the nature of the block traversal process. In particular, we traverse
the blocks in the window in a row-by-row manner rather than in quadrant order (i.e., NW,
NE, SW, SE).

Figures

4-8 represent the first five steps of the execution of the algorithm for the query
window w. The heavy lines in Figure 4 represent the active border for window w at the
initial stage of the algorithm. In generating a new block, the window decomposer has to
consult the active border in order to avoid generating a disk I/O request for a window
region that has already been processed by a block of the underlying spatial database that
has already been retrieved.
The active border is maintained as follows. First, a window block, say b, is generated
by the window decomposer and a disk I/O request is issued to access the region of the
underlying spatial database corresponding to b. Assume that b overlaps in space with
block u in the underlying spatial database. Therefore, u is retrieved as a result of the
disk I/O request corresponding to b. The spatial objects inside u are processed and thus
there is no need to retrieve u again. As a result, the active border needs to be updated
by block b or u depending on which one provides more coverage of the window region.

Figures

4-8 illustrate the updating process of the active border. If u has a larger overlap
with the unprocessed portion of the window than b (e.g. window block 1 and block p
of the underlying spatial database in Figure 4, as well as window block 3 and block q
of the underlying spatial database), then the active border is expanded using u's region

Figure

5). If u is contained in b (e.g., window block 2 and block r of the underlying
spatial database in Figure 4), then all the other blocks in the underlying spatial database
have to be retrieved as well, and the active border is expanded by b's region (Figure 6).
If the sizes of b and u are the same (e.g., window block 12 and block s of the underlying
spatial database in Figure 4), then the active border is expanded by either one of them

Figure

8). Notice that, if we were using Algorithm-1, window blocks 4, 8, 10, and 7
would still be processed and hence would generate four redundant disk I/O requests to
retrieve blocks p and q.
The generation of the maximal quadtree blocks inside a given window is controlled by
procedure WINDOW RETRIEVE whose basic structure is given in Figure 9. WINDOW RETRIEVE
scans the window row-by-row (in the block domain rather than in the pixel domain),
and visits the blocks within it that have not been visited in previous scans 3 . For each
visited window block, say b, the underlying spatial database is queried and a corresponding
quadtree block, say q, is retrieved from the database. Procedures GEN SOUTHERN MAXIMAL
and MAX BLOCK generate b's or q's maximal southern neighboring blocks (in fact, only the
3 Observe that we could have chosen to scan the window in a column-by-column fashion instead of
row-by-row. The result is unchanged as long as the data structures for keeping track of the active border
are reoriented appropriately.
portion of q that lies inside the window will be used) according to the three cases presented
earlier in this section, that relate the location and size of both b and q with respect to
the query window. WINDOW RETRIEVE also makes sure that any of the remaining columns
of row r that lie within b or q are skipped. For example, consider Figure 2, where five
scans are needed to cover the 12 \Theta 12 window with maximal blocks. The first scan visits
blocks 1, 2, and 3; the second scan visits blocks 12, 5, 6, and 9; the remaining scans
visit blocks 14 and 11; 13; and 15. Notice that once blocks 5 and 6 have been visited,
their columns (i.e., 2-5 in the window) have been completely processed. Also, observe
that when block 1 is generated, block p of the underlying spatial database, which overlaps
with block 1, is retrieved. As a result, window blocks, 4, 8, and 10 are skipped. This way,
the algorithm can avoid reaccessing p by skipping all the window blocks that overlap with
p. As a consequence, the southern neighbors of p (and not those of block 1) are generated
by the algorithm.
Procedure GEN SOUTHERN MAXIMAL generates the southern neighbors (maximal blocks)
for each maximal block B generated by WINDOW RETRIEVE and that is not
contained in another maximal block. There are a number of possible cases illustrated in

Figure

greater than or equal to B. Otherwise, the total width of
blocks N 1 through Nm is equal to that of B. It is impossible for the total length to exceed
that of B unless there is only one neighbor (see Figure 10b). Procedure MAX BLOCK takes
as its input a window, say w, and the values of the x and y coordinates of a pixel, say
(col,row), and returns the maximal block in w with (col,row) as its upper-leftmost corner.
The resulting block has width 2 s , where s is the maximum value of i (0 - i - log T , where
\Theta T is the size of the image space) such that row mod and the point
(row lies inside w.

Figure

11a gives the active border's most general form. The active border does not
contain any holes (see Lemma 1 in Section 4 and thus Figure 11b corresponds to an
impossible situation). When a block of the underlying spatial database, say q, is retrieved,
the algorithm checks its size against the corresponding window block, say b. If q's size
is larger than that of b, then the algorithm knows that q has to intersect one of the
window's boundaries (see Lemma 1 in Section 4). We make use of this property here.

Figure

3 shows the four possible cases where the block retrieved from the underlying
spatial database intersects with one of the window boundaries. Each of the four cases
must be treated separately by the algorithm.
There is no need to maintain any data structures to explicitly store the northern portion
of the active border since WINDOW RETRIEVE can handle this portion directly. During the
first row-by-row scan of the window by WINDOW RETRIEVE, if a block of the underlying
spatial database, say q, is retrieved that happens to intersect the northern boundary of
the window (Figure 3a), then WINDOW RETRIEVE skips the window blocks in the current
row scan that overlap with q. The portion of the southern boundary of q that lies inside
the window is used to generate the southern neighboring blocks to be processed in the
next scan.
When block q of the underlying spatial database intersects only the southern boundary
of the window (Figure 3d), then it also suffices for WINDOW RETRIEVE to skip all the window
blocks that are adjacent to the window block that initiated q's retrieval. Although this
seems intuitive, it is not straight-forward to see that all of the processing of block q by
WINDOW RETRIEVE is localized in one part of the algorithm. In particular, although true,
it is not directly obvious that all the blocks that overlap with q will be processed by
WINDOW RETRIEVE at the same time so that they can be skipped. Thus as a result of this
localized processing, there is no need to maintain any explicit data structures in this case
either.
If q intersects the western or eastern boundaries (Figures 3b and 3c), its overlap with
the window creates a pocket-like region that needs to be stored in two separate lists,
WestList or EastList, respectively. Each time a window block is generated, it has to be
checked against the active border in order to make sure that the block is not covered by
a previously retrieved block of the underlying spatial database. Below, we show how to
perform this check in constant time.
To facilitate our presentation, we represent both WestList and EastList as two one-dimensional
arrays, each of length equal to the height of the window: WestList[r : r+n\Gamma1]
where the height of the window is n and (r; c) corresponds
to the x and y coordinate values of its upper-left corner. Figure 12b shows the border
represented by each of the two arrays as a result of extracting an 8 \Theta 12 window from the
underlying spatial database in Figure 12a. Let (r q ; c q ) be the location of the upper-left
corner of q. If q intersects the west boundary of the window, then WestList[r q ] is set to the
where the first component of the pair denotes the x coordinate value
of q's east boundary while the second component (i.e., s q ) denotes the size of q. The pair
represents the pocket-like region resulting from the intersection of q with w.
Similarly, if q intersects the east boundary of the window, then EastList[r q ] is set to the
Each time a window block is generated it has to be checked against the
active border in order to make sure that the block is not covered by a previously retrieved
block of the underlying spatial database. Notice that updating the active border only
requires one array access (either updating WestList or EastList depending on whether
q intersects the west or east boundaries of the window, respectively), while checking a
window block against the active border takes only two array accesses (one access to each
of WestList and EastList). Therefore, maintaining the active border, whether updating
or checking, takes O(1) time.
Observe that WINDOW RETRIEVE always generates maximal neighboring blocks, and
a bounded number of non-maximal blocks. An example of this situation arises when
processing blocks A-J in the first row of the window in Figure 13. Each of blocks B,
can generate at most one non-maximal neighboring block. Even
though these non-maximal blocks are generated, procedure WINDOW RETRIEVE skips them
in the next scan since they are subsumed (i.e., contained) in the previously processed
maximal block in the scan. For example, when scanning block K in Figure 13, blocks L,
M, and N are skipped since they are contained in it. This is easy to detect because for
each block we know the x and y coordinate values of its upper-left corner and its size.
Proving that the algorithm is correct involves showing that every block of the underlying
spatial database that overlaps with the query window is retrieved and processed by the
algorithm. In order to prove this, we can structure our algorithm in the following way.
The algorithm consists of two mechanisms: one for generating maximal quadtree blocks
inside the window (also termed the window decomposition algorithm), and the other for
retrieving blocks from the underlying spatial database and maintaining the active border.
The active border keeps track of the blocks on the boundary of the window that have
already been retrieved. This guarantees that each block in the underlying spatial database
is not retrieved more than once. Our strategy for proving that the algorithm is correct
is to separate these two mechanisms, show that each one is correct, and then prove that
they interact properly.
The algorithm has two cases. The first case arises when all the quadtree blocks of
the underlying spatial database that overlap the window are smaller than or equal to the
size of the smallest quadtree block in the window. The second case arises when this size
criterion is not satisfied.
In the first case the window decomposition algorithm will have to generate all of the
maximal quadtree blocks inside the window and none will be skipped - i.e., each one
causes a block of the underlying spatial database to be retrieved. In other words, there
are no pockets and thus the arrays WestList and EastList are never updated or accessed.
This means that the algorithm reduces to the window decomposition algorithm given in [3].
The window decomposition algorithm is proved correct in [3] and thus we will not
address it here. However, we only state that proving that the window decomposition
algorithm is correct involves showing that the execution of the algorithm generates a list
of maximal blocks that lie entirely inside the window and that cover each point inside the
window. In other words, each point inside the window is covered by one maximal block
that is generated through the execution of the algorithm. The following two theorems are
proved in [3]:
Theorem 1: Each point inside a window is covered by one and only one maximal block
generated by the algorithm.
Theorem 2: The window decomposition algorithm generates all the maximal blocks
inside the window and only maximal blocks, and hence is correct.
We now address the second case where some of the blocks in the underlying spatial
database are larger than the smallest block in the window - i.e., blocks of the underlying
spatial database whose sizes are larger than the overlapping window blocks. We need to
show that the interaction and maintenance of the active border with the window decomposition
algorithm (1) guarantees that every block of the underlying spatial database that
overlaps with the query window is retrieved and processed by the algorithm, and (2) does
not interfere negatively with the window decomposition algorithm. From the complexity
point of view, we prove, in Section 5, that every block of the underlying spatial database
that overlaps with the window is retrieved only once.
First, we use the concept of a maximal zone [3] to facilitate the presentation of the
proofs. Assume a window having (c; r) as the x and y coordinate values of its upper-left
corner with height w h (i.e., in the y direction) and width ww (i.e., in the x direction). First,
let us look at the x direction. Processing along the width ww , we subdivide the window
into p vertical strips with coordinate values of their upper-left corner
. p is defined so that c An example of such a decomposition
into vertical strips is shown in Figure 14a. The vertical strips are termed maximal columns.
We now subdivide the window into horizontal strips in the same way. In particular,
we have q horizontal strips with (c; r i as the x and y coordinate values of
their upper-left corner where r
r . q is defined so that r An example of
such a decomposition into horizontal strips is shown in Figure 14b. The horizontal strips
are termed maximal rows.
Now we define the term maximal zones as follows. A maximal zone, say Z ij , is the
region between the vertical strips (i.e., maximal columns) having c i and c i+1 as the x-coordinate
values of their upper-left corner and the horizontal strips (i.e., maximal rows)
having r j and r j+1 as the y-coordinate values of their upper-left corner where

Figure

14c gives an example of decomposing a window into its maximal
zones.
Below, we state some propositions dealing with properties of maximal zones. Their
proofs are straightforward, and we omit them in the interest of brevity. They are illustrated
in

Figure

14d.
Proposition 1: Each maximal block inside the window is entirely contained in one and
only one maximal zone.
Proposition 2: All the maximal blocks inside a maximal zone are of the same size.
Proposition 3: A maximal zone contains either one maximal block, or one row of maximal
blocks, or one column of maximal blocks.
Proposition 4: All the southern neighbors of a block lie in one maximal zone.
Proposition 5: There exists a maximal column, say c k , inside the window such that,
In other words, the sequence of distances between (width of) the maximal columns forms
a monotonically increasing sequence followed by a monotonically decreasing sequence. An
equivalent property exists for maximal rows.
A useful invariant that holds during the execution of the window decomposition algorithm
that also relates to maximal columns is stated below.
Invariant 1: Each maximal window block and its southern neighbor window blocks that
are both generated by the window decomposition algorithm always lie inside the same
maximal column.
In other words, the blocks inside a maximal column are processed independently of the
blocks in other maximal columns inside the window. Put differently, although the algorithm
scans the window row-by-row (in the block domain) and generates the maximal
neighboring blocks to the south of each block encountered, there is no interaction between
blocks of different maximal columns. We make use of this invariant to prove the lemmas
below.
Lemma 1: Assume that a block, say b, is a maximal block that lies inside the window
w and overlaps with a block of the underlying spatial database, say q. If q is of greater
size than b, then q must intersect with at least one of the boundaries of the window w

Figure

3).
Proof by Contradiction: Since b overlaps with q and b is smaller than q, then b is
contained in q (by the definition of a quadtree decomposition of space). Assume to the
contrary that the database block q lies entirely inside w. If q is of greater size than the
window block b that overlaps with it, then b is not a maximal block since we can use a
window block b 1 that contains b and that coincides with q as our new maximal block,
which leads to a contradiction. 2
As a result, we deal with three categories of blocks of the underlying spatial database
that intersect the window boundary: blocks that intersect the north boundary, blocks
that intersect the east (west) boundary, and blocks that intersect the south boundary.
Notice that the algorithm treats blocks that intersect both the west (east) and the south
boundaries of the window as if they just intersect the west (east) boundary. On the other
hand, it treats blocks that intersect both the north and west (east) boundaries of the
window as if they just intersect the north boundary. Blocks intersecting the east or west
boundary of the window receive the same type of processing and hence are considered as
one group. We prove the correctness of the interaction of each category separately.
Lemma 1 means that the active border does not contain any holes (see Figure 11b)
since the query window is scanned row-by-row, and large-sized blocks of the underlying
spatial database intersect only the window boundary. Therefore, storing only the outer
boundary of the active border is enough.
Lemma 2a: If a block of the underlying spatial database, say q, intersects the west (east)
window boundary, then the east (west) boundary of q that lies inside the window must
coincide with a boundary of one of the maximal columns of the window.
Proof: We prove the lemma for the case when q intersects the west boundary of the
window. The other case is similar. Assume the lemma does not hold - i.e., that q
intersects the window boundary but that the eastern boundary of q that lies inside the
window does not coincide with a maximal column of the window. Therefore, one of two
possible cases must occur. These are illustrated in Figure 15. Both of the cases cannot
happen since, by the definition of a quadtree decomposition, blocks cannot overlap in this
manner. 2
An analogous lemma can be stated for blocks intersecting the north or south boundary of
the window.
Lemma 2b: If a block of the underlying spatial database, say q, intersects the north
window boundary, then the south (north) boundary of q that lies inside the window
must coincide with a boundary of one of the maximal rows of the window.
Lemma 3: If a block of the underlying spatial database, say q, intersects the west (east)
window boundary, then the part, if any, of the south boundary of q, say s, that lies inside
the window must coincide with the north boundary of a maximal block inside the window.
Proof: Assume that q intersects the west boundary of the window. From Lemma 2a,
q's east boundary coincides with a boundary of a maximal column of the window, say c.
However, other maximal columns to the west of c may intersect q as well (for example, in

Figure

12, maximal column C 1 intersects block p of the underlying spatial database). If
q intersects with no maximal columns other than c, then only two cases are possible (as
illustrated in Figures 16a and 16b). Figure 16a cannot occur in a quadtree decomposition,
while

Figure

16b satisfies the Lemma. If q intersects with one or more maximal columns
other than c, then s must coincide with a maximal row inside the window (Figure 16c) as
the other case cannot exist in a quadtree decomposition (Figure 16d). Since a maximal
row coincides with the north boundary of maximal blocks across the whole window, then
this applies to q as well. 2
Lemma 4: If a block of the underlying spatial database, say q, intersects the west (east)
window boundary, then the window decomposition strategy will only skip the window
blocks covered by q while maintaining normal processing otherwise. In other words, up-dating
the active border with q does not adversely affect the mechanism used for window
decomposition.
Proof: Assume that q intersects the west border of the window. By Lemma 2a, the east
boundary of q coincides with a maximal column of the window. Therefore, the window
decomposition mechanism will function properly to the east of q since, by Invariant 1, the
block generation process works independently inside each maximal column. The portion
of the south boundary of q, say s, that lies inside the window, is used by the algorithm to
generate the new window blocks to the south of q. However, from Lemma 3, all parts of
s coincide with the north boundary of a maximal block inside the window. Therefore, by
applying a maximal block computation at s, the algorithm would still generate maximal
blocks of the window to the south of q after skipping the ones inside q (and hence avoid
retrieving q more than once by the overlapping window blocks). If the south boundary of
lies outside the window, then the Lemma holds since no further processing to the south
of q is needed. In addition, by Invariant 1, the window decomposition process to the east
of q is not affected by q since the east boundary of q coincides with a maximal column.We now study the case where a block of the underlying spatial database intersects the
south boundary of the window. We make use of the following lemma. Its proof is given
in [3] (where it is Lemma 4).
Lemma 5: All the maximal blocks arranged in a row inside a maximal zone are processed
in the same iteration of the main loop of procedure WINDOW RETRIEVE.
Lemma a block of the underlying spatial database, say q, intersects only the south
boundary of the window, then q lies entirely inside one maximal column of the window.
Proof: By Proposition 5, if q overlaps with more than one maximal column of the window,
then either the size of q is not a power of two (a contradiction) or q must intersect with
the east or west boundary of the window (a contradiction). Therefore, q lies inside one
maximal column. 2
Combining Lemmas 5 and 6, we get the following result:
Lemma 7: If a block of the underlying spatial database, say q, intersects only the south
boundary of the window, then the window decomposition strategy will only skip the
window blocks covered by q, while maintaining normal processing otherwise.
Proof: By Lemma 6, q lies inside only one maximal column of the window. By Lemma 5,
if one maximal window block, say b, results in retrieving q, then the rest of the window
blocks in the maximal zone that lie in the same row as b, will exist in the same iteration of
the main loop of procedure WINDOW RETRIEVE. Therefore, all of them can be automatically
skipped by the algorithm once q is retrieved, and hence no additional data structure is
needed to record q's retrieval. Since the south boundary of q is already outside the window,
no further processing is needed to the south of q. The effect of this is that it results in
skipping all the window blocks that overlap with q and that lie to the south of b up to
the south boundary of the window. Also, by Invariant 1, q lies inside only one maximal
column and hence does not affect other portions of the window decomposition mechanism.Lemma 8: If a block of the underlying spatial database, say q, intersects the north
boundary of the window, then the window decomposition strategy will only skip the
window blocks covered by q while maintaining normal processing otherwise.
Proof: Since q intersects the north boundary of the window, q will be retrieved when
the algorithm scans the first row in the window. In addition, q will be retrieved by the
leftmost maximal window block, say b, that overlaps with q since scanning is from left
to right. Therefore, all the window blocks to the right of b and that overlap with q are
automatically skipped by the algorithm since all of them immediately follow b in TopList,
the list of blocks to be processed. Processing of the algorithm resumes at the first window
block to the right of q in the current row scan. By Lemma 2b, the part of q's south border,
say s, that lies inside the window will coincide with a maximal row of the window. Since
a maximal row coincides with the north boundary of maximal blocks across the whole
window, this applies to s as well. Therefore, using s to generate maximal blocks to the
south of q will resume regular processing of the decomposition algorithm as it results in
generating legitimate maximal blocks of the window after skipping the window blocks that
overlap with q. Therefore, q is retrieved just once by the algorithm without affecting the
normal processing of the algorithm. 2
Combining Theorems 1 and 2 and Lemmas 4, 7, and 8 we get the following theorem:
Theorem 3; Every block of the underlying spatial database that overlaps with the query
window is retrieved by procedure WINDOW RETRIEVE and hence the algorithm is correct.
Proof: By Theorem 1, maximal blocks of the window cover every point inside the window
(without overlap). Therefore, if blocks of the underlying spatial database are smaller than
the window blocks, then, by Theorem 2, the window decomposition algorithm will generate
all the maximal blocks inside the window, and hence all the blocks of the underlying
spatial database overlapping with the window blocks are retrieved. If some of the blocks
of the underlying database, say D, are larger than the corresponding maximal window
blocks, then by Lemma 1, each block, say q in D, has to intersect with some of the
window boundaries. By Lemmas 4, 7, and 8, the algorithm will skip all but one of the
maximal blocks of the window that overlap with q (this is because when one of the maximal
blocks has to retrieve q, then the rest of the overlapping window blocks are skipped).
Lemmas 4, 7, and 8 also show that the normal window decomposition mechanism is
resumed after processing each block of the underlying spatial database that overlaps the
5 Complexity Analysis
Based on Observation 1 (restated as Lemma 1) that relates the size of the query window
blocks to the size of the underlying database blocks that they intersect, we are able to
restrict the size of the active border, so that it has a worst-case space complexity of O(n)
instead of O(n 2 ) for an n \Theta n query window.
Analyzing the time complexity of our algorithm is a bit complex as there are two
processes going on, and hence two ways of measuring it. The first is in terms of the blocks
of the underlying spatial database that are retrieved (the I/O cost), while the second is
in terms of the maximal blocks in the window, i.e., the window decomposition mechanism
and the maintenance of the active border (the CPU cost).
The CPU cost of the process of generating the maximal quadtree blocks in the window
is computed as follows. First, we find the number of maximal quadtree blocks, say N ,
inside the window, and then compute the cost of generating each one of the maximal
quadtree blocks, say T gen . The overall CPU cost T cpu is the product of these two terms,
i.e.,
It is important to note that, usually, not all of the maximal blocks inside the window are
generated. However, in the worst case, when none of the blocks in the underlying spatial
database intersect the border of the window, all the maximal blocks inside the window
are generated.
It is known that the number of maximal quadtree blocks inside a square window of
size n \Theta n is
in the worst case ([8, 11, 27]). It remains to compute the cost of generating each maximal
quadtree block comprising the window, i.e., T gen . This consists of the work, say Tm ,
to generate a maximal quadtree block, say B, and the work that is wasted, say Tw , in
generating southern neighboring blocks of B that are non-maximal. Therefore, the total
CPU execution time of the window decomposition algorithm is
Given a point (x,y) in a T \Theta T space, there can be at most log T different blocks
of size 2 i (0 - i - log T ) with (x,y) as their upper-left corner. We use a binary search
through this set of blocks to determine the maximal quadtree block inside the window [3].
Thus Tm is O(log log T ).
To compute Tw , we need to show that each maximal quadtree block inside the window
is generated once, and that only a limited number of non-maximal blocks are generated.
We say that the work required to generate blocks that are not maximal with respect to
a particular window is wasted. Such blocks are ignored (i.e., bypassed) in subsequent
processing. For example, the work in generating the southern neighbors of blocks B, C,
D, F, G, H, and J (i.e., L, M, N, P, Q, R, and T, respectively) in Figure 13 is wasted.
This is formulated and proved in the following two Lemmas.
Lemma 9: Each maximal quadtree block inside window w is generated at most once.
Proof: In Theorem 2, we proved that every maximal quadtree block inside window w
is generated by the algorithm. To show that it is generated only once we observe that
each window block processed by the algorithm generates only its southern neighbors. The
facts that non-maximal window blocks are bypassed by the algorithm, and that maximal
blocks do not overlap, mean that each maximal window block, say B, is generated as the
southern neighbor of only one other maximal window block, say C. Note that this worst
case only arises if WINDOW RETRIEVE generates all of the maximal blocks (i.e., none are
Lemma 10: Each window block visited by the algorithm can waste at most O(log log T )
work in generating intermediate non-maximal window blocks.
Proof: Assume that window block B generates wasted work. We show that this work takes
O(log log T ) time. B can generate neighboring southern maximal blocks that are either
smaller or larger. When the size of the neighboring block is greater than or equal to the size
of B, then the algorithm takes O(log log T of whether or not it is wasted
and the Lemma holds. When more than one southern neighboring block is generated (this
number can be of the same order as the size of B), we need to show that all the generated
southern blocks are maximal, and cannot be bypassed, i.e, they are not wasted work.
We shall prove this by contradiction. Assume that B generates more than one southern
neighboring block and that all of them are bypassed (i.e., not visited) in subsequent
processing. It should be clear that due to the nature of the quadtree decomposition of
space, either all of them are visited, or all are bypassed. Our assumption means that there
exists a block C whose width is greater than the total width of B's southern neighbors.
Let (B x ,B y ) and (C x ,C y ) be the locations of the upper-leftmost pixels of blocks B and
C, respectively. Also, let B s and C s be the widths of blocks B and C, respectively. It is
easy to see that the fact that B and C are maximal blocks that are southern neighbors
of other visited maximal blocks means that C . The fact that C s ? B s means
that the lower-rightmost pixel of C is at (C x which is in the window.
Therefore, which is the lower-rightmost pixel of B's southern
neighbor of equal size, say D, is also in the window. This means that D is B's neighboring
southern maximal block. However, this contradicts the existence of more than one such
block. Thus the assumption that all of the southern neighboring blocks of B are bypassed
is invalid. Therefore, no work is wasted in generating B's southern neighbors in this case,
and the Lemma holds. 2
Combining the results for Tm and Tw , and Theorem 3 means that we have proven the
following theorem.
Theorem 4: Given an n \Theta n window in a T \Theta T image, the worst-case CPU execution
time for the algorithm is O(n log log T
In order to compute the disk I/O execution time of the algorithm, say T io , we need to
prove the following theorem.
Theorem 5: Every block of the underlying spatial database that overlaps the query
window is retrieved once, and only once, by WINDOW RETRIEVE.
Proof: By Lemma 9, each maximal block is generated at most once. Let q be a block in
the underlying spatial database and suppose that q overlaps the window. If q lies inside
the window and is of equal or smaller size than the overlapping window block, say b, then
q will be retrieved once by the algorithm when b is generated, and hence the theorem holds
(notice that maximal blocks do not overlap). If q overlaps the window and if q contains
more than one window block, then q will be retrieved by the first window block, say b, that
encounters q. However, from that point onwards, all the window blocks that overlap q will
be skipped and block q will not be retrieved again. By Lemma 1, q has to intersect one
of the window boundaries. If q intersects the east or west boundaries of the window, then
by Lemma 4 the active border (i.e., WestList and EastList) prevents block q from being
retrieved again by the remaining window blocks that overlap q. Otherwise, if q intersects
the north or south boundaries of the window, then by Lemmas 7 and 8 the algorithm skips
the remaining window blocks that overlap q. Therefore q will be retrieved once, and only
once. Hence the theorem also holds when q is of larger size than the overlapping window
blocks. 2
Note that there is an onto relation between the set of blocks of the underlying spatial
database that are retrieved by the algorithm and the set of maximal window blocks
generated by the algorithm. This relation is only onto, rather than one-to-one onto, because
a window block, say b, may overlap more than one block in the underlying spatial
database (i.e., the overlapped blocks are smaller than b), in which case several blocks in
the underlying spatial database will be retrieved. However, they will only be retrieved
once.
The actual disk I/O cost of the algorithm depends on how the quadtree is implemented.
Assume that the underlying database consists of a total of K quadtree blocks and that M
of these blocks are retrieved by the window query. Assume further that it spans a space
of size T \Theta T . A pointer-based quadtree implementation may have an overall I/O cost as
high as M log T as we must traverse at most log T pointers to access the relevant block
in the quadtree. Using a pointerless quadtree representation such as a linear quadtree
(e.g., [14]) where each leaf block is represented by a unique number which is stored in a
-tree, the overall I/O cost is O(M log K) as the cost to retrieve each block is O(log K).
6 Empirical Results
In this section, we study the performance of the two algorithms Algorithm-1 (which
is based on the window decomposition algorithm [3]) and WINDOW RETRIEVE, which is
proposed in this paper. The window decomposition part of the two algorithms has the
same worst-case execution time complexity (i.e., O(n log log T )) as shown in Section 5. As
a result, we only focus on comparing the I/O cost of the two algorithms.

Figure

17 shows the results of experiments comparing the number of disk I/O requests
(i.e., blocks retrieved) to answer a window query using Algorithm-1(labeled Old Alg))
with the number of disk I/O requests generated by WINDOW RETRIEVE (labeled New Alg).
Our data consists of maps of the road network of the US provided by the Bureau of the
Census. A sample map corresponding to Falls Church containing 640 line segments is
given in Figure 18. The maps are represented using the PMR-quadtree [18, 21], a variant
of a quadtree for storing vector data.
The x-axis corresponds to the ratio between the window area and the area of the
underlying spatial database which spanned a 512 \Theta 512 image. Experiments were run for
the ratios .01, .001, .0001, and .00001. For example, the ratio .00001 corresponds to a 5 \Theta 5
window, while the ratio .01 corresponds to a 50 \Theta 50 window. For each such ratio, a set of
500 randomly positioned rectangles were generated. A window query is processed for each
rectangle using both algorithms. The y-axis corresponds to the average of the disk I/O
requests for each set of rectangles plotted on a logarithmic scale. Not surprisingly, use of
WINDOW RETRIEVE does not lead to a great reduction in disk I/O requests for small window
sizes (about 25%) since for both the window and the corresponding area in the underlying
database the number of blocks is relatively small. However, for larger size windows, the
reduction is much more pronounced, and, in fact, use of WINDOW RETRIEVE leads to an
improvement of over one order of magnitude (e.g., a factor of 10).
7 Concluding Remarks
An algorithm was presented for retrieving the blocks in a quadtree-base spatial database
environment that overlap a given window. It is based on decomposing a window into its
maximal quadtree blocks, and performing simpler sub-queries to the underlying spatial
database. Each block in the underlying spatial database is only retrieved once. The
algorithm is proven (analytically and experimentally) to lead to an improvement in disk
I/O performance. The algorithm requires some extra space (on the order of the width of
the window), to store the active border. It remains to consider how the algorithm can be
adapted to handle spatial databases with non-disjoint objects (i.e., overlapping).
Performance can be enhanced further by selecting a suitable buffering strategy for
the underlying B-tree [1, 5]. In particular, if we can adjust the scan order of the window
algorithm so that the window quadtree blocks are visited in Morton order, and accompany
this with a most-recently-used buffer replacement policy, then this would guarantee that
B-tree pages, both leaf and non-leaf pages, would be requested by the algorithm only
once, and hence no redundant disk I/O requests would result. For more clarification on
this issue, see [1, 5].



--R

An algorithm and a cost model for window queries in spatial databases.
Uniquely reporting spatial objects: yet another operation for comparing spatial data structures.
Decomposing a window into maximal quadtree blocks.
Hashing by proximity to process duplicates in spatial databases.
Multidimensional window retrieval in large spatial databases.

The ubiquitous B-tree
The space efficiency of quadtrees.
Advances in Spatial Databases - Fourth International Symposium
Fundamentals of Database Systems.
Analytical results on the quadtree decomposition of arbitrary rectangles.
Adaptive grids for geometric operations.
The BANG file: a new kind of grid file.
An effective way to represent quadtrees.
Efficient structures for geometric data management.
R-trees: a dynamic index structure for spatial searching.
The LSD tree: spatial access to multidimensional point and non-point data
A consistent hierarchical representation for vector data.
The grid file: an adaptable
Applications of Spatial Data Structures: Computer Graphics
The Design and Analysis of Spatial Data Structures.
Spatial data models and query processing.
Computing geometric properties of images represented by linear quadtrees.
Storing a collection of polygons using quadtrees.
The buddy-tree: an efficient and robust access method for spatial data base systems

A formula for computing the number of quadtree node fragments created by a shift.
--TR

--CTR
Ashraf Aboulnaga , Walid G. Aref, Window Query Processing in Linear Quadtrees, Distributed and Parallel Databases, v.10 n.2, p.111-126, September 2001

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'fault-based testing'
 'zero storage biometric authentication' 'scheduling']
marked:['data structures', 'spatial databases', 'databases', 'design of algorithms', 'range query', 'clipping', 'active border', 'quadtree space decomposition', 'window block retrieval']
--T
A Multi-Resolution Content-Based Retrieval Approach for Geographic Images.
--A
Current retrieval methods in geographic image databases use only pixel-by-pixel spectral information. Texture is an important property of geographical images that can improve retrieval effectiveness and efficiency. In this paper, we present a content-based retrieval approach that utilizes the texture features of geographical images. Various texture features are extracted using wavelet transforms. Based on the texture features, we design a hierarchical approach to cluster geographical images for effective and efficient retrieval, measuring distances between feature vectors in the feature space. Using wavelet-based multi-resolution decomposition, two different sets of texture features are formulated for clustering. For each feature set, different distance measurement techniques are designed and experimented for clustering images in a database. The experimental results demonstrate that the retrieval efficiency and effectiveness improve when our clustering approach is used.
--B
Introduction
Satellite remote sensing development generated huge amount of image data during the last two
decades. With the launch of the new breed of high spatial resolution satellite systems, the volume
This research is partially supported by Xerox Corporation and NCGIA at Buffalo.
of digital image data will significantly increase. Paralleling with the remote image sensing is the
availability of Geographic Information System (GIS) data such as digital orthophotographs and
digital elevation models that are available in pixel format and in sheer volumes. Thus, there is
a great demand for effective retrieval mechanisms. In processing these geographic images, the
ultimate interest is to recognize and locate specific contents. A content-based retrieval approach is
paramount to organizing geographic images. In the design of such retrieval approaches, significant
features must first be extracted from geographical data in their pixel format. The features are
numerical measurements to characterize an image. These features can then be used to cluster and
index the images for efficient retrieval. Methods have been developed for indexing and accessing
alpha-numerical data in traditional databases. However, the traditional approaches to indexing
may not be appropriate in the context of content-based image retrieval [CYDA88, RS91, BPJ93].
In this context, a challenging problem arises with many image databases, within which queries are
posed via visual or pictorial examples (termed visual queries). A typical visual query might entail
the location of all images in a database that contain a subimage similar to a given query image.
Such a query is well suited to and most appropriate for geographic images in identifying specific
geographic contents such as land use/land cover types.
Texture in images has been an important aspect for geographic image classification and retrieval
[Har79, Joh94, RW96, WH90], due primarily to the heterogeneity nature of geographic
images and the association between the texture patterns and the geographic contents. The variety
and complexity of geographic texture present a challenge as well as an impetus for seeking
more effective approaches in feature representation. We have conducted experimental research
on image data retrieval based on texture feature extraction [SZ97, RSZSM97, SZB97]. In most
existing content-based retrieval approaches, feature vectors of images are first constructed (e.g.,
from wavelet transforms), which are then used to distinguish images through measuring distances
between feature vectors. Our experimental results demonstrate that this type of approaches can be
effectively used to perform content-based retrieval based on similarity comparison between images.
However, we encounter a critical problem that the feature vectors of some semantically irrelevant
images may be located very close in the feature space. Figure 1 presents two images of water
and stone between which the distance of the feature vectors is very small, but these images are
semantically not similar. Given a query such that its feature vector is located in the neighborhood
of the feature vectors of the two given images, both images are highly possible to be retrieved
together in response to the query. Thus, indexing alone, using R-tree or its variants based on the
closeness of feature vectors in the feature space, sometimes may not provide satisfactory solutions.
An effective clustering approach needs to be integrated into indexing techniques for efficient and
effective retrieval of image data. Once the query is narrowed to a specific category, image retrieval
can then proceed efficiently.
(a) (b)

Figure

1: Semantically different Images with similar feature vectors.

Figure

shows two arbitrary shape semantic clusters C 1 and C 2 in a two-dimensional feature
space. We may assume that C 1 and C 2 represent the images of stone and water in the feature space.
Consider the query image q which belongs to the cluster C 1 . If we only consider the closeness of
feature vectors in the feature space to return relevant images, we may retrieve many images from
cluster C 2 that are semantically irrelevant. Indexing trees such as R-trees are very efficient in
retrieving the nearest neighbors of the query image, but since they do not consider semantics
of images, the retrieved images might be semantically irrelevant. If we successfully classify the
database images into different semantic clusters, a visual query can be quickly narrowed to a
specific category. Therefore, combining clustering with R-tree related indexing, image retrieval can
proceed both effectively and efficiently.
x
x
x
x
x
x
x
x
x
x
x
xx
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

Figure

2: Two semantic clusters.
In this paper, we present an approach to effective and efficient retrieval of images in large
geographical image database systems. The goal is, given a query image of size n \Theta n pixels, to
retrieve all the database images of size N \Theta N (where n - N ), that contain a portion similar to
the query image. Database images have been decomposed into smaller segments (down to n \Theta n
segments) offline. The query image will be compared to the corresponding segments (as will be
discussed later). A clustering approach is designed to be incorporated into the existing indexing
methods [SRF87, BKSS90, LJF94] to achieve the effectiveness and efficiency. Similar to most of
other current clustering methods, our approach cannot distinguish overlapping clusters 1 . Thus,
1 The proposed method in [SCZ98] handles the overlapping clusters using heterogeneous features.
we assume that different clusters do not overlap in the feature space. The semantic clusters can
have arbitrary shapes or can be disconnected. We first choose a set of training samples to identify
predefined semantics clusters (for example, residential or agriculture). These clusters include the
semantically similar images. Then using a hierarchical method we find template images representing
each cluster. This approach helps consider both semantics and feature vectors in classifying images.
Using the multi-resolution property of wavelet transforms, we extract the features at different scales
where only the coarse features are used in clustering. In our clustering approach, distribution of
feature vectors in the feature space is also considered. Combining the proposed clustering with any
existing indexing approach, our experiments demonstrate that retrieval from clustered database
outperforms the methods that are based only on nearest neighbor retrieval. In addition, since the
retrieval of images is narrowed to a specific category, image retrieval can proceed more efficiently.
Since geographical images (specially airphoto images) are rich in texture, and have relatively well-defined
clusters, it makes our approach, which extracts multi-resolution texture features and clusters
the images, a good candidate to be used.
The remainder of this paper is organized as follows. Section 2 presents the approach to multi-resolution
texture feature extraction. Section 3 discusses the application of the feature extraction
approach on image clustering and retrieval. Experiments are presented in Section 4. Concluding
remarks are offered in Section 5.
In this section, we will first discuss the importance of texture features in retrieval. We will then
introduce wavelet transforms and the wavelet-based texture features extracted from images.
2.1 Texture Features
In the current methods, spectral features (pixel values) are usually used to retrieve images from a
geographical image database. However, spectral features alone may not be sufficient to characterize
a geographical image because they fail to provide information about spatial pattern of pixels.
One way of obtaining better classifications and retrieval is the use of spatial pattern recognition
techniques that are sensitive to factors such as image texture and pixel content. In addition, using
texture features of a group of pixels instead of individual pixels can reduce the information to be
processed. Hence it decreases the required memory storage and computation for image retrieval
and clustering. Because of the spatial nature of geographic images, texture has been used to
distinguish among the images [RW96, Lan80, HK69, MM96, Joh94, GT83]. However, compared to
purely spectral based procedures, texture-based retrieval systems have received limited attention
in remote sensing in the past [LK94].
Different methods defining texture were proposed to model natural texture feature or to be
used in classification. These approaches have included the use of Fourier transforms [SF86], fractals
[Man77], random mosaic models [AR81, JMV81], mathematical morphology [DP89], syntactic
methods, and linear models [CJ83]. However, most of these approaches have been used in the field
of pattern recognition and have not been applied to the analysis of remotely sensed data [RW96].
One method in texture feature extraction that has been used for remotely sensed data is to pass a
filter over the images and use the variance of the pixels covered by the filter as the value of the pixel
in the texture image [AN91, BN91, Jen79]. Woodcock and Ryherd found that using the above local
variance texturing technique in an adaptive windowing procedure resulted in an improved image
for certain applications [WR89]. The location of the adaptively located window used to calculate
local variance is the window with the lowest local variance of all windows that include the pixel.
They then apply a multi-pass, pair-wise, region-growing algorithm to find spatially cohesive units,
or "regions", in the image [RW96].
Johnson used a knowledge-based approach to classification of built-up land from multi-spectral
data. The approach involves spectral classification, determination of size and neighbor relations for
the segments in the spectrally classified image, and rule-based classification of the image segments
into land-use categories [Joh94]. The implementation is slow, and performance problems were
encountered due to the large number of objects being processed [Joh94].
Gray-level co-occurrence matrix, a matrix of second order probabilities, has been used in texture
analysis [BL91, HTC86]. The element s(i; j; v) of the co-occurrence matrix is the estimated
probability of going from gray-level i to j, given the displacement vector \Deltay). A set of
statistical values computed from co-occurrence matrices, such as entropy, angular second momen-
tum, and inverse difference moment have also been used [HSD73, PF91]. In practice, there are two
problems regarding this approach. The first is that the co-occurrence matrix depends not only on
the spatial relationship of gray-levels, but also on regional intensity background variation within
the image. The second issue is the choice of displacement vector because the co-occurrence
matrix characterizes the spatial relationship between pixels for a given vector v [WH90].
Wang and He presented a statistical method for texture analysis, where they define texture unit as
the smallest complete unit that best characterizes the local texture in eight directions from a pixel.
The distribution of texture units results in texture spectrum which can be used in texture analysis
[WH90]. Since the local texture information for a pixel is extracted from a neighborhood of 3 \Theta 3
pixels, instead of one displacement vector used in the gray-level co-occurrence matrix, their method
is more complete for the characterization of texture. They provided limited experimental results.
The textural structures that can be described within a neighborhood are naturally limited to
those which are observable within the size of neighborhood. Hence, features that are extracted based
on measurements within a fixed size neighborhood (for example 3 \Theta 3 pixels window) have poor
discrimination power when applied to textures not observable within the neighborhood because of
wrong scale [BGW91]. All the above texture analysis methods share this common problem. This
issue motivates the use of a multi-resolution method.
Bigun et al formulated a general n-D least mean square problem whose solution facilitates the
extraction of dominant orientation [BGW91]. In 2-D space, they used the linear symmetry along
with the Laplacian pyramids approach, to produce feature images. The linear symmetry features
directly measure the dominant orientation information in the frequency channels [BGW91]. QBIC is
a content-based image retrieval system which retrieves images based on their color, shape, or texture
In QBIC, texture features are based on modified versions of the coarseness, contrast,
and directionality features proposed in [TMY78]. The distance between the three-elements feature
vectors is measured by weighted Euclidean distance. However, new mathematical representations of
image attributes are needed in QBIC [FSN 95]. Features that describe new image properties such
as alternate texture measures or that are based on fractals or wavelet representations, for example,
may offer advantages of representations, indexability, and ease of similarity matching [FSN
Jacobs et al presented a searching algorithm for query images in an image database [JFS95].
They apply wavelet transform on query and database images. The wavelet coefficients are then
truncated and only the coefficients with largest magnitudes are used in comparing images. They
developed an image query metric which uses those truncated and quantized wavelet coefficients.
According to them, wavelet coefficients provide information that is independent of the original
resolution. Thus, a wavelet scheme allows queries to be specified to any resolution (potentially
different to that of database images) [JFS95]. Hence they call their method multi-resolution image
querying. Manjunath and Ma used Gabor wavelet transform to extract texture features and showed
that it performed better than other texture classification methods [MM96]. They used mean and
variance of frequency sub-bands to represent texture. In our method, we take advantage of multi-resolution
property of wavelet explicitly.
In our approach, as will be explained later, we apply wavelet transform to extract texture
features. Based on the property that was mentioned in [JFS95], a query image can have a different
size from that of a database image, which we can use. In addition to that, by applying wavelet
transform, we can have information about images at different scales (resolutions) from fine to coarse
at the same time. Our interpretation of the term (multi)resolution is different from that of [JFS95].
In their case, resolution indicates number of pixels in the image, and since number of pixels of
the query image and the database image do not necessarily have to be the same, they call their
method multi-resolution image querying. In our method, since we extract the image features at
different scales or levels of details (based on number of levels in wavelet transform), we have a
multi-resolution representation of the image. Having information about images at different scales
(resolutions), we can control the desired level of accuracy in comparing images. For example, in
clustering images where we roughly categorize images, we use only the coarse features, whereas
in image retrieval, all the fine details of images will be used. However, the method proposed in
[JFS95] uses a distance metric with fixed accuracy. Moreover, their method directly compares
the m largest wavelet coefficients of images one by one. Thus, if the image is translated, so will
the corresponding wavelet coefficients. Hence the result of comparison will be very different. In
our method, we compare derived values (mean, variance, or number of edge pixels) from each
of the corresponding frequency sub-bands, resulting in less sensitive comparison with respect to
translation. The method by Jacobs et al has the advantage of being fast. However, it has the
common problem of the nearest neighbor retrieval approaches that just rely on the closeness of
the feature vectors without considering the semantics of images. As we explained in the previous
section, such methods sometimes may not provide satisfactory solutions. In our approach, we
consider the semantics of images through clustering which results in more effective retrieval.
Feature extraction may be performed on either a whole image or the segments of images. We
assume that images are decomposed into segments properly. Some decomposition approaches can
be found in [SC94a, SC94b, RSZSM97, SRGZ97]. Upon the proper decomposition of images, the
system extracts the features of all the segments of images. The query image will be compared to
the corresponding segments of database images with the proper size. In our implementation, we
used nona-tree decomposition [RSZSM97, SRGZ97].
Nona-tree decomposition is a block-oriented approach in which images (and their segments)
are hierarchically decomposed into nine quadrants. The nine overlapping quadrants are uniformly
distributed segments. If there are k levels in nona-tree, the size of its segments will be N 2 , N 2
, where N 2 is the size of the original image. A query image is compared to the smallest
segments in nona-tree whose size is greater than or equal to the size of the query image. If the size
of nona-tree's smallest segments is larger than that of the query image, the query image will not
be compared to any of the segments. For example, if a nona-tree has segments of sizes 128\Theta128,
64\Theta64, and 32\Theta32, given a query image of size 64\Theta64, it will be compared to the 64\Theta64 segments.
A query image of size 60\Theta60 will be compared to the 64\Theta64 segments but a 24\Theta24 query image
will not be compared to any of the segments. In general, the segments generated by block-oriented
decomposition methods such as quad-tree, quin-tree, or nona-tree may not perfectly align the
portion which matches the query image. For example, in quad-tree, in the worst case, only 25% of
the matching segment will be covered. However, in [RSZSM97, SRGZ97], we proved that nona-tree
segments at least cover 56% of the matching segment. In average case, a larger portion of matching
segment will be covered, and our experiments showed that nona-tree performs better than other
block-oriented decomposition approaches [RSZSM97, SRGZ97].
2.2 Wavelet Transform
Wavelet transform is a type of signal representation that can give the frequency content of the signal
at a particular instant of time. In this context, one row/column of image pixels can be considered
as a signal. Applying a wavelet transform on such a signal decomposes the signal into different
frequency sub-bands (for example, high frequency and low frequency sub-bands). Initially, regions
of similar texture need to be separated out. This may be achieved by decomposing the image in
the frequency domain into a full sub-band tree using filter banks [Vai93]. Each of the sub-bands
obtained after filtering has uniform texture information. A filter bank based on wavelets could be
used to decompose the image into low-pass and high-pass spatial-frequency bands [Mal89a].
We will now briefly review the wavelet-based multi-resolution decomposition. More details can
be found in Mallat's paper [Mal89b]. To have the multi-resolution representation of signals we can
use a discrete wavelet transform. We can compute a coarser approximation of input signal A 0 by
convolving it with the low pass filter ~
H and down sampling the signal by two [Mal89b]. By down
sampling, we mean skipping every other signal sample (for example a pixel in an image). All the
discrete approximations A j , is the maximum possible scale), can thus be computed
from A 0 by repeating this process. Scales become coarser with increasing j. Figure 3 illustrates
the method.
A
A
A
G
G

Figure

3: Block diagram of multi-resolution wavelet transform.
We can extract the difference of information between the approximation of signal at scale
and j. D j denotes this difference of information and is called detail signal at the scale j. We
can compute the detail signal D j by convolving A j \Gamma1 with the high pass filter ~
G and returning
every other sample of output. The wavelet representation of a discrete signal A 0 can therefore be
computed by successively decomposing A j into A j+1 and D j+1 for This representation
provides information about signal approximation and detail signals at different scales. We denote
wavelet representation of signal A 0 after K levels as . The
idea of using multi-resolution property of wavelets in clustering is to use the features of the wavelet
coefficients at the coarse scale levels.
Corresponding to the lowpass filter, there is a continuous-time scaling function OE(t), and corresponding
to the highpass filter, there is a wavelet !(t). The dilation equation produces OE(t), and
the wavelet equation produces !(t) [SN96]. For example, for Haar wavelet transform with ~
[1=
2], and ~
2], the dilation equation is
and the wavelet equation is

Figure

4 shows the Haar wavelet !(t).1t
w(t)

Figure

4: The Haar wavelet !(t).
We can easily generalize wavelet model to 2 dimensions for images, in which we can apply 2
separate one-dimensional transforms [HJS94]. The image is first filtered along the horizontal (x)
dimension, resulting in a lowpass image L and a highpass image H. We then down sample each
of the filtered images in the x dimension by 2. Both L and H are then filtered along the vertical
(y) dimension, resulting in four subimages: LL, LH, HL, and HH. Once again, we down sample
the subimages by 2, this time along the y dimension. The two-dimensional filtering decomposes an
image into an average signal (LL) and three detail signals which are directionally sensitive: LH
emphasizes the horizontal image features, HL the vertical features, and HH the diagonal features.

Figure

5-a shows a sample airphoto image. Figures 5-b,c, and d show the wavelet representation
of the image at three scales from fine to coarse. At each level, sub-band LL (the wavelet approximation
of the original image) is shown in the upper left quadrant. Sub-band LH (horizontal edges)
is shown in the upper right quadrant, sub-band HL (vertical edges) is displayed in the lower left
quadrant, and sub-band HH (corners) is in the lower right quadrant.
Our feature extraction and clustering methods can use any appropriate wavelet transforms such
as Haar, Daubechies, Cohen-Daubechies-Feauveau ((2,2) and (4,2)), or Gabor wavelet transforms
a) b) c) d)

Figure

5: Multi-resolution wavelet representation of an airphoto image: a)original image; b) wavelet
representation at scale 1; c) wavelet representation at scale 2; d) wavelet representation at scale 3.
[Vai93, SN96, URB97, MM96]. Applying wavelet transform on images results in wavelet coefficients
corresponding to each sub-band. We can extract different features from wavelet coefficients of each
of these sub-bands. Next subsection explains the features that we used in the experiments.
2.3 Image Features
In clustering database images, different kinds of features such as shape, color, texture, layout, and
position of various objects in images can be used. Texture in images has been recognized as an
important aspect of human visual perception [TMY78, Har79, Joh94, RW96, WH90]. Tamura et al.
[TMY78] have discussed various texture features including contrast, directionality, and coarseness.
Contrast measures the vividness of the texture and is a function of gray-level distribution. Some
factors influencing the contrast are dynamic range of gray-levels, polarization of the distribution
of black and white on the gray-level histogram, and sharpness of edges. Directionality measures
the "peakedness" of the distribution of gradient directions in the image. Coarseness measures
the scale of texture. When two patterns differ only in scale, the magnified one is coarser. In
a geographical image database we classify images into residential, agriculture, water, and grass
clusters. These clusters have different textures. For example, their contrast is usually different;
or, the cluster of water images does not have any directionality, whereas residential images are
considered as directional images. These texture features of geographical images contain main visual
characteristics of images that can be utilized in geographical image clustering and retrieval.
Applying wavelet transform on images can provide information about the contrast of images.
Also, different sub-bands generated by wavelet transform have information about horizontal, verti-
cal, and diagonal edges in images [Mal89b], which helps in extracting features related to directionality
of images. Moreover, multi-resolution aspect of wavelet transform provides information about
images at different scales (from coarse to fine). These properties make wavelet transform a good
candidate to extract texture features of geographical images.
We calculated the mean and variance of wavelet coefficients to represent the contrast of the
image. We also count number of edge pixels in horizontal, vertical, and diagonal directions to have
an approximation of directionality of the image. These features are extracted at different scales
to use the multi-resolution property of wavelets. Edge pixels in the image have high amplitude in
detail signals (LH; HL, and HH). It means that the wavelet coefficient of such pixels has high
positive/negative value. Let Image(i; j) be a pixel in the image of size n \Theta m. Let WLH (i; j),
WHL (i; j), and WHH (i; j) be the corresponding wavelet coefficients of Image(i; j), for LH;HL,
and HH sub-bands, respectively. We define mLH as the maximum of absolute value of WLH (i; j).
That is,
To detect horizontal edge pixels, we first compute mLH . We consider Image(i; as a horizontal
edge pixel if
where ff is a factor that is determined by experiments. In our experiments, we chose ff as 0.35.
Similarly, we define
and
Image(i; is considered as an edge pixel if

Tables

1, 2, and 3 show the wavelet coefficients of Figure 5-a at scale 3 using Haar wavelet
transform. The corresponding edge pixels are shown in Figure 5-d.

Table

1: Wavelet coefficients WLH of Figure 5-a at scale 3.

Table

2: Wavelet coefficients WHL of Figure 5-a at scale 3.
28
26 43 -40 -45
28

Table

3: Wavelet coefficients WHH of Figure 5-a at scale 3.
Figure

The seven sub-bands generated after applying two levels of wavelet transform.
We tested our approach using two different sets of features. To obtain the features in feature set
1, we apply a two-level wavelet transform. For the sub-band LL of the coarsest scale of the wavelet
transform, we consider mean and variance of absolute value of wavelet coefficients as its features
?. For the other sub-bands, in addition to mean and variance, the system
counts number of edge pixels in the sub-band. So the feature vector for each of these sub-bands is
number of edge pixels ?. In this feature set, we will have seven sub-bands with
features. Figure 6 shows the seven sub-bands of image schematically. In clustering, we
just consider the features of the sub-bands of the coarsest scale of wavelet transform (11 features)
to compute the distance. Table 4 shows the extracted feature set 1 for Figure 5-a.
Number of
Sub-band Mean Variance edge pixels

Table

4: Feature set 1 of Figure 5-a.
In feature set 2, the two-level wavelet transform is applied giving seven sub-bands. The feature
vector for each sub-band is ! mean; variance ?. Each subsegment of the image has 14 features.
To compute the distance between feature vectors in clustering, we only consider the features of the
second scale of the wavelet transform (4 sub-bands, 8 features). We have extracted the feature sets
using both Haar and Daubechies wavelet transforms [SN96]. Feature set 2 for Figure 5-a is shown
in

Table

5 where Haar wavelet transform is applied.
Sub-band Mean Variance

Table

5: Feature set 2 of Figure 5-a.
To give equal contributions to all the features in determining the similarity, and to remove the
arbitrary affects of different measurement units, we normalize the feature values and recast them
in dimensionless units. To normalize the features, we first compute the mean -
f i and variance oe i of
each feature f i using the training samples. Our training samples include residential, agriculture,
water, and grass areas. Let a feature vector be normalized feature
vector be -
f n ). We normalized f i using Equation (8):
f i is the normalized value of f i . Our experiments show that we should give more (controlled)
contribution to the more important features. We can do so, by assigning weights to the features
and giving more weights to the more important features. Let
vector, where w i is the weight that is assigned to feature f i . The weighted feature vector ~
f of
feature vector f will be:
~
3 Clustering and Retrieval
Applying the feature sets introduced in Section 2, we will now present template-based image clustering
and retrieval approaches. We first describe how cluster templates are chosen. Given clustering
templates and features, images in a database can be classified based on their similarity to the
cluster templates. Based upon the resulting clusters, image retrieval can be efficiently supported.
3.1 Clustering - Related Work
Given a set of feature vectors, the feature space is usually not uniformly occupied. Clustering
the data identifies the sparse and the dense places, and hence discovers the overall distribution of
patterns of the feature vectors. In many existing clustering algorithms, k-medoid methods have been
used, where each cluster is represented by the center of gravity of the cluster. For example, PAM
(Partitioning Around Medoids) [KR90] was proposed to determine the most centrally located object
medoid for each cluster. Each non-selected object is grouped with the medoid to which it is the
most similar. CLARA (Clustering LARge Applications) [KR90] draws a sample of data set, applies
PAM on the sample, and finds the medoids of the sample. Ng and Han introduced CLARANS
(Clustering Large Applications based on RANdomaized Search) which is an improved k-medoid
method [NH94]. Ester et al presented a clustering algorithm DBSCAN relying on a density-based
notion of clusters which is designed to discover clusters of arbitrary shapes [EKSX96]. The key idea
in DBSCAN is that for each point of a cluster, the neighborhood of a given radius must contain
at least a minimum number of points, i.e. the density in the neighborhood has to exceed some
threshold. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) incrementally
and dynamically clusters incoming data points to produce the best clusters with the available
resources (i.e. available memory and time constraints) [ZRL96]. Ma and Manjunath used a neural
network to learn the similarity measure and partition the feature space into clusters representing
visually similar patterns [MM95].
As mentioned in Section 1, retrieval only based on closeness of feature vectors may not necessarily
give satisfactory results, because it is possible that semantically irrelevant images have
close feature vectors. Indexing trees such as R-trees and its variants have been successfully used
in the past and are very efficient in retrieving the nearest neighbors of the query image in the
feature space. However, they retrieve all the nearest neighbors, whether or not they have the same
semantic as that of the query image.
Our goal in clustering is to group together images considering their semantics. In other words,
we want to cluster images such that images with the same semantics belong to the same cluster.
These semantic clusters can be used to narrow down the search to the areas in the feature space
where images have similar semantics to that of the query image. Searching only the semantically
relevant images makes the retrieval more effective, and also more efficient. Note that, our proposed
approach is not an alternative to R-tree or its variants. These indexing trees can be integrated within
each cluster to provide better efficiency, while the clusters preserve the similarity in semantics.
Semantics of images is application dependent, for example, in a GIS visual database, we can
consider residential, water, grass, and agriculture clusters.
3.2 Template-based Clustering
To support efficient content-based image retrieval, methods have been proposed to first cluster
database images based on their similarity to predefined image templates, which are termed cluster
templates or cluster icons [WAL To retrieve database images which are similar to or
contain a query image, the query image will first be compared to the cluster templates. Only
database images in the clusters corresponding to the chosen cluster templates will be searched.
The cluster templates and their associated information (as will be explained later) approximate
the clusters. An effective clustering of database images can greatly reduce the number of images to
be searched, resulting in a more efficient retrieval. Indexing techniques can be incorporated within
each cluster to provide a faster retrieval.
R-trees can be used to find the templates representing the clusters. But since in practice,
we have high dimensional feature vectors (20 features), R-trees might suffer from "dimensionality
curse" [BKK96]. Thus, we decided to apply hierarchical clustering to find the cluster templates.
Using separate training data for each cluster, we classify images considering their semantics. The
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

(a) (b)

Figure

7: a) arbitrary shaped clusters. b) circular sub-clusters of clusters.
type of semantic clusters is application dependent and should be determined by the application
experts.
For example, in a geographical image database, we can define the clusters of residential, agricul-
ture, water, grass, and a special cluster other. The cluster other contains all images which cannot
be included in any of the residential, agriculture, water, or grass clusters. We represent clusters
by cluster templates to organize database images. In the feature space, since semantically different
image data may have their feature vectors close to each other, each cluster may have arbitrary
shape.

Figure

7-a shows an example in two dimensional space where clusters have arbitrary shapes.
Our proposed method is a hybrid (both supervised and unsupervised) approach. Using supervised
classification approach, we choose a set of training samples for each cluster. Then using
unsupervised classification we find the sub-clusters within each cluster. The centroids of those
sub-clusters are used as the cluster templates of the cluster. Figure 7-b shows such centroids within
each cluster as black square dots. Cluster other should include all the objects in Figure 7-a shown
by "   ". In a hierarchical approach, sub-clusters are defined within each cluster at different levels.
Suppose the training set of a cluster C c has n objects. Initially, we assume there are n sub-clusters
corresponding to the n objects in the cluster. We compute the distances between all pairs of
sub-clusters. We then group together the two closest sub-clusters, resulting in
We repeat the process of computing the distances between all pairs of current sub-clusters, and
grouping the two closest ones until having only one sub-cluster.
This hierarchically nested set of sub-clusters can be represented by a tree-diagram, or a dendrogram
[Gor81]. Figure 8 shows a group of 5 objects and their corresponding dendrogram. Each of
n leaves of the tree represents the single object whose label appears below it. Each position up the
tree at which branches join, has an associated numerical value d, which determines the distance
between the two grouped sub-clusters. The smaller the value of d, the more similar two sub-clusters
will be. We considered the distance between the centroids of sub-clusters as the distance between
sub-clusters. We call this method as centroid method. There are different methods to define the distance
between sub-clusters. In unweighted pair-group method using arithmetic averages (UPGMA),
the distance between two sub-clusters is the arithmetic average of the distances between the objects
d d
d2
(a) (b)

Figure

8: A group of objects and the corresponding dendrogram.
in one cluster and the objects in the other [Rom84]. In single linkage clustering method (SLINK),
the distance is the minimum of the distances between the objects in one cluster and the objects in
the other one (distance between two closest objects). The distance between sub-clusters in complete
linkage clustering method (CLINK), is defined as the maximum of the distances between objects in
the two sub-clusters [Rom84]. Applying either of these methods may give a different dendrogram.
We can cut the dendrogram of each cluster at different levels to obtain k number of sub-clusters
(cluster templates). The choice of k and the corresponding cutting distance is one of the most
difficult problems of cluster analysis, for which no unique solution exists. For example, if we cut
the dendrogram in Figure 8-b, where the distance between sub-clusters is d 1 , we will have two
sub-clusters, whereas if we cut at level d 2 , we will have three sub-clusters. Let
be the sub-cluster containing objects e 1 . If we cut a dendrogram at the level in which
the distance between the last two grouped sub-clusters is d i , then the maximum distance between
the children of the remaining sub-clusters will be d i . For example, in Figure 8-b, if we cut the
dendrogram at level d 1 , the distance between sub-clusters f1g and f2g, or f3g and f4g, or f3,4g
and f5g, will be less than d 1 .
A measure of the quality of clustering can be obtained by computing the average silhouette
width of the n image samples for each k value. Given an image i, which is assigned to
a cluster C be the average distance of i to all other images in cluster C j and b i
be the average distance of i to all images in cluster C h , where C h is the nearest neighbor cluster of
cluster C j (h 6= j). The s k is calculated as follows:
1:0. The value of s k can be used to select an optimal value of k such that s k will
be as high as possible. The maximum value of s k is known as the silhouette coefficient. In general,
a silhouette coefficient value of 0.70 or higher indicates a good clustering selection [KR90].
Assume that a cluster C c consists of m sub-clusters S c1 In clustering database im-
ages, whenever a sub-cluster S ci is chosen, the database images will be assigned to the corresponding
cluster C c . Similarly, in image retrieval, if a sub-cluster S ci is chosen, only the corresponding cluster
C c will be searched.
We also tested different methods in determining the distance between sub-clusters, to make the
dendrograms.
3.3 Clustering
To cluster database images, we first compute the distance between the cluster feature vectors of
cluster templates and all database images. Using the multi-resolution property of the wavelet trans-
form, we extract the image features at different scales. In comparing cluster feature vectors, only
coarse scale features will be used; while in retrieving images all the features will be compared. In
Section 2.2, we used the notation fAK as the wavelet representation
of signal A 0 after K levels of the wavelet transform. We define clustering scale (K; c) as the
number of coarsest scales that will be used in clustering after applying K levels of the wavelet trans-
form. It means that in clustering only the features extracted from fAK ; DK will
be used, where 1 - K - J , 1 - c - J , and c ! K. For example, clustering scale (5, 2) means that
the 5-level wavelet transform will be applied and the features of the 2 coarsest scales (fA 5
will be used in clustering. We define retrieval scale (K; r) as the number of scales that will be used
in retrieval after applying K levels of transform. In retrieving images, the features extracted from
will be used, where 1 - K - J , usually r ? c. In
retrieving, we will use all the features. Clustering scale and retrieval scale for both feature sets 1
and 2 are (2,1) and (2,2), respectively.
Let X and Y be two undecomposed images (for example, a query image, a cluster template, or
an undecomposed segment of a database image). Let CFX
be their cluster feature vectors extracted from the subbands fAK based
on the clustering scale (K; c). Let RFX be the retrieval
feature vectors of X and Y respectively, extracted from the subbands fAK
according to the retrieval scale (K; r). To measure the similarity between X and Y in clustering,
we define the clustering distance d c as follows:
where dist could be any appropriate distance function. Here, we use Euclidean distance for dist. To
measure the similarity between X and Y in retrieval, we define the retrieval distance d r as follows:
d r (X; Y
A database image Y can be decomposed into a set of smaller segments fY g. To measure
the distance between the database image Y and the undecomposed image X (a query image, or a
cluster template) in clustering, we use D c :
Similarly, in retrieval, we use D r to compute the distance between the query image X and the
database image Y:
In determining the correct cluster(s) for a database image, we use the distribution of objects
(feature vectors) in each sub-cluster. For each sub-cluster we define a scope. Let - i and - i be the
mean and variance of clustering distances d c of objects to the centroid of the sub-cluster i. The
scope of sub-cluster i is part of the feature space whose distance to the centroid of sub-cluster is less
than Parameter fi is a factor that is determined by experiments. In a two dimensional
space, the scope of a sub-cluster is a circle with radius In our clustering approach,
whenever a database image Y falls within the scope of any sub-cluster, it will be assigned to the
cluster corresponding to the sub-cluster. The reason is that a database image may contain the
features of several different cluster templates and it should be assigned to all the corresponding
clusters.
If the database image Y is not within the scope of any of the sub-clusters, but the distance
between the database image and the closest cluster template t is less than a threshold T ,
it will be assigned to the corresponding cluster represented by the closest cluster template. If there
is no cluster template for which the distance between the database image and the cluster template
is less than T , the database image will be assigned to the special cluster other. The cluster other
contains all the database images that cannot be assigned to the predefined clusters.
As an example, Figure 9 shows the scope of 3 sub-clusters A, B, and C in a two dimensional
space. In general, these sub-clusters do not necessarily belong to different clusters. The object
denoted by point 1 falls in the scope of sub-clusters A and B and will be assigned to both.
x
x
Cx
x
x
x
x
x
x
x
x
x
x
x
x
x
A
x
x
x
x
x3

Figure

9: Three sample sub-clusters.
However, some objects may not fall in any of the scopes considered above. For example, Point
2 in

Figure

9 is not in the scope of any of sub-clusters. Our clustering approach assigns such an
object to the closest sub-cluster. Although the distance d c between point 2 and the centroid of
sub-cluster C is less than that of sub-clusters A and B, but based on the distribution of objects
within clusters, it is more logical to assign object 2 to sub-cluster A. So, we first compute the
distance between the object and the border of the sub-clusters' scope, and then choose the closest
cluster. Hence, object 2 will be assigned to sub-cluster A. The object shown as point 3 in Figure 9
will be assigned to the cluster other, because it is far away from all clusters and its distance d c to
the closest cluster is greater than threshold T .
Algorithm 1 shows the steps in choosing the cluster(s) for database images. After generating
the templates, for each image, the time complexity of choosing clusters (either for adding the image
to the clusters or for retrieving images) would be O(t) where t is the number of templates. The
required time to assign database images to the clusters is O(tN) where N is the number of images.
Since the number of templates is generally far less than the number of images, that is t !! N , the
time complexity will be O(N ). That is, the time complexity of generating clusters will be
linear in terms of number of database images. So it can process large image databases efficiently. As
it will be shown in the experiments, just by clustering database images, the system avoids searching
more than 75% of the database images.
Algorithm 1
Choose Cluster To Add(Image)
/* Based on distance between Image and subclusters, */
adds Image to the corresponding cluster(s). */
f
/* compute the distance D c between all */
/* the subclusters and database image */
to Number Of Subclusters
Adjusted Distance
/* distance to the border of subcluster's scope */
/* find the index of the closest subcluster */
Closest Find Min(Adjusted Distance);
if (Adjusted DistanceClosest
f/* add to special cluster other */
Add Image T
return;
/* add image to "all" clusters that are close enough */
to Number Of Subclusters
fif (Distance
f/* add to the corresponding cluster of subcluster i */
Add Image T
if (Cluster Added == FALSE) /* image not added to any cluster */
/* add to the closest cluster */
Add Image T
3.4 Image Retrieval
To retrieve database images similar to or containing the query icon, the distance d c between query
icon and cluster templates will first be computed. If the query icon is within the scope of any
cluster template, the corresponding cluster(s) will be searched. To measure the similarity of query
icon and the database images in the cluster(s), the distance D r will be used. If the query icon
does not fall in scope of any cluster template, but its distance to the closest cluster template is less
than a threshold T , then the cluster corresponding to the closest one will be searched. Otherwise,
the cluster other will be searched to retrieve the relevant images. Given a good set of features,
comparing with the nearest-neighbor search used in R-tree and its variants, retrieval on the clustered
image database can potentially eliminate those images which are close to the query in the feature
space, but semantically not similar.
4 Experiments
As test data we used 1032 air photo images. To find the cluster templates representing each of
residential, water, grass, and agriculture clusters, we used between 50 to 100 training image data
per cluster. The training images were chosen such that the clusters do not overlap in the feature
space. We tried the four different methods to compute the distance between sub-clusters: centroid
method, CLINK, SLINK, and UPGMA. Figure 10 shows the sample cluster templates found by
the hierarchical approach using these methods.
Residential Water Grass Agriculture
a)
c)
d)

Figure

images: a) Centroid method; b) CLINK; c) SLINK; d) UPGMA.
It should be noted if additional terrains (such as jungle, or connifer forest) need to be added as
new clusters, a set of training samples should be chosen for them. By applying the clustering method
given in Section 3.2, the corresponding templates (subclusters) and their scopes can be determined.
Using the new templates and the previously chosen templates (for the current clusters), database
images can be easily reclustered. In other words, given new clusters (that are separate from current
clusters), our system can accommodate them.
To evaluate the performance of our clustering approach, we first visually inspected all database
images and assigned them to their corresponding clusters. We consider these clusters as ideal
clusters. We then compared the result of our clustering approach to the ideal clusters. Let C c be
the set of images in the system's output for cluster c. Let I c be the set of images in the ideal cluster
for cluster c. We define P c , the precision of clustering for cluster c as
The recall of cluster c, R c , is defined as
The higher P c and R c are, the better the clustering will be. As mentioned in Section 3.3, scope
of a subcluster S i is determined by - . The values - i and - i are calculated by the system
while generating the dendrogram. In our experiments a value between -0.5 to -1.5 for fi gives
reasonable results. The threshold T is used to assign a database image (or search) in the cluster
other. Based on our experiments, T - 2   max(- i ) can give satisfactory results. To find appropriate
values for ff, fi, and T , we can first choose a sample set of database images with known clusters. We
then determine the ideal clusters I c for the images in the sample set. After applying the clustering
method on the sample data, precision and recall of clustering (P c and R c ) can be calculated. We
can tune the values of ff, fi, and T by trail and error to obtain the desired P c and R c for the sample
set. The optimal values of these parameters can then be used to cluster the whole database.
In our experiments we chose 150 as the value of threshold T . We then tried the system for
different values of parameter fi. In the experiments, we used the original extracted features (without
any processing), normalized features, and weighted-normalized features. For weighted-normalized
features, we assigned a weight of 4 to the first feature of the feature vector, and equal weight of 1
to the rest of features. We chose these weights on a trail and error basis. Assigning weights is an
important issue that affects the results. The tables report the results when Haar wavelet transform
is applied. Table 6 shows P c and R c for different methods of finding distance between sub-clusters
when weighted-normalized features are used.
A good clustering should have high recall and precision. But recall is more important than
precision in clustering, because in retrieval from a cluster, we want to miss as few images as possible
comparing to retrieval from the whole database. In our experiments, the results obtained by CLINK
(maximum distance between objects in two sub-cluster) and UPGMA (average of distances between
objects) are better than those of others. They yield both very high precision and recall of clustering
Water 0.90 0.98 0.86 1.00 0.85 1.00 0.84 1.00
Agriculture 0.98 0.88 0.98 0.88 0.97 0.90 0.96 0.93
Residential
Grass 0.83 0.86 0.85 0.83 0.86 0.82 0.88 0.81
a) Centroid
Water 0.95 0.98 0.94 0.97 0.94 0.97 0.94 0.97
Agriculture
Residential 0.98 0.95 0.99 0.94 1.00 0.93 1.00 0.92
Grass 0.94 0.91 0.93 0.93 0.91 0.93 0.89 0.92
Water 0.92 0.97 0.89 0.98 0.87 0.99 0.78 1.00
Agriculture 0.96 0.84 0.96 0.84 0.96 0.84 0.96 0.84
Residential 0.98 0.90 0.98 0.90 0.98 0.90 0.98 0.90
Grass 0.79 0.87 0.81 0.85 0.82 0.83 0.82 0.77
c) SLINK
Water 0.90 0.98 0.86 1.00 0.85 1.00 0.84 1.00
Agriculture
Residential 0.97 0.98 0.97 0.98 0.97 0.98 0.97 0.98
Grass 0.97 0.86 0.98 0.84 0.98 0.83 0.98 0.81
d) UPGMA

Table

Precision and recall of clustering for different values of fi. Weighted-normalized feature
set 1.
(more than 0.90). As Table 6 shows, varying the value of fi makes slight changes in the performance
of clustering.
To study the impact of clustering on effectiveness of retrieval, we chose 10 query icons of size 32 \Theta
pixels. Our system processes queries such as "retrieve top N (say 50) images that have the same
texture as the query icon". We calculated the effectiveness of retrieval in terms of retrieval \Gamma rate.
Retrieval rate for top n images is defined as
retrieval number of relevant images in top n images
A high retrieval rate means that most of retrieved images are what the user looks for, and thus it
shows a better performance in retrieval. We calculated retrieval rates for values 10, 20, 30, 40, and
50 of n for the 10 query icons and then found their average value. We first computed retrieval rate
without using clustering. In this context, we consider nearest neighbor search approaches (such as
R-tree or serial search) as without clustering methods. We then computed the retrieval rate when
the clustering approach is used (with value 150 for threshold T and values 0.0, -0.5, -1.0, and -1.5
for fi). Different feature sets form different feature spaces. By normalizing features or assigning
weights to them, the feature space changes which results in altering the shape of clusters or the
location of query images. Also, different distance methods (centroid, CLINK, SLINK, UPGMA)
result in different shapes of clusters. As a consequence, we get different performances from these
various feature spaces and clusters. Given the appropriate feature vectors and distance method our
proposed approach can outperform nearest neighbor methods in retrieval.

Table

7 shows the average retrieval rate of top n images
set 1.

Table

8 has the same information for feature set 2. Tables 7 and 8 show that retrieval from
clustered database (using all different distance methods) outperforms the retrieval from the whole
database without clustering. In general, when a query image is near the boundary of the cluster, our
method avoids retrieving the images outside the cluster (the ones that are semantically irrelevant)
resulting in better performance. When the query image is in the middle of clusters, retrieving
nearest neighbors (without clustering), and retrieval using our clustering approach would be the
same. The retrieval rates obtained from CLINK and UPGMA are higher than those of others. It
confirms with our previous clustering results where these two methods had higher precision and
recall of clustering. In our experiments, we achieved the highest improvement in retrieval rate (from
0.70 to 0.95) when using original features set 2 and CLINK as shown in Table 8-a.
Original feature set 1 provides a higher retrieval rate than that of feature set 2. Normalizing
and then assigning weights to the features improves the retrieval rate when clustering is not used
for both feature sets 1 and 2. When clustering is applied, weighted-normalized features provide a
higher retrieval rate for feature set 2 (comparing to the original feature set), but not for feature set
1. In the latter case, another set of weights should be chosen. In general, a better method should
be used to find the best weights for the features. For the same distance method and feature set,
different values of fi do not make major changes in the retrieval rate.
DB Ret DB Ret DB Ret DB Ret
Centroid
Clustering 1.00 0.73 1.00 0.73 1.00 0.73 1.00 0.73
a) Original features
DB Ret DB Ret DB Ret DB Ret
Centroid
UPGMA
b) Normalized features
DB Ret DB Ret DB Ret DB Ret
Centroid
Clustering 1.00 0.76 1.00 0.76 1.00 0.76 1.00 0.76
c) Weighted-normalized features

Table

7: DB-Ratio DB, and retrieval-rate Ret for different values of fi. a) Original features; b)
Normalized features; c) Weighted-normalized features. (feature set 1).
The other issue that should be considered is the number of images that are searched in retrieval
from a clustered database. We define the ratio of database search DB \Gamma ratio as
number of images searched in database
total number of images in database
The lower DB \Gamma ratio is, the better clustering would be. The time to search the database is
directly proportional to the number of images to be searched. Thus a low DB \Gamma ratio means that
less time is used to search the database. Tables 7 and 8 show the average DB \Gamma ratio for the 10
query icons. In average our clustering approach yields a DB \Gamma ratio of about 0.25, that is, about
75% of irrelevant images will not be searched. We performed the experiments on a SUN SPARC
workstation using 168 MHz UltraSparc CPU with SunOS operating system and 1024 MB memory.
The average time to search the whole database was 1.5 seconds, whereas after clustering, it only
took about 0.23 seconds to return the relevant images. Integrating current indexing methods such
as R-tree or its variants can speed up the retrieval process more.

Figure

11 shows the 10 closest retrieved images (ordered from left to right) for a sample grass
query image without clustering and using clustering, respectively. For the query image in Figure 11,
DB Ret DB Ret DB Ret DB Ret
Centroid 0.22 0.94 0.23 0.95 0.23 0.96 0.23 0.96
Clustering 1.00 0.70 1.00 0.70 1.00 0.70 1.00 0.70
a) Original features
DB Ret DB Ret DB Ret DB Ret
Centroid
Clustering 1.00 0.71 1.00 0.71 1.00 0.71 1.00 0.71
b) Normalized features
DB Ret DB Ret DB Ret DB Ret
Centroid
Clustering 1.00 0.78 1.00 0.78 1.00 0.78 1.00 0.78
c) Weighted-normalized features

Table

8: DB-Ratio DB, and retrieval-rate Ret for different values of fi. a) Original features; b)
Normalized features; c) Weighted-normalized features. (feature set 2)
the first 10 retrieved images without clustering include four cases (images 6 through 10) which are
not grass images. As shown in Figure 11-c, those semantically irrelevant images are not retrieved
when our clustering approach is applied, which demonstrates the advantage of our method.
a) Query icon
Retrieved images without clustering
c) Retrieved images with clustering

Figure

11: Retrieval results
5 Conclusion
We have designed a content-based retrieval system that supports novel clustering and retrieval for
geographical images. This system utilizes the rich texture features existing in the geographical
images to assist content-based retrieval. A clustering approach is designed to automatically categorize
images based on themes such as land use or land cover. Using the multi-resolution property
of wavelet transforms, we extracted the features at different scales where only the coarse features
were used in clustering. Since the semantic distribution of feature vectors in the feature space is
considered in the clustering, the retrieval of the relevant images can be performed by narrowing
down to the relevant clusters. Thus, more precise image searching can be supported rather than
the plain nearest-neighbor search. Through the experimental analysis, we have shown that texture
features can be effectively used to cluster geographical images. In addition, our experimental results
demonstrated that retrieval from the clustered database can be more effective than that of
the whole database, given the appropriate feature sets and distance methods.

Acknowledgments

We would like to thank the reviewers of this paper for their constructive criticism and useful
comments throughout the revision of the paper.



--R

Comparisons between spectral mapping units derived from spot image texture and field soil map units.
Mosaic models for texture.
Multidimensional orientation estimation with applications to texture analysis and optical flow.


Sar sea ice discrimination using texture statistics: A multivariate approach.
Seasonal variation of heterogeneity in the tallgrass prairie: A quantitative measure using remote sensing.
A Visual Information Management System for the Interactive Retrieval of Faces.
Markov random field texture models.
An Intelligent Image Database System.

A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise
Query by image and video content: The QBIC system.
Classification Methods for the Exploratory Analysis of Multivariate Data.
The use of contextual information in the classification of remotely sensed data.
Statistical and Structural Approaches to Texture.
Compressing Still and Moving Images with Wavelets.
Pattern recognition with measurement space and spatial cluster ing for multiple image.
Textural features for image classi- fication
Use of texture operators in segmen- tation
Spectral and texture features to classify elusive land cover at the urban fringe.
Fast multiresolution image querying.


Finding Groups in Data: an Introduction to Cluster Analysis.
The development of a spectral-spatial classifier for earth observational data

Remote Sensing and Image Interpretation.
Multiresolution approximation and wavelet orthonormal bases of L 2 (R).
A theory for multiresolution signal decomposition: the wavelet representa- tion

Image indexing using a texture dictionary.
Texture Features for Browsing and Retrieval of Image Data.
Efficient and Effective Clustering Methods for Spatial Data Mining.
Image texture processing and data integration for surface pattern discrimination.
Analysis for Researchers.
Automatic Image Indexation and Retrieval.
Supporting Content-Based Retrieval in Large Image Database Systems
Combining spectural and texture data in the segmentation of remotely sensed images.
Transform Features For Texture Classification and Discrimination in Large Image Databases.

Semantic clustering and querying on heterogeneous features for visual data.
A Fourier-based textural feature extraction procedure


Image decomposition and representation in large image database systems.
An Approach to Clustering Large Visual Databases Using Wavelet Transform.
Geographical Data Classification and Re- trieval
Texture Features Corresponding to Visual Perception.
Wavelet transforms using lifting scheme.
Multirate Systems And
Facial Images Retrieval
A new statistical approach to texture analysis.
Generation of texture image using adaptive windows.
BIRCH: An Efficient Data Clustering Method for Very Large Databases.
--TR

--CTR
Wei Wang , Daoying Ma , Yimin Wu , Aidong Zhang , David M. Mark, Webview: a distributed geographical image retrieval system, Proceedings of the 2002 annual national conference on Digital government research, p.1-4, May 19-22, 2002, Los Angeles, California
Gholamhosein Sheikholeslami , Wendy Chang , Aidong Zhang, SemQuery: Semantic Clustering and Querying on Heterogeneous Features for Visual Data, IEEE Transactions on Knowledge and Data Engineering, v.14 n.5, p.988-1002, September 2002
Tao Li , Qi Li , Shenghuo Zhu , Mitsunori Ogihara, A survey on wavelet applications in data mining, ACM SIGKDD Explorations Newsletter, v.4 n.2, p.49-68, December 2002

extracted:['feedback controls' 'feature weights' 'fault-tolerant software systems'
 'fault-tolerant routing algorithm' 'fault-tolerant routing'
 'fault-tolerant algorithms' 'fault-tolerance' 'finite domain'
 'zero storage biometric authentication' 'global illumination']
marked:['hierarchical clustering', 'geographical image retrieval', 'multi-resolution wavelet transform', 'texture features']
